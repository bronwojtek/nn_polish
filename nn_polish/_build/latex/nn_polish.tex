%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[a4paper,12pt,polish]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{polish}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Sonny]{fncychap}
\ChNameVar{\Large\normalfont\sffamily}
\ChTitleVar{\Large\normalfont\sffamily}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Sieci neuronowe dla początkujących w Pythonie: wykłady w Jupyter Book}
\date{09 mar 2023}
\release{}
\author{Wojciech Broniowski}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{docs/index::doc}}




\sphinxAtStartPar
\sphinxhref{https://www.ujk.edu.pl/~broniows}{\sphinxstylestrong{Wojciech Broniowski}}





\sphinxAtStartPar
Niniejsze wykłady były pierwotnie prowadzone dla studentów inżynierii danych na  \sphinxhref{https://www.ujk.edu.pl}{Uniwersytecie Jana Kochanowskiego} w Kielcach i dla \sphinxhref{https://kisd.ifj.edu.pl/news/}{Krakowskiej Szkoły Interdyscyplinarnych Studiów Doktoranckich}. Wyjaśniają bardzo podstawowe koncepcje sieci neuronowych na najbardziej przystępnym poziomie, wymagając od studenta jedynie bardzo podstawowej znajomości Pythona, a właściwie dowolnego języka programowania. W trosce o prostotę, kod dla różnych algorytmów sieci neuronowych pisany jest od podstaw, tj. bez użycia dedykowanych bibliotek wyższego poziomu. W ten sposób można dokładnie prześledzić wszystkie etapy programowania.

\begin{sphinxadmonition}{note}{Zwięzłość}

\sphinxAtStartPar
Tekst jest zwięzły (wydruk pdf ma \textasciitilde{}130 stron wraz z załącznikami), więc pilny student może ukończyć kurs w kilka popołudni!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Linki}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Jupyter Book:
\sphinxurl{https://bronwojtek.github.io/nn\_polish/docs/index.html}

\end{itemize}



\sphinxAtStartPar
Pierwotna angielska wersja książki:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Jupyter Book:
\sphinxurl{https://bronwojtek.github.io/neuralnets-in-raw-python/docs/index.html}

\item {} 
\sphinxAtStartPar
pdf i kody: \sphinxhref{https://www.ifj.edu.pl/~broniows/nn}{www.ifj.edu.pl/\textasciitilde{}broniows/nn} lub \sphinxhref{https://www.ujk.edu.pl/~broniows/nn}{www.ujk.edu.pl/\textasciitilde{}broniows/nn}

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Jak uruchamiać kody w książce}

\sphinxAtStartPar
Główną zaletą książek wykonywalnych jest to, że czytelnik może cieszyć się z samodzielnego uruchamiania kodów źródłowych, modyfikowania ich, czy zabawy z parametrami. Nie jest potrzebne pobieranie, instalacja ani konfiguracja. Po prostu przejdź do

\sphinxAtStartPar
\sphinxurl{https://bronwojtek.github.io/nn\_polish/docs/index.html},

\sphinxAtStartPar
w menu po lewej stronie wybierz dowolny rozdział poniżej Wstępu, kliknij ikonę „rakiety” w prawym górnym rogu ekranu i wybierz „Colab” lub „Binder”. Po pewnym czasie inicjalizacji (za pierwszym razem dla Bindera trwa to dość długo) można uruchomić notebook.

\sphinxAtStartPar
Dla wykonywania lokalnego, kody dla każdego rozdziału w postaci
notebooków \sphinxhref{https://jupyter.org}{Jupytera} można pobrać klikając ikonę „strzałki w dół” w prawym górnym rogu ekranu. Pełen zestaw plików jest również dostępny z linków podanych powyżej.

\sphinxAtStartPar
Dodatek {\hyperref[\detokenize{docs/appendix:app-run}]{\sphinxcrossref{\DUrole{std,std-ref}{Jak uruchamiać kody książki}}}} wyjaśnia, jak postępować przy lokalnym wykonywaniu programów.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{\protect\(~\protect\)}

\sphinxAtStartPar
Książka wykonywalna, utworzona przez oprogramowanie \sphinxhref{https://beta.jupyterbook.org/intro.html}{Jupyter Book
2.0}, będące częścią
\sphinxhref{https://ebp.jupyterbook.org/en/latest/}{ExecutableBookProject}.
\end{sphinxadmonition}




\chapter{Wstęp}
\label{\detokenize{docs/intro:wstep}}\label{\detokenize{docs/intro::doc}}

\section{Cel wykładu}
\label{\detokenize{docs/intro:cel-wykladu}}
\sphinxAtStartPar
Celem kursu jest wyłożenie podstaw wszechobecnych sieci neuronowych z pomocą \sphinxhref{https://www.python.org/}{Pythona} {[}\hyperlink{cite.docs/conclusion:id5}{Bar16}, \hyperlink{cite.docs/conclusion:id3}{Gut16}, \hyperlink{cite.docs/conclusion:id2}{Mat19}{]}. Zarówno kluczowe pojęcia sieci neuronowych, jak i programy ilustrujące są wyjaśniane na bardzo podstawowym poziomie, niemal „licealnym”. Kody, bardzo proste, zostały szczegółowo opisane. Ponadto są utworzone bez użycia specjalistycznych bibliotek wyższego poziomu dla sieci neuronowych, co pomaga w lepszym zrozumieniu przedstawionych algorytmów i pokazuje, jak programować je od podstaw.

\begin{sphinxadmonition}{note}{Dla kogo jest ta książka?}

\sphinxAtStartPar
\sphinxstylestrong{Czytelnik może być zupełnym nowicjuszem, tylko w niewielkim stopniu zaznajomionym z Pythonem (a właściwie każdym innym językiem programowania) i Jupyterem.}
\end{sphinxadmonition}

\sphinxAtStartPar
Materiał obejmuje takie klasyczne zagadnienia, jak perceptron i jego najprostsze zastosowania, nadzorowane uczenie z propagacją wsteczną do klasyfikacji danych, uczenie nienadzorowane i klasteryzacja, sieci samoorganizujące się Kohonena oraz sieci Hopfielda ze sprzężeniem zwrotnym. Ma to na celu przygotowanie niezbędnego gruntu dla najnowszych i aktualnych postępów (nie omówionych tutaj) w sieciach neuronowych, takich jak uczenie głębokie, sieci konwolucyjne, sieci rekurencyjne, generatywne sieci przeciwników, uczenie ze wzmacnianiem itp.

\sphinxAtStartPar
W trakcie kursu nowicjuszom zostanie delikatnie przemycone kilka podstawowych programów w Pythonie. W kodach znajdują się objaśnienia i komentarze.

\begin{sphinxadmonition}{note}{Ćwiczenia}

\sphinxAtStartPar
Na końcu każdego rozdziału proponujemy kilka ćwiczeń, których celem jest zapoznanie czytelnika z poruszanymi tematami i kodami. Większość ćwiczeń polega na prostych modyfikacjach/rozszerzeniach odpowiednich fragmentów materiału wykładowego.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Literatura}

\sphinxAtStartPar
Podręczników i notatek do wykładów poświęconych zagadnieniom poruszanym na tym kursie jest niezliczona ilość, stąd autor nie będzie próbował przedstawiać nawet niepełnego spisu literatury. Przytaczamy tylko pozycje, na które może spojrzeć bardziej zainteresowany czytelnik.
\end{sphinxadmonition}

\sphinxAtStartPar
Z prostotą jako drogowskazem, wybór tematów był inspirowany szczegółowymi wykładami \sphinxhref{http://vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/IntroNeuralSyllabus.html}{Daniela Kerstena} w programie Mathematica, z internetowej książki \sphinxhref{https://page.mi.fu-berlin.de/rojas/neural/}{Raula Rojasa} (dostępna również w wersji drukowanej {[}\hyperlink{cite.docs/conclusion:id8}{FR13}{]}) oraz z punktu widzenia \sphinxstylestrong{fizyków} (jak ja!) z {[}\hyperlink{cite.docs/conclusion:id7}{MullerRS12}{]}.


\section{Inspiracja biologiczna}
\label{\detokenize{docs/intro:inspiracja-biologiczna}}
\sphinxAtStartPar
Inspiracją do opracowania matematycznych modeli obliczeniowych omawianych w tym kursie jest struktura biologiczna naszego układu nerwowego {[}\hyperlink{cite.docs/conclusion:id6}{KSJ+12}{]}. Centralny układ nerwowy (mózg) zawiera ogromną liczbę (\(\sim 10^{11}\)) \sphinxhref{https://human-memory.net/brain-neurons-synapses/}{neuronów}, które można postrzegać jako maleńkie  elementarne procesory. Otrzymują one sygnał poprzez \sphinxstylestrong{dendryty}, a jeśli jest on wystarczająco silny, jądro decyduje (obliczenie jest wykonane tutaj!) „wystrzelić” sygnał wyjściowy wzdłuż \sphinxstylestrong{aksonu}, gdzie jest on następnie przekazywany przez zakończenia aksonów do dendrytów innych neuronów. Połączenia aksonowo\sphinxhyphen{}dendryczne (połączenia \sphinxstylestrong{synaptyczne}) mogą być słabe lub silne, modyfikując przekazywany bodziec. Co więcej, siła połączeń synaptycznych może się zmieniać w czasie (\sphinxhref{https://en.wikipedia.org/wiki/Hebbian\_theory}{reguła Hebba} mówi nam, że połączenia stają się silniejsze, jeśli są używane wielokrotnie). W tym sensie neuron jest „programowalny”.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=450\sphinxpxdimen]{{neuron-structure}.jpg}
\caption{Biologiczny neuron (\sphinxurl{https://training.seer.cancer.gov/anatomy/nervous/tissue.html}).}\label{\detokenize{docs/intro:neuron-fig}}\end{figure}

\sphinxAtStartPar
Możemy zadać sobie pytanie, czy liczbę neuronów w mózgu rzeczywiście należy określać jako tak „ogromną”, jak się zwykle twierdzi. Porównajmy to do urządzeń obliczeniowych z układami pamięci. Liczba neuronów 10\(^{11}\) z grubsza odpowiada liczbie tranzystorów w chipie pamięci o pojemności 10 GB, co nie robi na nas specjalnego wrażenia, skoro w dzisiejszych czasach możemy kupić takie urządzenie za około 2\$.

\sphinxAtStartPar
Co więcej, prędkość przemieszczania się impulsów nerwowych, która jest wynikiem procesów elektrochemicznych, również nie jest imponująca. Najszybsze sygnały, takie jak te związane z pobudzaniem mięśni, przemieszczają się z prędkością do 120 m/s (osłonki mielinowe są niezbędne do ich osiągnięcia). Sygnały dotykowe osiągają około 80m/s, podczas gdy ból jest przenoszony ze stosunkowo bardzo małymi prędkościami 0,6m/s. To dlatego kiedy upuszczasz młotek na palec u nogi, czujesz to natychmiast, ale ból dociera do mózgu z opóźnieniem \textasciitilde{}1s, ponieważ musi pokonać odległość \textasciitilde{}1,5m. Z drugiej strony, w urządzeniach elektronicznych sygnał przemieszcza się w przewodach z prędkością rzędu prędkości światła, \(\sim 300000{\rm km/s}=3\times 10^{8}{\rm m/ s}\)!

\sphinxAtStartPar
W przypadku ludzi średni \sphinxhref{https://backyardbrains.com/experiments/reactiontime}{czas reakcji} wynosi 0,25 s na bodziec wizualny, 0,17 s na bodziec dźwiękowy i 0,15 s na dotyk. W ten sposób ustawienie progowego czasu dla falstartu w sprintach na 0,1 s jest bezpiecznie poniżej możliwej reakcji biegacza. Są to niezwykle powolne reakcje w porównaniu z odpowiedziami elektronicznymi.

\sphinxAtStartPar
Na podstawie zużycia energii przez mózg można oszacować, że neuron kory mózgowej \sphinxhref{https://aiimpacts.org/rate-of-neuron-firing/}{odpala} średnio raz na 6 sekund. Jest też mało prawdopodobne, aby przeciętny neuron odpalał częściej niż raz na sekundę. Pomnożenie tej szybkości wyzwalania przez liczbę wszystkich neuronów korowych, \(\sim 1.6 \times 10^{10}\), daje około 3 \(\times 10^{9}\) wyładowań/s w korze, czyli 3GHz. To jest cżęstotliwość This aktowania typowego chipa procesora! Jeśli więc odpalanie neuronu utożsamić z elementarnym obliczeniem, to tak określona moc mózgu jest z grubsza porównywalna z mocą standardowego procesora komputerowego.

\sphinxAtStartPar
Powyższe fakty wskazują, że z punktu widzenia naiwnych porównań z chipami krzemowymi ludzki mózg nie jest niczym szczególnym. Co zatem daje nam nasze wyjątkowe zdolności: niesłychanie wydajne rozpoznawanie wzorców wizualnych i dźwiękowych, myślenie, świadomość, intuicję, wyobraźnię? Odpowiedź wiąże się z niesamowicie rozbudowaną architekturą mózgu, w której każdy neuron (jednostka procesora) jest połączony poprzez synapsy średnio aż z 10000 (!) innych neuronów. Ta cecha sprawia, że ​​jest ona radykalnie inna i znacznie bardziej skomplikowana niż architektura składająca się z jednostki sterującej, procesora i pamięci w naszych komputerach (architektura \sphinxhref{https://en.wikipedia.org/wiki/Von\_Neumann\_architecture}{maszyny von Neumanna}) . Tam liczba połączeń jest rzędu liczby bitów pamięci, natomiast w ludzkim mózgu jest około \(10^{15}\) połączeń synaptycznych. Jak wspomniano, połączenia można „zaprogramować”, aby były silniejsze lub słabsze. Jeśli, dla prostego oszacowania, przybliżylibyśmy siłę połączenia tylko przez dwa stany synapsy, 0 lub 1, to całkowita liczba konfiguracji kombinatorycznych takiego systemu wynosiłaby \(2^{10^{15}}\) \sphinxhyphen{} „hiper\sphinxhyphen{}ogromna” liczba. Większość takich konfiguracji, oczywiście, nigdy nie jest realizowana w praktyce, niemniej jednak liczba możliwych stanów konfiguracyjnych mózgu lub „programów”, które może on realizować, jest naprawdę ogromna.

\sphinxAtStartPar
W ostatnich latach, wraz z rozwojem potężnych technik obrazowania, możliwe stało się mapowanie połączeń w mózgu z niespotykaną dotąd rozdzielczością, gdzie widoczne są pojedyncze wiązki nerwów. Wysiłki te są częścią {[}Projektu Human Connectome{]} (\sphinxurl{http://www.humanconnectomeproject.org}), którego ostatecznym celem jest dokłdne odwzorowanie architektury ludzkiego mózgu. W przypadku znacznie prostszej muszki owocowej, \sphinxhref{https://en.wikipedia.org/wiki/Drosophila\_connectome}{projekt drosophila connectome} jest bardzo zaawansowany.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=280\sphinxpxdimen]{{brain}.jpg}
\caption{Architektura mózgu włókna istoty białej (z projektu Human Connectome \sphinxhref{http://www.humanconnectomeproject.org/gallery/}{humanconnectomeproject.org})}\label{\detokenize{docs/intro:connectome-fig}}\end{figure}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Cecha „ogromnej łączności”, z miriadami neuronów służących jako równoległe procesory elementarne, sprawia, że ​​mózg jest zupełnie innym urządzeniem obliczeniowym niż \sphinxhref{https://en.wikipedia.org/wiki/Von\_Neumann\_architecture}{maszyna von Neumanna} (tj. nasze codzienne komputery).
\end{sphinxadmonition}


\section{Sieci feed\sphinxhyphen{}forward}
\label{\detokenize{docs/intro:sieci-feed-forward}}
\sphinxAtStartPar
Neurofizjologiczne badania mózgu dostarczają ważnych wskazówek dla modeli matematycznych stosowanych w sztucznych sieciach neuronowych (\sphinxstylestrong{ANN}). I odwrotnie, postępy w algorytmice ANN często przybliżają nas do zrozumienia, jak faktycznie może działać nasz „komputer mózgowy”!

\sphinxAtStartPar
Najprostsze sieci ANN to tak zwane sieci \sphinxstylestrong{feed forward}, zilustrowane w \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}. Składają się one z warstwy \sphinxstylestrong{wejściowej} (czarne kropki), która reprezentuje tylko dane cyfrowe, oraz warstw neuronów (kolorowych kropek). Liczba neuronów w każdej warstwie może być różna. Złożoność sieci i zadań, które może ona realizować, rośnie rzecz jasna wraz z liczbą warstw i liczbą neuronów.

\sphinxAtStartPar
W dalszej części tego rozdziału podamy, w dość skondensownej postaci, kilka ważnych definicji:

\sphinxAtStartPar
Sieci z jedną warstwą neuronów nazywane są sieciami \sphinxstylestrong{jednowarstwowymi}. Ostatnia warstwa (jasnoniebieskie kropki) nazywana jest \sphinxstylestrong{warstwą wyjściową}. W sieciach wielowarstwowych (więcej niż jedna warstwa neuronowa) warstwy neuronowe poprzedzające warstwę wyjściową (fioletowe kropki) nazywane są \sphinxstylestrong{warstwami pośrednimi}. Jeśli liczba warstw jest duża (np. 64, 128, …), mamy do czynienia ze stosowanymi od niedawna „przełomowymi” \sphinxstylestrong{głębokimi sieciami}.

\sphinxAtStartPar
Neurony w różnych warstwach nie muszą działać w ten sam sposób, w szczególności neurony wyjściowe mogą zachowywać się inaczej niż pośrednie.

\sphinxAtStartPar
Sygnał z wejścia wędruje po wskazanych strzałkami łączach (krawędziach, połączeniach synaptycznych) do neuronów w kolejnych warstwach. W sieciach typu feed\sphinxhyphen{}forward, jak ta na \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}, sygnał może poruszać się tylko do przodu (na rysunku od lewej do prawej): od wejścia do pierwszej warstwy neuronowej, od pierwszej do drugiej, i tak dalej, aż do osiągnięcia wyjścia. Nie jest dozwolone cofanie się do poprzednich warstw ani równoległa propagacja pomiędzy neuronami tej samej warstwy. Byłaby to wówczas sieć z \sphinxstylestrong{powracaniem}, o czym nieco mówimy w rozdziale {\hyperref[\detokenize{docs/som:lat-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Lateral inhibition}}}}.

\sphinxAtStartPar
Jak szczegółowo opisujemy w kolejnych rozdziałach, wędrujący sygnał jest odpowiednio \sphinxstylestrong{przetwarzany} przez neurony, stąd urządzenie wykonuje obliczenia: wejście jest przekształcane w wyjście.

\sphinxAtStartPar
W przykładowej sieci \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} każdy neuron z poprzedniej warstwy jest połączony z każdym neuronem w następnej warstwie. Takie sieci ANN są nazywane \sphinxstylestrong{w pełni połączonymi}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{feed_f}.png}
\caption{Przykładowa, w pełni połączona sztuczna sieć neuronowa typu feed\sphinxhyphen{}forward. Kolorowe plamy reprezentują neurony, a krawędzie uskazują połączenia synaptyczne. Sygnał rozchodzi się od wejścia (czarne kropki), przez neurony w kolejnych warstwach pośrednich (ukrytych) (fioletowe kropki), do warstwy wyjściowej (jasnoniebieskie kropki). Siła połączeń jest kontrolowana przez wagi (hiperparametry) przypisane do krawędzi.}\label{\detokenize{docs/intro:ffnn-fig}}\end{figure}

\sphinxAtStartPar
Jak omówimy bardziej szczegółowo późniwj, każda krawędź (połączenie synaptyczne) w sieci ma pewną „siłę” opisaną liczbą o nazwie \sphinxstylestrong{waga} (wagi są również określane jako \sphinxstylestrong{hiperparametry}). Nawet bardzo małe w pełni połączone sieci, takie jak ta z \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}, mają bardzo wiele połączeń (tutaj 30), stąd zawierają dużo hiperparametrów. Tak więc, choć czasami wyglądają niewinnie, ANN są w rzeczywistości bardzo złożonymi systemami wieloparametrycznymi. Co więcej, kluczową cechą jest tutaj nieliniowość odpowiedzi neuronów, co omawiamy w kolejnym rozdziale {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Neuron MCP}}}}.


\section{Dlaczego Python}
\label{\detokenize{docs/intro:dlaczego-python}}
\sphinxAtStartPar
Wybór języka \sphinxhref{https://en.wikipedia.org/wiki/Python\_(programming\_language)}{Python} dla prościutkich kodów tego kursu prawie nie wymaga wyjaśnienia. Zacytujmy tylko \sphinxhref{https://en.wikipedia.org/wiki/Tim\_Peters\_(software\_engineer)}{Tima Petersa}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Piękne jest lepsze niż brzydkie.

\item {} 
\sphinxAtStartPar
Jawne jest lepsze niż niejawne.

\item {} 
\sphinxAtStartPar
Proste jest lepsze niż złożone.

\item {} 
\sphinxAtStartPar
Złożone jest lepsze niż skomplikowane.

\item {} 
\sphinxAtStartPar
Liczy się czytelność.

\end{itemize}

\sphinxAtStartPar
Według \sphinxhref{https://developer-tech.com/news/2021/apr/27/slashdata-javascript-python-boast-largest-developer-communities/}{SlashData}, na świecie jest obecnie ponad 10 milionów programistów używających Pythona, zaraz po jezyku JavaScript (\textasciitilde{}14 milionów). W szczególności Python okazuje się bardzo praktyczny w zastosowaniach do sieci ANN.


\subsection{Importowane pakiety}
\label{\detokenize{docs/intro:importowane-pakiety}}
\sphinxAtStartPar
W trakcie tego kursu używamy kilku standardowych pakietów bibliotecznych Pythona do obliczeń numerycznych, wykresów itp. Jak podkreśliliśmy, nie korzystamy z żadnych bibliotek specjalnie dedykowanych sieciom neuronowym. Notebook każdego wykładu zaczyna się od zaimportowania niektórych z tych bibliotek:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}              \PYG{c+c1}{\PYGZsh{} numerical}
\PYG{k+kn}{import} \PYG{n+nn}{statistics} \PYG{k}{as} \PYG{n+nn}{st}         \PYG{c+c1}{\PYGZsh{} statistics}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt} \PYG{c+c1}{\PYGZsh{} plotting}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib} \PYG{k}{as} \PYG{n+nn}{mpl}        \PYG{c+c1}{\PYGZsh{} plotting}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{cm} \PYG{k}{as} \PYG{n+nn}{cm}      \PYG{c+c1}{\PYGZsh{} contour plots }

\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits}\PYG{n+nn}{.}\PYG{n+nn}{mplot3d}\PYG{n+nn}{.}\PYG{n+nn}{axes3d} \PYG{k+kn}{import} \PYG{n}{Axes3D}   \PYG{c+c1}{\PYGZsh{} 3D plots}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{display}\PYG{p}{,} \PYG{n}{Image}\PYG{p}{,} \PYG{n}{HTML} \PYG{c+c1}{\PYGZsh{} display imported graphics}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{\sphinxstylestrong{neural} package}

\sphinxAtStartPar
Tworzone podczas tego kursu funkcje, które są później wielokrotnie używane, są umieszczane w pakiecie prywatnej biblioteki \sphinxstylestrong{neural}, opisanym w załączniku {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Pakiet neural}}}}.
\end{sphinxadmonition}

\sphinxAtStartPar
Celem kompatybilności z środowikiem Google Colab, pakiet  \sphinxstylestrong{neural} importowany jest z repozytorium w następujący sposób:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-output}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}\PYG{n+nn}{.}\PYG{n+nn}{path} 

\PYG{n}{isdir} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{isdir}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lib\PYGZus{}nn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} check whether \PYGZsq{}lib\PYGZus{}nn\PYGZsq{} exists}

\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{isdir}\PYG{p}{:}
   \PYG{o}{!}git clone https://github.com/bronwojtek/lib\PYGZus{}nn.git \PYGZsh{} cloning the library from github

\PYG{k+kn}{import} \PYG{n+nn}{sys}                     
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./lib\PYGZus{}nn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} 

\PYG{k+kn}{from} \PYG{n+nn}{neural} \PYG{k+kn}{import} \PYG{o}{*}            \PYG{c+c1}{\PYGZsh{} importing my library package}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Więcej informacji można znaleźć w dodatku {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Pakiet neural}}}}.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Dla zwięzłości prezentacji, niektóre zbędne (np. import bibliotek) lub nieistotne fragmenty kodu są obecne tylko w notebookach Jupytera (do pobrania) i nie są ukazywane w książce. Dzięki temu tekst jest krótszy i czytelny.
\end{sphinxadmonition}


\chapter{Neuron MCP}
\label{\detokenize{docs/mcp:neuron-mcp}}\label{\detokenize{docs/mcp:mcp-lab}}\label{\detokenize{docs/mcp::doc}}

\section{Definicja}
\label{\detokenize{docs/mcp:definicja}}
\sphinxAtStartPar
Potrzebujemy podstawowego składnika ANN: sztucznego neuronu. Pierwszy model matematyczny pochodzi od Warrena McCullocha i Waltera Pittsa (MCP){[}\hyperlink{cite.docs/conclusion:id9}{MP43}{]}, którzy zaproponowali go w 1942 roku, a więc na samym początku ery komputerów elektronicznych podczas II wojny światowej. Neuron MCP przedstawiony na \hyperref[\detokenize{docs/mcp:mcp1-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp1-fig}}} jest podstawowym składnikiem wszystkich ANN omawianych w tym kursie. Jest zbudowany na bardzo prostych ogólnych zasadach, inspirowanych przez neuron biologiczny:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sygnał wchodzi do jądra przez dendryty z innych neuronów.

\item {} 
\sphinxAtStartPar
Połączenie synaptyczne dla każdego dendrytu może mieć inną (i regulowaną) siłę (wagę).

\item {} 
\sphinxAtStartPar
W jądrze sygnał ważony ze wszystkich dendrytów jest sumowany i oznaczony jako \(s\).

\item {} 
\sphinxAtStartPar
Jeżeli sygnał \(s\) jest silniejszy niż pewien zadany próg, to neuron odpala sygnał wzdłuż aksonu, w przeciwnym przypadku pozostaje pasywny.

\item {} 
\sphinxAtStartPar
W najprostszej realizacji, siła odpalanego sygnału ma tylko dwa możliwe poziomy: włączony lub wyłączony, tj. 1 lub 0. Nie są potrzebne wartości pośrednie.

\item {} 
\sphinxAtStartPar
Akson łączy się z dendrytami innych neuronów, przekazując im swój sygnał.

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=320\sphinxpxdimen]{{mcp-1a}.png}
\caption{Neuron MCP: \(x_i\) oznaczają wejście, \(w_i\)  wagi, \(s\) zsumowany sygnał, \(b\) próg, a \(f(s;b)\) reprezentuje funkcję aktywacji, dającą wyjście \(y =f(s;b)\). Niebieski owal otacza cały neuron, jak np. w notacji \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}.}\label{\detokenize{docs/mcp:mcp1-fig}}\end{figure}

\sphinxAtStartPar
Przekładając to na matematyczną receptę, przypisuje się komórkom wejściowym liczby \(x_1, x_2 \dots, x_n\) (punkt danych wejściowych). Siła połączeń synaptycznych jest kontrolowana przez \sphinxstylestrong{wagi} \(w_i\). Następnie łączny sygnał jest zdefiniowany jako suma ważona
\begin{equation*}
\begin{split}s=\sum_{i=1}^n x_i w_i.\end{split}
\end{equation*}
\sphinxAtStartPar
Sygnał staje się argumentem \sphinxstylestrong{funkcji aktywacji}, która w najprostszym przypadku przybiera postać funkcji schodkowej
\begin{equation*}
\begin{split}f(s;b) = \left \{ \begin{array}{l} 1 {\rm ~dla~}s \ge b \\ 0 {\rm ~dla~}s < b \end{array} \right .\end{split}
\end{equation*}
\sphinxAtStartPar
Gdy łączny sygnał \(s\) jest większy niż próg \(b\), jądro odpala. tj. sygnał idący wzdłuż aksonu wynosi 1. W przeciwnym przypadku wartość generowanego sygnału wynosi 0 (brak odpalenia). Właśnie tego potrzebujemy, aby naśladować biologiczny prototyp!

\sphinxAtStartPar
Istnieje wygodna konwencja, która jest często używana. Zamiast oddzielać próg od danych wejściowych, możemy traktować te liczby wrównoważny sposób. Warunek odpalenia może być trywialnie przekształcony jako
\begin{equation*}
\begin{split}
s \ge b \to s-b \ge 0 \to \sum_{i=1}^n x_i w_i - b \ge 0 \to \sum_{i=1}^n x_i w_i +x_0 w_0 \ge 0
\to \sum_{i=0}^n x_i w_i \ge 0,
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(x_0=1\) i \(w_0=-b\). Innymi słowy, możemy traktować próg jako wagę na krawędzi połączonej z dodatkową komórką z wejściem zawsze ustawionym na 1. Ta notacja jest pokazana na \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}. Teraz funkcja aktywacji wynosi po prostu
\begin{equation}\label{equation:docs/mcp:eq-f}
\begin{split}f(s) = \left \{ \begin{array}{l} 1 {\rm ~for~} s \ge 0 \\ 0 {\rm ~for~} s < 0 \end{array} \right .,\end{split}
\end{equation}
\sphinxAtStartPar
ze wskaźnikiem sumowania w \(s\) zaczynającym się \(0\):
\begin{equation}\label{equation:docs/mcp:eq-f0}
\begin{split}s=\sum_{i=0}^n x_i w_i = x_0 w_0+x_1 w_1 + \dots + x_n w_n.\end{split}
\end{equation}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=320\sphinxpxdimen]{{mcp-2a}.png}
\caption{Alternatywna, bardziej jednorodna representacja neuronu MCP, z \(x_0=1\) i \(w_0=-b\).}\label{\detokenize{docs/mcp:mcp2-fig}}\end{figure}

\sphinxAtStartPar
Wagi \(w_0=-b,w_1,\dots,w_n\) są ogólnie określane jako \sphinxstylestrong{hiperparametry}. Określają one funkcjonalność neuronu MCP i mogą ulegać zmianie podczas procesu uczenia się (trenowania) sieci (patrz kolejne rozdziały). Natomiast są one ustalone podczas używania już wytrenowanej sieci na określonej próbce danych wejściowych.

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Istotną właściwością neuronów w ANN jest \sphinxstylestrong{nieliniowość} funkcji aktywacji. Bez tej cechy neuron MCP reprezentowałby po prostu iloczyn skalarny, a (wielowarstwowe) sieci feed\sphinxhyphen{}forward sprowadzałyby się do trywialnego mnożenia macierzy.
\end{sphinxadmonition}


\section{Neuron MCP w Pythonie}
\label{\detokenize{docs/mcp:neuron-mcp-w-pythonie}}\label{\detokenize{docs/mcp:mcp-p-lab}}
\sphinxAtStartPar
Zaimplementujemy teraz model matematyczny neuronu MCP w Pythonie. Rzecz jasna, potrzebujemy tablic (wektorów), które są reprezentowane jako

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{w} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{2.5}\PYG{p}{]}
\PYG{n}{x}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1, 3, 7]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
i (\sphinxstylestrong{co ważne}) są indeksowane począwszy od 0, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważ, że wypisanie nazwy zmiennej na końcu komórki powoduje wydrukowanie jej zawartości.

\sphinxAtStartPar
Funkcje biblioteczne numpy mają przedrostek \sphinxstylestrong{np}, który jest aliasem podanym podczas importu. Funkcje te działają \sphinxstyleemphasis{dystrybucyjnie} na tablice, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.84147098, 0.14112001, 0.6569866 ])
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
co jest bardzo wygodną własnością przy programowaniu. Mamy też do dyspozycji iloczyn skalarny \(x \cdot w = \sum_i x_i w_i\), którego używamy do określenia sygnału \(s\) wchodzącego do neuronu MCP:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
21.5
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie musimy zdefiniować funkcję aktywacji neuronu, która w najprostszej postaci jest funkcją schodkową \eqref{equation:docs/mcp:eq-f}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} step function (in the neural library)}
     \PYG{k}{if} \PYG{n}{s} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} condition satisfied}
        \PYG{k}{return} \PYG{l+m+mi}{1}
     \PYG{k}{else}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} otherwise}
        \PYG{k}{return} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Funkcja znajduje się też a pakiecie \sphinxstylestrong{neural}, zob. {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{dodatek}}}}. Dla wzrokowców, wykres funkcji schodkowej jest następujący:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} set the size and resolution of the figure}

\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} array of 100+1 equally spaced points in [\PYGZhy{}2, 2]}
\PYG{n}{fs} \PYG{o}{=} \PYG{p}{[}\PYG{n}{step}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)} \PYG{k}{for} \PYG{n}{z} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} corresponding array of function values}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{signal s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} axes labels}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response f(s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} plot title}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{mcp_25_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Ponieważ z definicji \(x_0=1\), nie chcemy przekazywać tej wartości w argumentach funkcji modelujących neuron MCP. Będziemy zatem dodawać \(x_0=1\) na początku danych wejściowych, jak w tym przykładzie:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} insert 1 in x at position 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([1, 5, 7])
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jesteśmy teraz gotowi by zdefiniwać {\hyperref[\detokenize{docs/mcp:mcp1-fig}]{\sphinxcrossref{\DUrole{std,std-ref}{neuron MCP}}}}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neuron}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} (in the neural library)}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}                 }
\PYG{l+s+sd}{    MCP neuron}

\PYG{l+s+sd}{    x: array of inputs  [x1, x2,...,xn]}
\PYG{l+s+sd}{    w: array of weights [w0, w1, w2,...,wn]}
\PYG{l+s+sd}{    f: activation function, with step as default}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}} 
    \PYG{k}{return} \PYG{n}{f}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} insert x0=1 into x, output f(x.w)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Starannie umieszczamy stosowne komentarze w potrójnych cudzysłowach, aby w razie potrzeby móc uzyskać pomoc:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{help}\PYG{p}{(}\PYG{n}{neuron}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Help on function neuron in module \PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}:

neuron(x, w, f=\PYGZlt{}function step at 0x7fc98dc57550\PYGZgt{})
    MCP neuron
    
    x: array of inputs  [x1, x2,...,xn]
    w: array of weights [w0, w1, w2,...,wn]
    f: activation function, with step as default
    
    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważ, że funkcja \sphinxstylestrong{f} jest argumentem \sphinxstylestrong{neuron}u. Argument ten jest domyślnie ustawiony jako \sphinxstylestrong{step}, więc nie musi być obecny na liście argumentów. Przykładowe użycie z \(x_1=3\), \(w_0=-b=-2\) i \(w_1=1\) to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jak widzimy, w tym przypadku neuron odpalił, poniważ \(s=1*(-2)+3*1>0\).

\sphinxAtStartPar
Poniżej pokazujemy, jak neuron działa na daną wejściową \(x_1\) wziętą z przedziału \([-2,2]\). Zmieniamy również wartość progu, aby zilustrować jego rolę: jeśli sygnał \(x_1 w_1\) jest większy niż \(b=-x_0\), neuron odpala.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)} 

\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{fs1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} more function on one plot}
\PYG{n}{fs0} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}
\PYG{n}{fsm12} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Change of bias}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=\PYGZhy{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs0}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fsm12}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=1/2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} legend}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{mcp_35_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Kiedy znak wagi \(w_1\) jest ujemny, dostajemy \sphinxstylestrong{odwrotne} zachowanie: neuron odpala dla \(x_1 |w_1| < w_0\):

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{mcp_37_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Począwszy od teraz, dla zwięzłości prezentacji, ukrywamy niektóre komórki kodu o powtarzającej się strukturze. Czytelnik może znaleźć pełny kod w oryginalnych notatnikach Jupytera.
\end{sphinxadmonition}

\sphinxAtStartPar
Trzeba przyznać, że w ostatnim przykładzie odchodzi się od biologicznego wzorca, ponieważ ujemne wagi nie są możliwe do zrealizowania w biologicznym neuronie. Przyjeta swoboda wzbogaca jednak model matematyczny, który w oczywisty sposób można budować bez ograniczeń biologicznych.


\section{Funkcje logiczne}
\label{\detokenize{docs/mcp:funkcje-logiczne}}\label{\detokenize{docs/mcp:bool-sec}}
\sphinxAtStartPar
Skonstruowawszy neuronu MCP w Pythonie możemy zadać pytanie: \sphinxstyleemphasis{Jaka jest najprostsze (ale wciąż nietrywialne) zastosowanie, w którym możemy go użyć?} Są to {[}funkcje logiczne{]}(\sphinxurl{https://en} .wikipedia.org/wiki/Boolean\_function) lub sieci logiczne utworzone za pomocą sieci neuronów MCP.

\sphinxAtStartPar
Funkcje logiczne z definicji mają argumenty i wartości zawierające się w zbiorze \(\{ 0,1 \}\) lub \{Prawda, Fałsz\}.

\sphinxAtStartPar
Na rozgrzewkę zacznijmy od zgadywania, gdzie bierzemy neuron o wagach \(w=[w_0,w_1,w_2]=[-1,0.6,0.6]\) (dlaczego nie). Oznaczmy też \(x_1=p\), \(x_2=q\), zgodnie z tradycyjną notacją zmiennych logicznych, gdzie \(p,q \in \{0,1\}\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q n(p,q)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} print space}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} loop over p}
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over q}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print all cases}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q n(p,q)

0 0  0
0 1  0
1 0  0
1 1  1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Natychmiast rozpoznajemy w powyższym wyniku tabelkę logiczną dla koniunkcji, \(n(p,q)=p \land q\) lub logicznej operacji \sphinxstylestrong{AND}. Jest zupełnie jasne, dlaczego tak działa nasz neuron. Warunek odpalenia \(n(p,q)=1\) wynosi \(-1+p*0.6+q*0.6 \ge 0\) i jest spełniony wtedy i tylko wtedy, gdy \(p=q=1\), co jest definicją koniunkcji logicznej. Oczywiście moglibyśmy użyć tutaj 0.7 zamiast 0.6, lub ogólnie \(w_1\) i \(w_2\) takie, że \(w_1<1, w_2<1, w_1+w_2 \ge 1\). W terminologii elektronicznej obecny neuron możemy więc nazwać \sphinxstylestrong{bramką AND}.

\sphinxAtStartPar
Możemy w ten sposób zdefiniować funkcję

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
W podobny sposób możemy zdefiniować inne funkcje logiczne (bramki logiczne) dwóch zmiennych logicznych. W szczególności bramka NAND (negacja koniunkcji) i bramka OR (alternatywa) są realizowane poprzez następujące neurony MCP:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:}   \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{1.2}\PYG{p}{,}\PYG{l+m+mf}{1.2}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Odpowiadają następującym tabelkom logicznym

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q  NAND OR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q  NAND OR

0 0   1   0
0 1   1   1
1 0   1   1
1 1   0   1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Problem z bramką XOR}
\label{\detokenize{docs/mcp:problem-z-bramka-xor}}
\sphinxAtStartPar
Bramka XOR, lub \sphinxstylestrong{alternatywa wykluczjąca}, jest zdefiniowana za pomocą następującej tabelki logicznej:
\begin{equation*}
\begin{split}
\begin{array}{ccc}
p & q & p \oplus q \\
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0
\end{array}
\end{split}
\end{equation*}
\sphinxAtStartPar
Jest to jedna z możliwych funkcji binarnych dwóch argumentów (w sumie mamy 16 różnych funkcji tego rodzaju, dlaczego?). Moglibyśmy teraz próbować dobrać wagi w naszym neuronie, aby zachowywał się jak bramka XOR, ale jesteśmy skazani na porażkę. Oto jej powód:

\sphinxAtStartPar
Z pierwszego wiersza powyższej tabelki wynika, że dla wejścia 0, 0 neuron nie powinien odpalić. Stąd

\sphinxAtStartPar
\(w_0 + 0* w_1 + 0*w_2 <0\) lub \(-w_0>0\).

\sphinxAtStartPar
W przypadku wierszy 2 i 3 neuron musi odpalić, zatem

\sphinxAtStartPar
\(w_0+w_2 \ge 0\) i \(w_0+w_1 \ge 0\).

\sphinxAtStartPar
Dodając stronami te trzy uzyskane nierówności otrzymujemy \(w_0+w_1+w_2 >0\). Jednak czwarty rząd tabelki daje
\(w_0+w_1+w_2<0\) (brak odpalenia), więc uzyskujemy sprzeczność. Dlatego nie istnieje taki wybor \(w_0, w_1, w_2\), aby neuron działał jak bramka XOR!

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Pojedynczy neuron MCP nie może działać jak bramka \sphinxstylestrong{XOR}.
\end{sphinxadmonition}


\subsection{XOR ze złożenia bramek AND, NAND i OR}
\label{\detokenize{docs/mcp:xor-ze-zlozenia-bramek-and-nand-i-or}}
\sphinxAtStartPar
Można rozwiązać problem konstrukcji bramki XOR, składając trzy neurony MCP, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurXOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neurAND}\PYG{p}{(}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{,}\PYG{n}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q XOR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} 
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurXOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powyższa konstrukcja odpowiada prostj sieci \hyperref[\detokenize{docs/mcp:xor-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:xor-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=260\sphinxpxdimen]{{xor}.png}
\caption{Bramka XOR złożona z neuronów NAND, OR i AND.}\label{\detokenize{docs/mcp:xor-fig}}\end{figure}

\sphinxAtStartPar
Zauważmy, że po raz pierwszy mamy tu do czynienia z siecią posiadającą warstwę pośrednią, składającą się z neuronów NAND i OR. Ta warstwa jest nieodzowna do budowy bramki XOR.


\subsection{Bramka XOR złożona z bramek NAND}
\label{\detokenize{docs/mcp:bramka-xor-zlozona-z-bramek-nand}}
\sphinxAtStartPar
W ramach teorii sieci logicznych udowadnia się, że dowolna sieć (lub dowolna funkcja logiczna) może składać się wyłącznie z bramek NAND lub wyłącznie z bramek NOR. Mówi się, że bramki NAND (lub NOR) są \sphinxstylestrong{zupełne}. W szczególności bramka XOR może być skonstruowana jako

\sphinxAtStartPar
{[} p NAND ( p NAND q ) {]} NAND {[} q NAND ( p NAND q ) {]},

\sphinxAtStartPar
co możemy napisać w Pythonie jako

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{nXOR}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{j}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q XOR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} 
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{nXOR}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Dowodzi się, że sieci logiczne są zupełne w sensie \sphinxhref{https://en.wikipedia.org/wiki/Church-Turing\_thesis}{Churcha\sphinxhyphen{}Turinga}, tj. (jeśli są wystarczająco duże) mogą wykonać każde możliwe obliczenie. Ta własność jest bezpośrednio przenoszona na sieci ANN. Historycznie, było to podstawowe odkrycie przełomowego artykułu MCP {[}\hyperlink{cite.docs/conclusion:id9}{MP43}{]}.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Wniosek}

\sphinxAtStartPar
Dostatecznie duże ANN mogą wykonac każde obliczenie!
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/mcp:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}

\sphinxAtStartPar
Skonstruuj (wszystko w Pythonie)
\begin{itemize}
\item {} 
\sphinxAtStartPar
bramkę realizująca koniunkcję kilku zmiennych logicznych;

\item {} 
\sphinxAtStartPar
bramki NOT, NOR;

\item {} 
\sphinxAtStartPar
bramki OR, AND i NOT poprzez \sphinxhref{https://en.wikipedia.org/wiki/NAND\_logic}{złożenie bramek NAND};

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Adder\_(electronics)}{pół sumator i pełny sumator},

\end{itemize}

\sphinxAtStartPar
jako sieci neuronów MCP.
\end{sphinxadmonition}


\chapter{Modele pamięci}
\label{\detokenize{docs/memory:modele-pamieci}}\label{\detokenize{docs/memory::doc}}

\section{Pamieć skojarzeniowa (heteroasocjacyjna)}
\label{\detokenize{docs/memory:pamiec-skojarzeniowa-heteroasocjacyjna}}\label{\detokenize{docs/memory:het-lab}}

\subsection{Skojarzenia par}
\label{\detokenize{docs/memory:skojarzenia-par}}
\sphinxAtStartPar
Przechodzimy teraz do dalszych ilustracji elementarnych możliwości ANN, opisujących dwa bardzo proste modele pamięci oparte na algebrze liniowej, uzupełnione o (nieliniowe) filtrowanie. Mówiąc o pamięci, na miejscu jest słowo przestrogi. Mamy tu do czynienia z dość uproszczonymi narzędziami, które są dalekie od złożonego i dotychczas niezrozumiałego mechanizmu pamięci działającego w naszym mózgu. Obecne rozumienie jest takie, że te mechanizmy obejmują sprzężenie zwrotne w sieciach, co wykracza poza rozważane tutaj sieci typu feed\sphinxhyphen{}forward.

\sphinxAtStartPar
Pierwszy rozważany model dotyczy tzw. pamięci \sphinxstylestrong{heterasocjacyjnej}, w której niektóre obiekty (tutaj graficzne symbole bitmapowe) są kojarzone w pary. Dla konkretnego przykładu bierzemy zbiór pięciu symboli graficznych \{A, a, I, i, Y\} i definiujemy dwa skojarznia par: A \(\leftrightarrow\) a oraz I \(\leftrightarrow\) i, czyli pomiędzy różnymi (hetero) symbolami. Symbol Y pozostaje nieskojarzony.

\sphinxAtStartPar
Symbole są zdefiniowane jako 2\sphinxhyphen{}wymiarowe (\(12 \times 12\)) tablice pikseli, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{n}{A} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}     
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}  
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Pozostałe symbole są zdefiniowane podobnie.

\sphinxAtStartPar
Użyjemy standardowego pakietu do rysowania, zaimportowanego wcześniej. Cały zestaw naszych symboli wygląda jak poniżej, z kolorem żółtym=1 i fioletowym=0:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sym}\PYG{o}{=}\PYG{p}{[}\PYG{n}{A}\PYG{p}{,}\PYG{n}{a}\PYG{p}{,}\PYG{n}{ii}\PYG{p}{,}\PYG{n}{I}\PYG{p}{,}\PYG{n}{Y}\PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} array of symbols, numbered from 0 to 4}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} figure with horizontal and vertical size}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} loop over 5 figure panels, i is from 1 to 5}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} panels, numbered from 1 to 5}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} no axes}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{sym}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} plot symbol, numbered from 0 to 4}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_13_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{warning}{Ostrzeżenie:}
\sphinxAtStartPar
W Pythonie zakres range \((i,j)\) zawiera \(i\), ale nie obejmuje \(j\), tj. równa się tablicy \([i, i+1, \dots, j-1]\). Ponadto range\((i)\) obejmuje \(0, 1, \dots, i-1\). Różni się to od konwencji przyjętej w niektórych innych językach programowania.
\end{sphinxadmonition}

\sphinxAtStartPar
Wygodniej jest pracować nie z powyższymi tablicami dwuwymiarowymi, ale z jednowymiarowymi wektorami uzyskanymi za pomocą tzw. procedury \sphinxstylestrong{spłaszczania}, gdzie macierz jest ,,pocięta” wzdłuż swoich wierszy, złożonych w wektor. Na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} a matrix}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)}                            
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}                    \PYG{c+c1}{\PYGZsh{} matrix flattened into a vector   }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[1 2 3]
 [0 4 0]
 [3 2 7]]
[1 2 3 0 4 0 3 2 7]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
A zatem przeprowadzamy spłaszczenie na naszym zestawie symboli

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fA}\PYG{o}{=}\PYG{n}{A}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fa}\PYG{o}{=}\PYG{n}{a}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fi}\PYG{o}{=}\PYG{n}{ii}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fI}\PYG{o}{=}\PYG{n}{I}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fY}\PYG{o}{=}\PYG{n}{Y}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
aby otrzymać, np.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_20_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_21_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Zaletą pracy z wektorami jest to, że możemy użyć wbudowanego iloczynu skalarnego. Zauważmy, że tutaj iloczyn skalarny wektorów odpowiadających dwóm symbolom jest po prostu równy liczbie wspólnych żółtych pikseli. Na przykład dla spłaszczonych symboli A oraz i, narysowanych powyżej, mamy tylko dwa wspólne żółte piksele:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
2
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jasne jest, że można użyć iloczynu skalarnego jako miary podobieństwa między symbolami. Aby poniżej pzedstawiony model pamięci skojarzeniowej działał, symbole nie powinny być zbyt podobne, ponieważ mogą być wtedy „mylone”.


\subsection{Macierz pamięci}
\label{\detokenize{docs/memory:macierz-pamieci}}
\sphinxAtStartPar
Następna koncepcja algebraiczna, której potrzebujemy, to \sphinxstylestrong{iloczyn zewnętrzny}. Dla dwóch wektorów \(v\) i \(w\) jest on zdefiniowany jako \(v w^T = v \otimes w\) (w przeciwieństwie do iloczynu skalarnego, gdzie \(w^T v = w \cdot v\)). Tutaj \(T\) oznacza transpozycję. Wynikiem jest macierz z liczbą wierszy równą długości \(v\) i liczbą kolumn równą długości \(w\).

\sphinxAtStartPar
Na przykład dla
\begin{equation*}
\begin{split} v = \left ( \begin{array}{c} v_1 \\ v_2 \\v_3 \end{array}  \right ), \;\;\;\; w = \left ( \begin{array}{c} w_1 \\ w_2 \end{array}  \right ), \end{split}
\end{equation*}
\sphinxAtStartPar
mamy
\begin{equation*}
\begin{split} 
v \otimes w = v w^T=
\left ( \begin{array}{c} v_1 \\ v_2 \\v_3 \end{array}  \right ) (w_1,w_2)
= \left ( \begin{array}{cc} v_1 w_1 & v_1 w_2 \\ v_2 w_1 & v_2 w_2 \\v_3 v_1 & v_3 w_2 \end{array}  \right ).
\end{split}
\end{equation*}
\sphinxAtStartPar
(przypomnij sobie z algebry, że mnożymy „wiersze przez kolumny”). w \sphinxstylestrong{numpy}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} outer product of two vectors}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[ 2  7]
 [ 4 14]
 [ 6 21]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie konstruujemy \sphinxstylestrong{macierz pamięci} potrzebną do modelowania pamięci heteroasocjacyjnej. Załóżmy najpierw, dla uproszczenia notacji, że mamy tylko dwa skojarzenia: \(a \to A\) i \(b \to B\). Niech
\begin{equation*}
\begin{split}M = A a^T/a\cdot a + B b^T/b\cdot b.\end{split}
\end{equation*}
\sphinxAtStartPar
Wówczas
\begin{equation*}
\begin{split}M a=  A + B \, a\cdot b /b \cdot a, \end{split}
\end{equation*}
\sphinxAtStartPar
i jeśli \(a\) and \(b\) byłyby \sphinxstylestrong{ortogonalne}, tj. \(a \cdot b =0\), to

\sphinxAtStartPar
\( M a =  A\),

\sphinxAtStartPar
dając dokładne skojarzenie. Podobnie otrzymalibyśmy \(Mb = B\). Ponieważ jednak w ogólnym przypadku wektory nie są dokładnie ortogonalne, generowany jest pewien błąd \(B \, b \cdot a/a \cdot a\) (dla asocjacji \(a\)). Zwykle jest on mały, jeśli liczba pikseli w naszych symbolach jest duża, a symbole są, ogólnie rzecz biorąc, niezbyt do siebie podobne (nie mają zbyt wielu wspólnych pikseli). Jak wkrótce zobaczymy, pojawiający się błąd można wydajnie „odfiltrować”  odpowiednią funkcją aktywacji neuronów.

\sphinxAtStartPar
Wracając do naszego konkretnego przypadku, potrzebujemy zatem czterech członów w \(M\), ponieważ
\(a \to A\), \(A\to a\), \(I \to i\) oraz \(i \to I\):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{M}\PYG{o}{=}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}
   \PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{\PYGZsh{} associated pairs}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Teraz, jako test jak to działa, dla każdego spłaszczonego symbolu \(s\) obliczamy \(Ms\). Wynikiem jest wektor, który chcemy przywrócić do postaci tablicy pikseli \(12\times 12\). Operacją odwrotną do spłaszczania w Pythonie jest \sphinxstylestrong{reshape}. Na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test vector}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tt}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} cutting into 2 rows of length 2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[1 2]
 [3 5]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dla naszych wektorów mamy

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Yp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fY}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} we also try the unassociated symbol Y}

\PYG{n}{symp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{Ap}\PYG{p}{,}\PYG{n}{ap}\PYG{p}{,}\PYG{n}{Ip}\PYG{p}{,}\PYG{n}{ip}\PYG{p}{,}\PYG{n}{Yp}\PYG{p}{]}          \PYG{c+c1}{\PYGZsh{} array of associated symbols}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
W przypadku skojarzenia z A (które powinno wynosić a) procedura daje nastepujący wynik (stosujemy tu dla ładniejszego wydruku zaokrąglanie do 2 cyfr dziesiętnych poprzez \sphinxstylestrong{np.round})

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{Ap}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} pixel map for the association of the symbol A}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.25 0.85 0.25 0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.85 0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   1.   1.6  1.85 1.89 0.   0.   0.   0.   0.  ]
 [0.   0.   1.   0.   0.6  0.25 1.6  0.   0.   0.   0.   0.  ]
 [0.   0.   1.   0.6  0.   0.54 1.29 0.6  0.   0.   0.   0.  ]
 [0.   0.   1.   0.6  0.   0.25 1.29 0.6  0.   0.   0.   0.  ]
 [0.   0.   0.6  1.6  1.6  1.85 1.89 1.6  0.6  0.   0.   0.  ]
 [0.   0.   0.6  0.   0.   0.25 0.29 0.   0.6  0.   0.   0.  ]
 [0.   0.6  0.   0.   0.   0.25 0.   0.29 0.29 0.6  0.   0.  ]
 [0.   0.6  0.   0.   0.25 0.25 0.25 0.   0.   0.6  0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważamy, że intensywność pikseli niekoniecznie jest teraz równa 0 lub 1, tak jak w oryginalnych symbolach. Przedstawienie graficzne wygląda następująco:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_37_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Powinniśmy być w stanie zobaczyć na powyższym obrazku sekwencję a, A, i, I oraz nic szczególnego w powiązaniu z Y. Prawie tak jest, ale sytuacja nie jest idealna ze względu na omówiony powyżej błąd wynikający z nieortogonalności.


\subsection{Nakładanie filtra}
\label{\detokenize{docs/memory:nakladanie-filtra}}
\sphinxAtStartPar
Wynik znacznie się poprawi, gdy do powyższych map pikseli zostanie zastosowany filtr. Patrząc na powyższy rysunek zauważamy, że powinniśmy pozbyć się „słabych cieni”, a pozostawić tylko piksele o wystarczającej sile, które następnie powinny otrzymac warość 1. Innymi słowy, piksele poniżej progu filtra \(b\) powinny zostać zresetowane do 0, a te powyżej lub równe \(b\) powinny zostać zresetowane do 1. Można to zgrabnie osiągnąć za pomocą naszego \sphinxstylestrong{neuronu} z rozdz. {\hyperref[\detokenize{docs/mcp:mcp-p-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Neuron MCP w Pythonie}}}}. Funkcja ta została umieszczona w bibliotece \sphinxstylestrong{neural} (patrz {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Dodatek}}}}).

\sphinxAtStartPar
A zatem definiujemy filtry jako neurony MCP o wagach \(w_0=-b\) i \(w_1=1\):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{filter}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} a \PYGZhy{} symbol (2\PYGZhy{}dim pixel array), b \PYGZhy{} bias}
    \PYG{n}{n}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} number of rows (and columns)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{b}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
       \PYG{c+c1}{\PYGZsh{} 2\PYGZhy{}dim array with the filter applied}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Działając na symbol Ap z odpowiednio dobranym \(b=0.9\) (przyjęty poziom progu jest tutaj bardzo istotny), uzyskujemy

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{n}{Ap}\PYG{p}{,}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 1 1 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 0 1 1 1 1 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
gdzie możemy zauważyć „czysty” symbol a. Sprawdzamy, czy faktycznie filtrowanie działa tak doskonale we wszystkich naszych skojarzeniach:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_47_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Można łatwo podać reprezentację właśnie przedstawionego modelu pamięci heteroasocjacyjnej jako \sphinxstylestrong{jednowarstwową} sieć neuronów MCP. Na poniższym wykresie ukazujemy wszystkie operacje, idąc od lewej strony do prawej. Symbol wejściowy jest spłaszczony. Warstwy wejściowa i wyjściowa są w pełni połączone krawędziami (których nie pokazano) łączącymi komórki wejściowe z neuronami w warstwie wyjściowej. Wagi krawędzi są równe elementom macierzy \(M_{ij}\), oznaczonej symbolem M. Funkcja aktywacji jest taka sama dla wszystkich neuronów i ma postać funkcji schodkowej.

\sphinxAtStartPar
Na dole rysunku wskazujemy elementy wektora wejściowego \(x_i\), sygnału docierającego do neuronu \(j\), tj. \(s_j=\sum_i x_i M_{ij}\) oraz wynik końcowy \(y_j=f(s_j)\).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_49_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Podsumowanie modelu pamieci heteroassociatywnej}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Zdefiniuj pary skojarzonych symboli i skonstruuj macierz pamięci \(M\).

\item {} 
\sphinxAtStartPar
Wejście to symbol w postaci 2\sphinxhyphen{}wymiarowej tablicy pikseli o wartościach 0 lub 1.

\item {} 
\sphinxAtStartPar
Spłaszcz symbol do wektora, który tworzy warstwę danych wejściowych \(x_i\).

\item {} 
\sphinxAtStartPar
Macierz wag w pełni połączonej sieci ANN to \(M\).

\item {} 
\sphinxAtStartPar
Sygnał wchodzący do neuronu \(j\) w warstwie wyjściowej to \(s_j=\sum_i x_i M_{ij}\).

\item {} 
\sphinxAtStartPar
Funkcja aktywacji to funkcja schodkowa z odpowiednio dobranym progiem. Daje ona \(y_j=f(s_j)\).

\item {} 
\sphinxAtStartPar
Potnij wektor wyjściowy na macierz pikseli, która stanowi ostateczny wynik.
Powinien to być symbol skojarzony z symbolem na wejściu.

\end{enumerate}
\end{sphinxadmonition}


\section{Pamieć autoasocjatywna}
\label{\detokenize{docs/memory:pamiec-autoasocjatywna}}

\subsection{Samo\sphinxhyphen{}skojarzenia}
\label{\detokenize{docs/memory:samo-skojarzenia}}
\sphinxAtStartPar
Model pamięci autoasocjacyjnej jest w bliskiej analogii do przypadku pamięci skojarzeniowej, ale teraz każdy symbol jest kojarzony \sphinxstylestrong{z samym sobą}. Dlaczego robimy coś takiego, stanie się jasne, gdy weźmiemy pod uwagę zniekształcone dane wejściowe. Definiujemy macierz asocjacji w następujący sposób:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ma}\PYG{o}{=}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}
    \PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Po przemnożeniu spłaszczonego symbolu przez macierz Ma i przefiltrowaniu (wszystkie kroki jak w przypadku skojarzeniowym) otrzymujemy poprawnie oryginalne symbole (poza Y, który nie był z niczym powiązany).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_57_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Zniekształcone symbole}
\label{\detokenize{docs/memory:znieksztalcone-symbole}}
\sphinxAtStartPar
Teraz wyobraźmy sobie, że oryginalny symbol zostaje częściowo zniszczony, a niektóre piksele są losowo zmieniane z 1 na 0 i odwrotnie. Tworzymy roboczą kopię oryginalnych symboli, a nastepnie losowo przekłamujemy pewną liczbe pikseli (tutaj 8):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sym2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{sym}\PYG{p}{)}                 \PYG{c+c1}{\PYGZsh{} copy of symbols}

\PYG{n}{ne}\PYG{o}{=}\PYG{l+m+mi}{8}                              \PYG{c+c1}{\PYGZsh{} number of alterations}

\PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{sym2}\PYG{p}{:}                    \PYG{c+c1}{\PYGZsh{} loop over symbols}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ne}\PYG{p}{)}\PYG{p}{:}           \PYG{c+c1}{\PYGZsh{} loop over alterations}
        \PYG{n}{i}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random row}
        \PYG{n}{j}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random column}
        \PYG{n}{s}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}           \PYG{c+c1}{\PYGZsh{} trick to switch between 1 and 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Po tym zniszczeniu symbole wejściowe wyglądają tak:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_62_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Odtworzenie symboli}
\label{\detokenize{docs/memory:odtworzenie-symboli}}
\sphinxAtStartPar
Następnie stosujemy nasz model pamięci autoasocjacyjnej do wszystkich „zniszczonych” symboli (które najpierw spłaszczamy):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fA2}\PYG{o}{=}\PYG{n}{sym2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fa2}\PYG{o}{=}\PYG{n}{sym2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fi2}\PYG{o}{=}\PYG{n}{sym2}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fI2}\PYG{o}{=}\PYG{n}{sym2}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fY2}\PYG{o}{=}\PYG{n}{sym2}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fA2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fa2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fI2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fi2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Yp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fY2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}

\PYG{n}{symp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{Ap}\PYG{p}{,}\PYG{n}{ap}\PYG{p}{,}\PYG{n}{Ip}\PYG{p}{,}\PYG{n}{ip}\PYG{p}{,}\PYG{n}{Yp}\PYG{p}{]} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
co daje

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_68_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Po przefiltrowaniu z odpowiednio dobranym progiem tutaj (\(b=0.8\)) odzyskujemy originalne symbole:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{:}     
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}  
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}       
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{n}{symp}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} plot filtered symbol}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_70_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zastosowanie algorytmu może zatem odszyfrować „zniszczony” tekst lub, bardziej ogólnie, zapewnić mechanizm korekcji błędów. Metoda działa, gdy przekłamań nie jest zbyt wiele.

\begin{sphinxadmonition}{note}{Podsumowanie modelu pamieci autoasocjatywnej}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Skonstruuj macierz pamięci \(Ma\).

\item {} 
\sphinxAtStartPar
Wejście to symbol w postaci 2\sphinxhyphen{}wymiarowej tablicy pikseli o wartościach 0 lub 1, gdzie
pewna liczba pikseli jest losowo przekłamana.

\item {} 
\sphinxAtStartPar
Spłaszcz symbol do wektora, który tworzy warstwę danych wejściowych \(x_i\).

\item {} 
\sphinxAtStartPar
Macierz wag w pełni połączonej sieci ANN to \(Ma\).

\item {} 
\sphinxAtStartPar
Sygnał wchodzący do neuronu \(j\) w warstwie wyjściowej to \(s_j=\sum_i x_i M_{ij}\).

\item {} 
\sphinxAtStartPar
Funkcja aktywacji to funkcja schodkowa z odpowiednio dobranym progiem. Daje ona \(y_j=f(s_j)\).

\item {} 
\sphinxAtStartPar
Potnij wektor wyjściowy na macierz pikseli, która stanowi ostateczny wynik. Powinien zostać przywrócony oryginalny symbol.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Konkluzja: ANN z jedną warstwą neuronów MPC mogą służyć jako bardzo proste modele pamięci!
\end{sphinxadmonition}

\sphinxAtStartPar
Zauważmy jednak, że skonstruowaliśmy macierze pamięciowe algebraicznie, niejako zewnętrznie. Dlatego sieć tak naprawdę nie nauczyła się skojarzeń z doświadczenia. Są na to sposoby, ale wymagają one bardziej zaawansowanych metod (patrz np. {[}\hyperlink{cite.docs/conclusion:id11}{FS91}{]}), podobne do omówionych w kolejnych częściach tego wykładu.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Implementacja omawianych modeli pamięci w programie Mathematica znajduje się w
{[}\hyperlink{cite.docs/conclusion:id10}{Fre93}{]} (\sphinxurl{https://library.wolfram.com/infocenter/Books/3485}) oraz we wspomnianych już wykładach \sphinxhref{http://vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/IntroNeuralSyllabus.html}{Daniela Kerstena}.
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/memory:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}

\sphinxAtStartPar
Pobaw się kodem z wykładu i
\begin{itemize}
\item {} 
\sphinxAtStartPar
dodawaj coraz więcej symboli;

\item {} 
\sphinxAtStartPar
zmieniaj poziom filtra;

\item {} 
\sphinxAtStartPar
zwiększ liczbę przekłamań w modelu autoasosjatywnym.

\end{itemize}

\sphinxAtStartPar
Omów swoje spostrzeżenia i przedyskutuj ograniczenia modeli.
\end{sphinxadmonition}


\chapter{Perceptron}
\label{\detokenize{docs/perceptron:perceptron}}\label{\detokenize{docs/perceptron:perc-lab}}\label{\detokenize{docs/perceptron::doc}}

\section{Uczenie nadzorowane}
\label{\detokenize{docs/perceptron:uczenie-nadzorowane}}
\sphinxAtStartPar
W poprzednich rozdziałach pokazaliśmy, że nawet najprostsze sieci ANN mogą wykonywać przydatne zadania (emulować sieci logiczne lub dostarczać proste modele pamięci). Ogólnie rzecz biorąc, każdy ANN ma
\begin{itemize}
\item {} 
\sphinxAtStartPar
pewną \sphinxstylestrong{architekturę}, czyli liczbę warstw, liczbę neuronów w każdej warstwie, schemat połączeń między neuronami (w pełni połączone lub nie, feed\sphinxhyphen{}forward, rekurencyjne, …);

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{wagi (hiperparametry)} na połącznieach, z określonymi wartościami definiującymi funkcjonalność sieci.

\end{itemize}

\sphinxAtStartPar
Podstawowym pytaniem praktycznym jest to, jak ustawić (dla danej architektury) wagi tak, aby żądany cel funkcjonalności sieci został zrealizowany, tj. dla określonych danych wejściowych uzuskać pożądany wynik na wyjściu.
W zadaniach omówionych wcześniej wagi mogą być skonstruowane \sphinxstyleemphasis{a priori}, czy to dla bramek logicznych, czy dla modeli pamięci. Jednak dla bardziej skomplikowanych aplikacji chcemy mieć „łatwiejszy” sposób określania wag. Co więcej, dla skomplikowanych problemów „teoretyczne” określenie wag a priori nie jest w ogóle możliwe. To podstawowy powód, dla którego wymyślono \sphinxstylestrong{algorytmy uczenia się} sieci, które ,,automatycznie” dostosowują wagi na podstawie dostępnych danych.

\sphinxAtStartPar
W tym rozdziale rozpoczynamy badanie takich algorytmów, poczynając od podejścia \sphinxstylestrong{uczenia nadzorowanego}, stosowanego \sphinxhref{http://m.in}{m.in}. do klasyfikacji danych.

\begin{sphinxadmonition}{note}{Uczenie nadzorowane}

\sphinxAtStartPar
W tej strategii dane muszą posiadać \sphinxstylestrong{etykiety}, które a priori określają poprawną kategorię dla każdego punktu. Pomyślmy na przykład o zdjęciach zwierząt (dane lub cechy, ang. features) i ich opisach (kot, pies,…), które nazywane są etykietami (ang. labels).
Te etykietowane dane są następnie dzielone na próbkę \sphinxstylestrong{szkoleniową} i próbkę \sphinxstylestrong{testową}.

\sphinxAtStartPar
Podstawowe kroki uczenia nadzorowanego dla danej ANN są następujące:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Zainicjuj w jakiś sposób wagi, na przykład losowo lub na zero.

\item {} 
\sphinxAtStartPar
Odczytuj kolejno punkty danych z próbki szkoleniowej i przepuszczaj je przez swoją sieć ANN. Otrzymana odpowiedź może różnić się od prawidłowej, zawartej w etykiecie. W takim przypadku wagi są zmieniane zgodnie z konkretną receptą (o czym później).

\item {} 
\sphinxAtStartPar
W razie potrzeby powtórz poprzedni krok. Zazwyczaj wagi zmienia się coraz mniej w miarę postępu algorytmu.

\item {} 
\sphinxAtStartPar
Zakończ szkolenie sieci po osiągnięciu kryterium zatrzymania (wagi nie zmieniają się już znacznie lub została osiągnięta maksymalna liczba iteracji).

\item {} 
\sphinxAtStartPar
Przetestuj tak wyszkoloną ANN na próbce testowej.

\end{itemize}

\sphinxAtStartPar
Jeśli jesteśmy zadowoleni, mamy pożądaną wyszkoloną sieć ANN wykonującą określone zadanie (takie jak np. klasyfikacja danych), której można teraz używać na nowych, nieetykietowanych danych. Jeśli nie, możemy inaczej podzielić próbkę na część szkoleniową i testową, po czym powtórzyć procedurę uczenia od początku. Możemy także spróbować pozyskać więcej danych (co może być kosztowne), lub też zmienić architekturę sieci.

\sphinxAtStartPar
Termin „nadzorowany” pochodzi z interpretacji procedury, w której etykiety posiadane są przez „nauczyciela”, który w ten sposób wie, które odpowiedzi są prawidłowe, a które błędne i który \sphinxstylestrong{nadzoruje} w ten sposób proces szkolenia. Oczywiście program komputerowy ma wbudowanego nauczyciela. tj. „nadzoruje się” sam.
\end{sphinxadmonition}


\section{Perceptron jako klasyfikator binarny}
\label{\detokenize{docs/perceptron:perceptron-jako-klasyfikator-binarny}}
\sphinxAtStartPar
Najprostszy algorytm uczenia nadzorowanego
to \sphinxhref{https://en.wikipedia.org/wiki/Perceptron}{perceptron}, wymyślony w 1958 roku przez Franka Rosenblatta. Może służyć \sphinxhref{http://m.in}{m.in}. do
konstruowania \sphinxstylestrong{klasyfikatorów binarnych} danych. \sphinxstyleemphasis{Binarny} oznacza, że sieć
służy do oceny, czy element danych ma określoną cechę, czy nie \sphinxhyphen{} są tylko dwie możliwości. Klasyfikacja wieloetykietowa jest również możliwa w przypadku ANN (patrz ćwiczenia), ale nie omawiamy jej tutaj.

\begin{sphinxadmonition}{note}{Uwaga}

\sphinxAtStartPar
Termin \sphinxstyleemphasis{perceptron} jest również używany dla ANN (bez lub z warstwami pośrednimi) składających się z neuronów MCP (por. rys. \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} i \hyperref[\detokenize{docs/mcp:mcp1-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp1-fig}}}), na których wykonywany jest algorytm perceptronu.
\end{sphinxadmonition}


\subsection{Próbka ze znaną regułą klasyfikacji}
\label{\detokenize{docs/perceptron:probka-ze-znana-regula-klasyfikacji}}
\sphinxAtStartPar
Na początek potrzebujemy danych treningowych, które wygenerujemy jako losowe punkty w kwadracie. Zatem współrzędne punktu, \(x_1\) i \(x_2\), należą do przedziału \([0,1]\). Definiujemy dwie kategorie: jedną dla punktów leżących powyżej linii \(x_1=x_2\) (nazwijmy je różowymi) oraz drugą dla punktów leżących poniżej tej linii (niebieskie). Podczas losowego generowania danych sprawdzamy, czy \(x_2 > x_1\) czy nie i przypisujemy odpowiednią  \sphinxstylestrong{etykietę} do każdego punktu, równą odpowiednio 1 lub 0. Te etykiety są oczekiwanymi „prawdziwymi” odpowiedziami sieci po jej wyszkoleniu.

\sphinxAtStartPar
Funkcja generująca opisany powyżej punkt danych z etykietą to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{point}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} generates random coordinates x1, x2, and 1 if x2\PYGZgt{}x1, 0 otherwise}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random number from the range [0,1]}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZgt{}}\PYG{n}{x1}\PYG{p}{)}\PYG{p}{:}                     \PYG{c+c1}{\PYGZsh{} condition met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 1}
    \PYG{k}{else}\PYG{p}{:}                          \PYG{c+c1}{\PYGZsh{} not met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Generujemy \sphinxstylestrong{próbkę szkoleniową}, składającą się z \sphinxstylestrong{npo}=300 etykietowanych punktów danych:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{npo}\PYG{o}{=}\PYG{l+m+mi}{300} \PYG{c+c1}{\PYGZsh{} number of data points in the training sample}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{  x1         x2         label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} header}
\PYG{n}{samp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npo}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} training sample, \PYGZus{} is dummy iterator}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}                           \PYG{c+c1}{\PYGZsh{} first 5 data points}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
  x1         x2         label
[[0.32934056 0.39891895 1.        ]
 [0.87245402 0.23216823 0.        ]
 [0.34298363 0.15572279 0.        ]
 [0.85658971 0.99224611 1.        ]
 [0.78826375 0.03085404 0.        ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Pętle w tablicy}

\sphinxAtStartPar
W Pythonie można wygodnie zdefiniować tablicę poprzez pętlę, np.
{[}i**2 for i in range(4){]} daje {[}1,4,9{]}.

\sphinxAtStartPar
W pętlach, jeśli wskaźnik nie występuje jawnie w wyrażeniu, można użyć symbolu \sphinxstylestrong{\_} , np.

\sphinxAtStartPar
{[}point() for \_ in range(npo){]}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Zakresy w tablicach}

\sphinxAtStartPar
Aby nie drukować niepotrzebnie bardzo długiej tabeli, po raz pierwszy użyliśmy powyżej \sphinxstylestrong{zakresów dla wskaźników tablic}. Np. 2:5 oznacza od 2 do 4 (przypomnijmy, że ostatni jest wykluczony!), :5 \sphinxhyphen{} od 0 do 4, 5: \sphinxhyphen{} od 5 do końca, wreszcie : \sphinxhyphen{} wszystkie elementy.
\end{sphinxadmonition}

\sphinxAtStartPar
Nasze wygenerowane dane przedstawia graficznie poniższy rysunek. Wykreślamy również linię \(x_2=x_1\), która oddziela niebieskie i różowe punkty. W tym przypadku podział jest możliwy a priori (znamy regułę) w sposób dokładny.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}                 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}                                  \PYG{c+c1}{\PYGZsh{} axes limits}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}       \PYG{c+c1}{\PYGZsh{} label determines the color}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} point size and color}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{)}                 \PYG{c+c1}{\PYGZsh{} separating line}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}                    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_17_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Zbiory liniowo rozłączne}

\sphinxAtStartPar
Dwa zbiory punktów (tutaj niebieski i różowy) na płaszczyźnie, które można rozdzielić linią prostą, nazywamy \sphinxstylestrong{liniowo rozłącznymi} (separowalnymi). W trzech wymiarach zbiory muszą być separowalne płaszczyzną, ogólnie w \(n\) wymiarach  zbiory muszą być separowalne za pomocą  \(n-1\) wymiarowej hiperpłaszczyzny.
\end{sphinxadmonition}

\sphinxAtStartPar
Analitycznie, jeżeli punkty w przestrzeni \(n\) wymiarowej  mają współrzędne \((x_1,x_2,\dots,x_n)\), to można dobrać parametry \((w_0,w_1,\dots,w_n)\) w taki sposób, aby zbiór jeden punktów spełniał warunek
\begin{equation}\label{equation:docs/perceptron:eq-linsep}
\begin{split}w_0+x_1 w_1+x_2 w_2 + \dots x_n w_n > 0\end{split}
\end{equation}
\sphinxAtStartPar
a drugi warunek przeciwny, ze znakiem \(>\) zastąpionym przez \(\le\).

\sphinxAtStartPar
A teraz kluczowa, choć oczywista obserwacja: powyższa nierówność jest dokładnie warunkiem zaimplementowanym w {[}neuronie MCP{]}(laboratorium MCP) (ze schodkową funkcją aktywacji) w konwencji \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}! Możemy więc zrealizować warunek \eqref{equation:docs/perceptron:eq-linsep} za pomocą funkcji \sphinxstylestrong{neuron} z biblioteki \sphinxstylestrong{neural}.

\sphinxAtStartPar
W naszym przykładzie dla różowych punktów, według konstrukcji,
\begin{equation*}
\begin{split}
x_2>x_1 \to s=-x_1+x_2 >0
\end{split}
\end{equation*}
\sphinxAtStartPar
skąd, używając równ. \eqref{equation:docs/perceptron:eq-linsep}, możemy od razu odczytać
\begin{equation*}
\begin{split}
w_0=0, \;\; w_1=-1, w_2=1.
\end{split}
\end{equation*}
\sphinxAtStartPar
Zatem funkcja \sphinxstylestrong{neuron} dla punktu próbki p jest używana w następujący sposób:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} sample point with x\PYGZus{}2 \PYGZgt{} x\PYGZus{}1}
\PYG{n}{w}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} weights as given above}

\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Neuron, odpalił, więc punkt p jest różowy.

\begin{sphinxadmonition}{note}{Wniosek}

\sphinxAtStartPar
Pojedynczy neuron MCP z odpowiednio dobranymi wagami może być użyty jako klasyfikator binarny dla \(n\)\sphinxhyphen{}wymiarowych danych separowalnych.
\end{sphinxadmonition}


\subsection{Próbka o nieznanej regule klasyfikacji}
\label{\detokenize{docs/perceptron:probka-o-nieznanej-regule-klasyfikacji}}
\sphinxAtStartPar
W tym miejscu czytelnik może być nieco zwiedziony pozorną błahością wyników. Wątpliwości mogą wynikać z tego, że w powyższym przykładzie od początku znaliśmy regułę określającą dwie klasy punktów (\(x_2>x_1\), lub odwrotnie). Jednak w ogólnej sytuacji „z prawdziwego życia” zwykle tak nie jest! Wyobraź sobie, że napotykamy (etykietowane) dane \sphinxstylestrong{samp2} wyglądające tak:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.50896192 0.26237741 0.        ]
 [0.50775256 0.1093865  0.        ]
 [0.44707124 0.04838339 0.        ]
 [0.26519082 0.33358304 0.        ]
 [0.5661581  0.53616119 0.        ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_28_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Sytuacja jest teraz w pewnym sensie odwrócona. Uzyskaliśmy skądś (liniowo separowalne) dane i chcemy znaleźć regułę, która definiuje te dwie klasy. Innymi słowy, musimy narysować linię podziału, która jest równoważna ze znalezieniem wag neuronu MCP \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}, który przeprowadziłby odpowiednią klasyfikację binarną.


\section{Algorytm perceptronu}
\label{\detokenize{docs/perceptron:algorytm-perceptronu}}\label{\detokenize{docs/perceptron:lab-pa}}
\sphinxAtStartPar
Moglibyśmy spróbować jakoś obliczyć właściwe wagi dla powyższego przykładu i znaleźć linię podziału, na przykład linijką i ołówkiem, ale nie o to tutaj chodzi. Chcemy mieć systematyczną procedurę algorytmiczną, która bez trudu zadziała w tej czy każdej podobnej sytuacji. Odpowiedzią jest wspomniany już \sphinxhref{https://en.wikipedia.org/wiki/Perceptron}{algorytm perceptronu}.

\sphinxAtStartPar
Przed przedstawieniem algorytmu zauważmy, że neuron MCP z pewnym zbiorem wag \(w_0, w_1, w_2\) zawsze daje jakąś odpowiedź dla etylirtowanego punktu danych, poprawną lub błędną. Na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}           \PYG{c+c1}{\PYGZsh{} arbitrary choice of weights}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{label  answer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} header}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} look at first 5 points}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{    }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)} 
            \PYG{c+c1}{\PYGZsh{} samp2[i,2] is the label, samp2[i,:2] is [x\PYGZus{}1,x\PYGZus{}2]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
label  answer
0      1
0      1
0      0
0      0
0      1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Widzimy, że niektóre odpowiedzi są równe etykietom (poprawne), a inne są od nich różne (błędne). Ogólną ideą jest teraz \sphinxstylestrong{użycie błędnych odpowiedzi}, aby sprytnie, małymi krokami zmieniać wagi, tak aby po wystarczającej liczbie iteracji wszystkie odpowiedzi dla danej próbki szkoleniowej były poprawne!

\begin{sphinxadmonition}{note}{Algorytm perceptronu}

\sphinxAtStartPar
Iterujemy po punktach próbki danych szkoleniowych.
Jeżeli dla danego punktu otrzymany wynik \(y_o\) jest równy prawdziwej wartości \(y_t\) (etykieta), tj. odpowiedź jest prawidłowa, nic nie robimy. Jeśli jednak jest błędna, zmieniamy nieco wagi, tak aby szansa na otrzymanie błędnej odpowiedzi spadła. Przepis jest następujący:

\sphinxAtStartPar
\(w_i \to w_i  +  \varepsilon  (y_t - y_o)  x_i\),

\sphinxAtStartPar
gdzie \( \varepsilon \) to mała liczba (nazywana \sphinxstylestrong{szybkością uczenia}), a \(x_i\) to współrzędne punktu wejściowego, gdzie \(i=0,\dots,n\).
\end{sphinxadmonition}

\sphinxAtStartPar
Prześledźmy, jak to działa. Załóżmy najpierw, że \(x_i> 0\). Wtedy jeśli etykieta \( y_t = 1 \) jest większa niż uzyskana odpowiedź \( y_o = 0 \), waga \(w_i\) jest zwiększana. Wtedy \(w \cdot x\) również wzrasta, a \( y_o = f (w \cdot x) \) z większą szansą przyjmie poprawną wartość 1 (pamiętamy, jak wygląda funkcja schodkowa \(f\)). Jeżeli natomiast etykieta \(y_t = 0 \) jest mniejsza niż uzyskana odpowiedź \( y_o = 1 \), to waga \(w_i\) maleje, \( w \cdot x \) maleje, a \( y_o = f(w \cdot x) \) ma większą szansę na osiągnięcie prawidłowej wartości 0.

\sphinxAtStartPar
Jeśli \( x_i < 0 \), łatwo analogicznie sprawdzić, że przepis również działa poprawnie.

\sphinxAtStartPar
Jeśli odpowiedź jest prawidłowa, \(y_t=y_0\), to \( w_i \to w_i\), więc nic się nie zmienia. Nie „psujemy” perceptronu!

\sphinxAtStartPar
Powyższy wzór można zastosować wielokrotnie dla tego samego punktu z próbki szkoleniowej. Następnie wykonujemy pętlę po wszystkich punktach próbki, a całą procedurę można jeszcze powtarzać w wielu rundach, aby uzyskać stabilne wagi (nie zmieniające się już w miarę kontynuacji procedury lub zmieniające się tylko nieznacznie).

\sphinxAtStartPar
Zazwyczaj w takich algorytmach szybkość uczenia \( \varepsilon \) jest zmniejszana w kolejnych rundach. Jest to bardzo ważne z praktycznego punktu widzenia, ponieważ zbyt duże aktualizacje mogą zepsuć uzyskane rozwiązanie.

\sphinxAtStartPar
Implementacja algorytmu perceptronu dla danych dwuwymiarowych w Pythonie wygląda następująco:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w0}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}  \PYG{c+c1}{\PYGZsh{} initialize weights randomly in the range [\PYGZhy{}0.5,0.5]}
\PYG{n}{w1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}
\PYG{n}{w2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.3}                     \PYG{c+c1}{\PYGZsh{} initial  learning speed }
   
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} in each round decrease the learning speed }
        
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npo}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over the points from the data sample}
        
        \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} repeat 5 times for each points}
            
            \PYG{n}{yo} \PYG{o}{=} \PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} obtained answer}
            
            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} weight update (the perceptron formula)}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Obtained weights:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{  w0     w1     w2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} header }
\PYG{n}{w\PYGZus{}o}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} obtained weights}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} result, rounded to 3 decimal places }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Obtained weights:
  w0     w1     w2
[\PYGZhy{}0.562 \PYGZhy{}1.114  2.192]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Otrzymane wagi, jak wiemy, definiują linię podziału. Tak więc, geometrycznie, algorytm tworzy linię podziału, narysowaną poniżej wraz z próbką szkoleniową.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_39_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Widzimy, że algorytm działa! Wszystkie różowe punkty znajdują się powyżej linii podziału, a wszystkie niebieskie poniżej. Podkreślmy, że linia podziału dana przez równanie
\begin{equation*}
\begin{split} w_0+x_1 w_1 + x_2 w_2=0,\end{split}
\end{equation*}
\sphinxAtStartPar
nie wynika z naszej wiedzy a priori, ale z treningu (uczenia nadzowowanego) neuronu MCP, który odpowiednio dopasowuje swoje wagi.

\begin{sphinxadmonition}{attention}{Uwaga:}
\sphinxAtStartPar
Można udowodnić, że algorytm perceptronu jest zbieżny wtedy i tylko wtedy, gdy dane są liniowo separowalne.
\end{sphinxadmonition}

\sphinxAtStartPar
Teraz możemy wyjawić nasz sekret! Dane próbki szkoleniowej \sphinxstylestrong{samp2} zostały etykietowane w momencie tworzenia regułą
\begin{equation*}
\begin{split} x_2> 0,25+0,52 x_1, \end{split}
\end{equation*}
\sphinxAtStartPar
co odpowiada wagom \(w_0=0.25\), \(w_1=-0.52\), \(w_2=1\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} weights used for labeling the training sample}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w\PYGZus{}c}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZhy{}0.25 \PYGZhy{}0.52  1.  ]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zwróćmy uwagę, że nie są to wcale te same wagi, jakie uzyskano podczas treningu:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZhy{}0.562 \PYGZhy{}1.114  2.192]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powód jest dwojaki. Po pierwsze, zauważmy, że warunek nierówności \eqref{equation:docs/perceptron:eq-linsep} pozostaje niezmieniony, jeśli pomnożymy obie stronynierówności  przez \sphinxstylestrong{dodatnią} stałą \(c\). Możemy zatem przeskalować wszystkie wagi przez \(c\), a sytuacja (odpowiedzi neuronu MCP, linia podziału) pozostaje dokładnie taka sama (napotykamy tutaj \sphinxstylestrong{klasę równoważności} wag przeskalowanych o czynnik dodatni) .

\sphinxAtStartPar
Z tego powodu dzieląc uzyskane wagi przez wagi użyte do oznaczenia próbki, otrzymujemy (prawie) stałe wartości:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{o}{/}\PYG{n}{w\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[2.249 2.143 2.192]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powodem, dla którego wartości stosunków wag dla \(i=0,1,2\) nie są dokładnie takie same, jest to, że próbka ma skończoną liczbę punktów (tutaj 300). W ten sposób zawsze istnieje pewna luka między dwiema klasami punktów i jest trochę miejsca na nieznaczne przesuwanie linii rozdzielającej. Przy większej liczbie punktów danych efekt różnicy stosunków wag zmniejsza się (patrz ćwiczenia).


\subsection{Testowanie klasyfikatora}
\label{\detokenize{docs/perceptron:testowanie-klasyfikatora}}
\sphinxAtStartPar
Ze względu na ograniczoną wielkość próbki szkoleniowej i opisany powyżej efekt „luki”, wynik klasyfikacji na próbce testowej jest czasami błędny. Dotyczy to zawsze punktów w pobliżu linii podziału, która jest wyznaczana z dokładnością zależną od krotności próbki szkoleniowej. Poniższy kod przeprowadza sprawdzenie na próbce testowej. Próbka ta składa się z etykietowanych danych wygenerowanych losowo za pomocą tej samej funkcji \sphinxstylestrong{point2} użytej uprzednio do wygenerowania danych szkoleniowych:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{point2}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random number from the range [0,1]}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZgt{}}\PYG{n}{x1}\PYG{o}{*}\PYG{l+m+mf}{0.52}\PYG{o}{+}\PYG{l+m+mf}{0.25}\PYG{p}{)}\PYG{p}{:}           \PYG{c+c1}{\PYGZsh{} condition met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 1}
    \PYG{k}{else}\PYG{p}{:}                          \PYG{c+c1}{\PYGZsh{} not met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Kod testujący jest następujący:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{er}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} initialize an empty 1 x 3 array to store misclassified points}

\PYG{n}{ner}\PYG{o}{=}\PYG{l+m+mi}{0}                 \PYG{c+c1}{\PYGZsh{} initial number of misclassified points}
\PYG{n}{nt}\PYG{o}{=}\PYG{l+m+mi}{10000}               \PYG{c+c1}{\PYGZsh{} number of test points}

\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nt}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over the test points}
    \PYG{n}{ps}\PYG{o}{=}\PYG{n}{point2}\PYG{p}{(}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} a test point }
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{ps}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{!=}\PYG{n}{ps}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} if wrong answer                                      }
        \PYG{n}{er}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{er}\PYG{p}{,}\PYG{p}{[}\PYG{n}{ps}\PYG{p}{]}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} add the point to er}
        \PYG{n}{ner}\PYG{o}{+}\PYG{o}{=}\PYG{l+m+mi}{1}                                 \PYG{c+c1}{\PYGZsh{} count the number of errors}
        
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{number of misclassified points = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{ner}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ per }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{nt}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ (}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{ner}\PYG{o}{/}\PYG{n}{nt}\PYG{o}{*}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{ )}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}        
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
number of misclassified points =  20  per  10000  ( 0.2 \PYGZpc{} )
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jak widać, niewielka liczba punktów testowych jest błędnie sklasyfikowana. Wszystkie te punkty leżą w pobliżu linii rozdzielającej.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_54_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Błędna klasyfikacja}

\sphinxAtStartPar
Przyczyną błędnej klasyfikacji jest fakt, że próbka szkoleniowa nie wyznacza dokładnie linii rozdzielającej, ponieważ między punktami występuje pewna luka. Aby uzyskać lepszy wynik, punkty treningowe musiałyby być „gęstsze” w sąsiedztwie linii rozdzielającej lub też próbka treningowa musiałaby być większa.
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/perceptron:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Pobaw się kodem z wykładu i zobacz, jak procent błędnie zaklasyfikowanych punktów zmniejsza się wraz ze wzrostem wielkości próbki szkoleniowej.

\item {} 
\sphinxAtStartPar
Gdy algorytm perceptronu jest zbieżny, w pewnym momencie wagi przestają się zmieniać. Popraw kod wykładu, wdrażając zatrzymywanie, gdy wagi nie zmieniają się więcej niż pewna wartość podczas przechodzenia do następnej rundy.

\item {} 
\sphinxAtStartPar
Uogólnij powyższy klasyfikator na punkty w przestrzeni trójwymiarowej.

\end{itemize}
\end{sphinxadmonition}


\chapter{Więcej warstw}
\label{\detokenize{docs/more_layers:wiecej-warstw}}\label{\detokenize{docs/more_layers:more-lab}}\label{\detokenize{docs/more_layers::doc}}

\section{Dwie warstwy neuronów}
\label{\detokenize{docs/more_layers:dwie-warstwy-neuronow}}
\sphinxAtStartPar
W poprzednim rozdziale pokazaliśmy, że neuron MCP ze schodkową funkcją aktywacji odpowiada nierówności \(x \cdot w=w_0+x_1 w_1 + \dots x_n w_n > 0\), gdzie \(n\) jest wymiarem przestrzeni wejściowej. Pouczające jest dalsze prześledzenie tej geometrycznej interpretacji. Przyjmując dla prostoty \(n=2\) (płaszczyzna), powyższa nierówność odpowiada jej podziałowi na dwie półpłaszczyzny. Jak wiemy, prosta wyrażona jest równaniem
\begin{equation*}
\begin{split}w_0+x_1 w_1 + x_2 w_2 = 0\end{split}
\end{equation*}
\sphinxAtStartPar
i stanowi \sphinxstylestrong{linię podziału} na dwie półpłaszczyzny.

\sphinxAtStartPar
Wyobraźmy sobie teraz, że mamy więcej takich warunków: dwa, trzy itd., ogólnie \(k\) niezależnych warunków. Biorąc koniunkcję tych warunków, możemy zbudować wypukłe obszary, jak przykładowo pokazano na \hyperref[\detokenize{docs/more_layers:regions-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:regions-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=620\sphinxpxdimen]{{regions}.png}
\caption{Przykładowe obszary wypukłe na płaszczyźnie, od lewej do prawej: z jednym warunkiem nierówności, z koniunkcjami 2 warunków i z koniukcjami 3 lub 4 warunków nierówności, dającymi \sphinxstylestrong{wielokąty}.}\label{\detokenize{docs/more_layers:regions-fig}}\end{figure}

\begin{sphinxadmonition}{note}{Obszar wypukły}

\sphinxAtStartPar
Z definicji obszar \(A\) jest wypukły wtedy i tylko wtedy, gdy odcinek pomiędzy dowolnymi dwoma punktami w \(A\) jest zawarty w \(A\). Obszar, który nie jest wypukły nazywa się \sphinxstylestrong{wklęsłym}.
\end{sphinxadmonition}

\sphinxAtStartPar
Oczywiście, \(k\) warunków nierówności można narzucić za pomocą \(k\) neuronów MCP.
Przypomnijmy sobie z rozdz. {\hyperref[\detokenize{docs/mcp:bool-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Funkcje logiczne}}}}, że możemy w prosty sposób budować funkcje logiczne za pomocą sieci neuronowych. W szczególności możemy dokonać koniunkcji \(k\) warunków, biorąc neuron o wagach \(w_0=-1\) i \(1/k < w_i < 1/(k-1)\), gdzie \(i=1,\dots ,k\). Jedną z możliwości jest np.
\begin{equation*}
\begin{split}w_i=\frac{1}{k-\frac{1}{2}}.\end{split}
\end{equation*}
\sphinxAtStartPar
Rzeczywiście, niech \(p_0=1\), a warunki narzucone przez nierówności oznaczymy jako \(p_i\), \(i=1,\dots,k\), które mogą przyjmować wartości 1 lub 0 (prawda lub fałsz). Następnie
\begin{equation*}
\begin{split}p \cdot w =-1 + p_1 w_1 + \dots + p_k w_k = -1+\frac{p_1+\dots p_k}{k-\frac{1}{2}} > 0\end{split}
\end{equation*}
\sphinxAtStartPar
wtedy i tylko wtedy, gdy wszystkie \(p_i=1\), tj. wszystkie warunki są spełnione.

\sphinxAtStartPar
Architektury sieci dla warunków \(k=1\), 2, 3 lub 4 są ukazane na \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Idąc od lewej do prawej począwszy od drugiego panelu, mamy sieci z dwiema warstwami neuronów i z \(k\) neuronami w warstwie pośredniej, zapewniającymi warunki nierówności, oraz jednym neuronem w warstwie wyjściowej, pełniącym funkcję bramki AND. Oczywiście dla jednego warunku wystarczy mieć jeden neuron, jak pokazano na lewym panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=820\sphinxpxdimen]{{nf1-4}.png}
\caption{Sieci zdolne do klasyfikowania danych w obszarach z \hyperref[\detokenize{docs/more_layers:regions-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:regions-fig}}}.}\label{\detokenize{docs/more_layers:nfn-fig}}\end{figure}

\sphinxAtStartPar
W interpretacji geometrycznej pierwsza warstwa neuronowa reprezentuje \(k\) półpłaszczyzn, a neuron w drugiej warstwie odpowiada obszarowi wypukłemu o \(k\) bokach.

\sphinxAtStartPar
Sytuacja w oczywisty sposób uogólnia się na dane w większej liczbie wymiarów. W takim przypadku mamy więcej czarnych kropek dla danych wejściowych na \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Geometrycznie dla \(n=3\) mamy do czynienia z dzieleniem na półprzestrzenie z pomocą płaszczyzn i tworzenie wypukłych \sphinxhref{https://en.wikipedia.org/wiki/Wielo\%C5\%9Bciany}{wielościanów}, a dla \(n>3\) z dzieleniem hiperprzestzreni {[}hiperpłaszczyznami{]}(https:/ /en.wikipedia.org/wiki/Hyperplane) i tworzeniem wypukłych \sphinxhref{https://en.wikipedia.org/wiki/Polytope}{politopów}.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Jeśli w warstwie pośredniej znajduje się wiele neuronów, powstały wielokąt ma wiele boków, które mogą przybliżać gładką granicę, taką jak łuk. Aproksymacja jest coraz lepsza w miarę wzrostu \(k\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Percepton z dwiema warstwami neuronów (z wystarczającą liczbą neuronów w warstwie pośredniej) może klasyfikować punkty należące do obszaru wypukłego w przestrzeni \(n\)\sphinxhyphen{}wymiarowej.
\end{sphinxadmonition}


\section{Trzy lub więcej warstw neuronów}
\label{\detokenize{docs/more_layers:trzy-lub-wiecej-warstw-neuronow}}
\sphinxAtStartPar
Pokazaliśmy właśnie, że sieć dwuwarstwowa może klasyfikować wielokąt wypukły. Wyobraźmy sobie teraz, że tworzymy dwie takie figury w drugiej warstwie neuronów, na przykład dzieki następującej sieci:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Zauważmy, że pierwsza i druga warstwa neuronów nie są tutaj w pełni połączone, ponieważ „układamy na sobie” dwie sieci tworzące trójkąty, jak w trzecim panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Następnie w trzeciej warstwie neuronowej (tutaj posiadającej pojedynczy neuron) implementujemy bramkę \(p \,\wedge \!\sim\!q\), czyli koniunkcję warunków, że punkty należą do jednego trójkąta, a nie należą do drugiego. Jak zaraz pokażemy, przy odpowiednich wagach powyższa siatka może wytworzyć obszar wklęsły, na przykład trójkąt z trójkątnym wgłębieniem:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{tritri}.png}
\caption{Trójkąt z trójkątnym wgłębieniem.}\label{\detokenize{docs/more_layers:tri-fig}}\end{figure}

\sphinxAtStartPar
Uogólniając ten argument na inne kształty, można pokazać ważne twierdzenie:

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Perceptron z trzema lub więcej warstwami neuronów (z wystarczającą liczbą neuronów w warstwach pośrednich) może klasyfikować punkty należące do \sphinxstylestrong{dowolnego} regionu w \(n\)\sphinxhyphen{}wymiarowej przestrzeni z \(n-1\)\sphinxhyphen{}wymiarowymi ograniczeniami przez hiperpłaszczyzny.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Warto tutaj podkreślić, że trzy warstwy zapewniają pełną funkcjonalność! Dodawanie kolejnych warstw do klasyfikatora nie zwiększa jego możliwości.
\end{sphinxadmonition}


\section{Feed forward w Pythonie}
\label{\detokenize{docs/more_layers:feed-forward-w-pythonie}}
\sphinxAtStartPar
Zanim przejdziemy do przykładu, potrzebujemy kodu w Pythonie do propagacji sygnału w przód w ogólnej, w pełni połączonej sieci. Będziemy reprezentować architekturę sieci z \(l\) warstwami neuronów jako tablicę postaci
\begin{equation*}
\begin{split}[n_0,n_1,n_2,...,n_l],\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(n_0\) jest liczbą węzłów wejściowych, a \(n_i\) liczbą neuronów w warstwach \(i=1,\dots,l\). Na przykład architektura sieci z czwartego panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}} to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} 
\PYG{n}{arch}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[2, 4, 1]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
W kodach tego kursu posługujemy się konwencją z \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}, a mianowicie próg jest traktowany jednolicie z pozostałym sygnałem. Jednak węzły progowe nie są uwzględniane w określaniu liczb \(n_i\) zdefiniowanych powyżej. W szczególności bardziej szczegółowy widok czwartego panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}} to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_23_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Czarne kropki oznaczają dane wejściowe, szare kropki odpowiadają węzłom progowym, dającym input równy 1, a niebieskie kółka to neurony.

\sphinxAtStartPar
Następnie potrzebujemy wagi połączeń. Mamy \(l\) zestawów wag, z których każdy odpowiada krawędziom wchodzącym do danej warstwy neuronowej od lewej strony.
W powyższym przykładzie pierwsza warstwa neuronów (niebieskie kółka po lewej stronie) ma wagi, które tworzą macierz \(3 \times 4\). Tutaj 3 to liczba węzłów w poprzedniej (wejściowej) warstwie (łącznie z węzłem progowym), a 4 to liczba neuronów w pierwszej warstwie neuronowej. Podobnie wagi związane z drugą (wyjściową) warstwą neuronową tworzą macierz \(4 \times 1\). Stąd w naszej konwencji macierze wag odpowiadające kolejnym warstwom neuronów \(1, 2, \dots, l\) mają wymiary
\begin{equation*}
\begin{split}
(n_0+1)\times n_1, \; (n_1+1)\times n_2, \; \dots \; (n_{l-1}+1)\times n_l.
\end{split}
\end{equation*}
\sphinxAtStartPar
Tak więc, aby przechowywać wszystkie wagi sieci, tak naprawdę potrzebujemy \sphinxstylestrong{trzech} wskaźników: jeden dla warstwy, jeden dla liczby węzłów w poprzedniej warstwie i jeden dla liczby węzłów w danej warstwie. Moglibyśmy tutaj użyć trójwymiarowej tablicy, ale ponieważ numerujemy warstwy neuronów zaczynając od 1, a tablice zaczynają się od 0, nieco wygodniej jest użyć struktury \sphinxstylestrong{słownika} Pythona. Przechowujemy zatem wagi jako
\begin{equation*}
\begin{split}w=\{1: arr^1, 2: arr^2, ..., l: arr^l\},\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(arr^i\) jest \sphinxstylestrong{dwuwymiarową} tablicą (macierzą) wag dla warstwy neuronowej \(i\). Dla przypadku z powyższego rysunku możemy wziąć na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[ 1.   2.   1.   1. ]
 [ 2.  \PYGZhy{}3.   0.2  2. ]
 [\PYGZhy{}3.  \PYGZhy{}3.   5.   7. ]]

[[ 1. ]
 [ 0.2]
 [ 2. ]
 [ 2. ]
 [\PYGZhy{}0.5]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dla sygnału rozchodzącego się w sieci stosujemy również odpowiednio słownik w postaci
\begin{equation*}
\begin{split}x=\{0: x^0, 1: x^1, 2: x^2, ..., l: x^l\},\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(x^0\) to wejście, a \(x^i\) to sygnał wychodzący z warstwy neuronowej \(i\), dla \(i=1, \dots, l\). Wszystkie symbole \(x^j\), \(j=0, \dots, l\) są tablicami jednowymiarowymi. Uwzględniamy tu węzły progowe, stąd wymiary \(x^j\) wynoszą \(n_j+1\), z wyjątkiem warstwy wyjściowej, która nie ma węzła odchylenia, stąd \(x^l\) ma wymiar \(n_l\). Innymi słowy, wymiary tablic sygnału są równe całkowitej liczbie węzłów w każdej warstwie.

\sphinxAtStartPar
Następnie przedstawiamy szczegółowo odpowiednie wzory, ponieważ jest to kluczowe dla uniknięcia ewentualnych pomyłek związanych z zapisem.
Wiemy już z \eqref{equation:docs/mcp:eq-f0}, że dla pojedynczego neuronu z \(n\) wejściami, sygnał wchodzący jest obliczany jako
\begin{equation*}
\begin{split}s = x_0 w_0 + x_1 w_1 + x_2 w_2 + ... + x_n w_n = \sum_{\beta=0}^n x_\beta w_\beta .\end{split}
\end{equation*}
\sphinxAtStartPar
Przy większej liczbie warstw (oznaczonych wskażnikiem górnym \(i\)) i liczbie neuronów \(n_i\) w warstwie \(i\), notacja uogólnia się na
\begin{equation*}
\begin{split}
s^i_\alpha=\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l.
\end{split}
\end{equation*}
\sphinxAtStartPar
Zauważmy, że sumowanie zaczyna się od \(\beta=0\), aby uwzględnić węzeł progowy w poprzedniej warstwie \((i-1)\), ale \(\alpha\) zaczyna się od 1, ponieważ tylko neurony (a nie węzeł progowy) w warstwie \(i\) odbierają sygnał (patrz rysunek poniżej).

\sphinxAtStartPar
W notacji macierzowej możemy też zapisać bardziej zwięźle
\(s^{iT} = x^{(i-1)T} W^i\), gdzie \(T\) oznacza transpozycję, tzn.
\begin{equation*}
\begin{split}
\begin{pmatrix} s^i_1 & s^i_2 & ...& s^i_{n_i} \end{pmatrix} = 
\begin{pmatrix} x^{i-1}_0 & x^{i-1}_1 & ...& x^{i-1}_{n_{i-1}} \end{pmatrix}
\begin{pmatrix} w^i_{01} & w^i_{02} & ...& w^i_{0,n_i} \\ w^i_{11} & w^i_{12} & ...& w^i_{1,n_i} \\ 
 ... & ... & ...& ... \\ w^i_{n_{i-1}1} & w^i_{n_{i-1}2} & ...& w^i_{n_{i-1}n_i} \end{pmatrix}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Jak już dobrze wiemy, wyjście z neuronu uzyskuje się działając na jego sygnał wejściowy funkcją aktywacji. W ten sposób w końcu mamy
\begin{equation*}
\begin{split} 
x^i_\alpha  = f(s^i_\alpha) = f \left (\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha} \right), \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l , \\
x^i_0 =1, \;\; i=1,\dots,l-1,  
\end{split}
\end{equation*}
\sphinxAtStartPar
z węzłami progowymi równymi jeden. Poniższy rysunek ilustruje naszą notację.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_29_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Implementacja propagacji feed\sphinxhyphen{}forward w Pythonie jest następująca:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: }
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    f \PYGZhy{} activation function (default: step)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: }
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of the neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary x}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal to x}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} loop over layers except the last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication }
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{f}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

                                  \PYG{c+c1}{\PYGZsh{} the output layer l \PYGZhy{} no adding of the bias node}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal   }
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{f}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dla zwięzłości przyjmujemy konwencję, w której nie przekazujemy w argumentach funkcji węzła progowego. Jest on wstawiany do funkcji za pomocą \sphinxstylestrong{np.insert(x\_in,0,1)}. Jak zwykle używamy \sphinxstylestrong{np.dot} do mnożenia macierzy.

\sphinxAtStartPar
Następnie testujemy, jak \sphinxstylestrong{feed\_forward} działa dla przykładowych danych wejściowych.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}0: array([ 1,  2, \PYGZhy{}1]), 1: array([1, 1, 0, 0, 0]), 2: [1]\PYGZcb{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Końcowy output z tej sieci jest uzyskany jako

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Dygresja o sieciach liniowych}
\label{\detokenize{docs/more_layers:dygresja-o-sieciach-liniowych}}
\sphinxAtStartPar
Zróbmy teraz następującą obserwację. Załóżmy, że mamy sieć z liniową funkcją aktywacji \(f(s)=c s\). Wtedy ostatnia formuła z powyższego wyprowadzenia przyjmuje postać
\begin{equation*}
\begin{split}
x^i_\alpha = c \sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l ,
\end{split}
\end{equation*}
\sphinxAtStartPar
lub w notacji macierzowej:
\begin{equation*}
\begin{split}
x^i = c x^{i-1}w^i.
\end{split}
\end{equation*}
\sphinxAtStartPar
Powtarzając to, otrzymujemy sygnał w warstwie wyjściowej
\begin{equation*}
\begin{split}
x^l = cx^{l-1} w^i = c^2 x^{l-2} w^{l-1} w^l =\dots= c^lx^0 w^1 w^2 \dots w^l =
x^0 W,
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(W=c^l w^1 w^2 \dots w^l\). Widzimy wiec, że taka sieć jest \sphinxstylestrong{równoważna} sieci jednowarstwowej z macierzą wag \(W\) określoną powyżej.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Z tego powodu nie ma sensu rozważanie sieci wielowarstwowych z liniową funkcją aktywacji.
\end{sphinxadmonition}


\section{Wizualizacja}
\label{\detokenize{docs/more_layers:wizualizacja}}
\sphinxAtStartPar
W celu wizualizacji prostych sieci w module \sphinxstylestrong{draw} pakietu \sphinxstylestrong{neural} udostępniamy kilka funkcji rysowania, które ukazują zarówno wagi, jak i sygnały w sieci. Funkcja \sphinxstylestrong{plot\_net\_w} rysuje wagi dodatnie na czerwono, a ujemne na niebiesko, przy czym szerokości odzwierciedlają ich wielkość. Ostatni parametr, tutaj 0.5, przeskalowuje szerokości tak, że grafika wygląda ładnie. Funkcja \sphinxstylestrong{plot\_net\_w\_x} drukuje dodatkowo wartości sygnału wychodzącego z węzłów każdej warstwy.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_40_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_41_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Klasyfikator z trzema warstwami neuronów}
\label{\detokenize{docs/more_layers:klasyfikator-z-trzema-warstwami-neuronow}}
\sphinxAtStartPar
Jesteśmy teraz gotowi do jawnego skonstruowania przykładu binarnego klasyfikatora punktów w obszarze wklęsłym: trójkąta z trójkątnym wycięciem z \hyperref[\detokenize{docs/more_layers:tri-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:tri-fig}}}.
Architektura sieci to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_44_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Warunki geometryczne i odpowiadające im wagi dla pierwszej warstwy neuronowej to


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
warunek
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^1\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\(x_1>0.1\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.1
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\(x_2>0.1\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.1
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
\(x_1+x_2<1\)
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
\(x_1>0.25\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.25
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
\(x_2>0.25\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.25
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
\(x_1+x_2<0.8\)
&
\sphinxAtStartPar
0.8
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Warunki 1\sphinxhyphen{}3 zapewniają granice dla większego trójkąta, a 4\sphinxhyphen{}6 dla mniejszego, zawartego w większym.
W drugiej warstwie neuronowej musimy zrealizować dwie bramki AND odpowiednio dla warunków 1\sphinxhyphen{}3 i 4\sphinxhyphen{}6, a zatem bierzemy


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{3\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{4\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{5\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{6\alpha}^2\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Na koniec w warstwie wyjściowej realizujemy bramkę \(p \wedge \! \sim\! q\), skąd


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^3\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^3\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^3\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
1.2
&
\sphinxAtStartPar
\sphinxhyphen{}0.6
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Łącząc to wszystko razem, uzyskujemy nastepujacy słownik wag:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
   \PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
   \PYG{l+m+mi}{3}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{1.2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Feed\sphinxhyphen{}forward dla prykładowego inputu daje

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mf}{0.3}\PYG{p}{]}
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_48_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Właśnie odkryliśmy, że punkt {[}0.2,0.3{]} znajduje się w naszym obszarze (1 z warstwy wyjściowej). Właściwie mamy tutaj więcej informacji z warstw pośrednich. Z drugiej warstwy neuronowej widzimy, że punkt należy do większego trójkąta (1 z dolnego neuronu), a nie należy do mniejszego trójkąta (0 z górnego neuronu). Z pierwszej warstwy neuronowej możemy odczytać warunki z sześciu nierówności.

\sphinxAtStartPar
Następnie testujemy działanie naszej sieci dla innych punktów. Najpierw definiujemy funkcję generującą losowy punkt w kwadracie \([0,1]\times [0,1]\) i propagujemy go przez sieć. Przypisujemy mu etykietę 1, jeśli należy do żądanego obszaru, a 0 w przeciwnym razie. Następnie tworzymy dużą próbkę takich punktów i generujemy grafikę, używając koloru różowego dla etykiety 1 i niebieskiego dla etykiety 0.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{po}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} random point from the [0,1]x[0,1] square}
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} feed forward}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{xi}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} the point\PYGZsq{}s coordinates and label}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{po}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.7088127  0.81098538 0.        ]
 [0.52044622 0.19265106 1.        ]
 [0.45637626 0.95670484 0.        ]
 [0.51723699 0.06066656 0.        ]
 [0.00879429 0.38239462 0.        ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_53_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Widzimy, że nasza maszynka działa doskonale!

\sphinxAtStartPar
W tym miejscu czytelnik może słusznie powiedzieć, że powyższe wyniki są trywialne: w istocie właśnie zaimplementowaliśmy pewne warunki geometryczne i ich koniunkcje.

\sphinxAtStartPar
Jednak, podobnie jak w przypadku sieci jednowarstwowych, istnieje ważny argument przeciwko tej pozornej błahości. Wyobraźmy sobie ponownie, że mamy próbkę danych z etykietami i tylko tele, podobnie jak w przykładzie pojedynczego neuronu MCP z rozdziału {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Neuron MCP}}}}. Wtedy na początku nie mamy warunków granicznych i potrzebujemy jakiegoś skutecznego sposobu, aby je znaleźć. Właśnie to zadanie wykonuje za nas \sphinxstylestrong{uczenie} klasyfikatorów: ustala wagi w taki sposób, że odpowiednie warunki są domyślnie wbudowane. Po materiale z tego rozdziału czytelnik powinien być przekonany, że jest to jak najbardziej możliwe i nie ma w tym nic magicznego! W następnym rozdziale pokażemy, jak to praktycznie zrobić.


\section{Ćwiczenia}
\label{\detokenize{docs/more_layers:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Zaprojektuj sieć i uruchom kod z tego wykładu dla wybranego regionu wypukłego.

\item {} 
\sphinxAtStartPar
Zaprojektuj i zaprogramuj klasyfikator dla czterech kategorii punktów należących do regionów utworzonych przez dwie przecinające się linie (wskazówka: uwzględnij wiecej komórek wyjściowych).

\end{itemize}
\end{sphinxadmonition}


\chapter{Propagacja wsteczna}
\label{\detokenize{docs/backprop:propagacja-wsteczna}}\label{\detokenize{docs/backprop::doc}}
\sphinxAtStartPar
W tym rozdziale pokażemy szczegółowo, jak przeprowadzić uczenie nadzorowane dla klasyfikatorów wielowarstwowych omówionych w rozdziale {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Więcej warstw}}}}. Ponieważ metoda opiera się na minimalizacji liczby błędnych odpowiedzi na próbce testowej, zaczynamy od dokładnego omówienia problemu minimalizacji błędów w naszej konfiguracji.


\section{Minimalizacja błędu}
\label{\detokenize{docs/backprop:minimalizacja-bledu}}
\sphinxAtStartPar
Przypomnijmy, że w naszym przykładzie z punktami na płaszczyźnie z rozdziału {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}} warunek dla różowych punktów był zadany przez nierówność

\sphinxAtStartPar
\(w_0+w_1 x_1 + w_2 x_2 > 0\).

\sphinxAtStartPar
Wspomnieliśmy już pokrótce o klasie równoważności związanej z dzieleniem obu stron tej nierówności przez dodatnią stałą \(c\). Ogólnie rzecz biorąc, co najmniej jedna z wag w powyższym warunku musi być niezerowa, aby był on nietrywialny. Załóżmy zatem, że \(w_0 \neq 0\) (inne przypadki można potraktować analogicznie). Następnie podzielmy obie strony nierówności przez \(|w_0|\), co daje
\begin{equation*}
\begin{split}\frac{w_0}{|w_0|}+\frac{w_1}{|w_0|} \, x_1 + \frac{w_2}{|w_0|} \, x_2 > 0. \end{split}
\end{equation*}
\sphinxAtStartPar
Wprowadzając notację \(v_1=\frac{w_1}{w_0}\) and \(v_2=\frac{w_2}{w_0}\), możemy zatem zapisać
\begin{equation*}
\begin{split}{\rm sgn}(w_0)( 1+v_1 \, x_1 +v_2 \, x_2) > 0,\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie znak \({\rm sgn}(w_0) = \frac{w_0}{|w_0|}\). Mamy więc w efekcie system dwuparametrowy (dla ustalonego znaku \(w_0\)).

\sphinxAtStartPar
Oczywiście przy pewnych wartościach \( v_1 \) i \( v_2 \) i dla danego punktu z próbki danych, perceptron poda w wyniku poprawną lub błędną odpowiedź. Naturalne jest zatem zdefiniowanie \sphinxstylestrong{funkcji błędu} \(E\) w taki sposób, że dla każdego punktu \(p\) próbki wnosi 1, jeśli odpowiedź jest niepoprawna, a 0, jeśli jest poprawna:
\begin{equation*}
\begin{split} E(v_1,v_2)=\sum_p \left\{ \begin{array}{ll} 1 -{\rm niepoprawna,~}\\ 0 -{\rm poprawna.} \end{array}\right .\end{split}
\end{equation*}
\sphinxAtStartPar
\(E\) ma zatem interpretację liczby źle sklasyfikowanych punktów.

\sphinxAtStartPar
Możemy łatwo skonstruować tę funkcję w Pythonie:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{error}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,} \PYG{n}{w1} \PYG{p}{,}\PYG{n}{w2}\PYG{p}{,} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    error function for the perceptron (for 2\PYGZhy{}dim data with labels)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    inputs:}
\PYG{l+s+sd}{    w0, w1, w2 \PYGZhy{} weights}
\PYG{l+s+sd}{    sample \PYGZhy{} array of labeled data points p }
\PYG{l+s+sd}{             p in an array in the format [x1, x1, label]}
\PYG{l+s+sd}{    f \PYGZhy{} activation function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    returns:}
\PYG{l+s+sd}{    error}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{er}\PYG{o}{=}\PYG{l+m+mi}{0}                                       \PYG{c+c1}{\PYGZsh{} initial value of the error}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} loop over data points       }
        \PYG{n}{yo}\PYG{o}{=}\PYG{n}{f}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} obtained answer}
        \PYG{n}{er}\PYG{o}{+}\PYG{o}{=}\PYG{p}{(}\PYG{n}{yo}\PYG{o}{\PYGZhy{}}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
                      \PYG{c+c1}{\PYGZsh{} sample[i,2] is the label}
                      \PYG{c+c1}{\PYGZsh{} adds the square of the difference of yo and the label}
                      \PYG{c+c1}{\PYGZsh{} this adds 1 if the answer is incorrect, and 0 if correct}
    \PYG{k}{return} \PYG{n}{er}  \PYG{c+c1}{\PYGZsh{} the error}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zastosowaliśmy tutaj małą sztuczkę, mając na uwadze przyszłe zastosowania. Oznaczając otrzymany wynik dla danego punktu danych jako \(y_o^{(p)}\), a wynik prawdziwy (etykietę) jako \(y_t^{(p)}\) (obydwa przyjmują wartości 0 lub 1), możemy zdefiniowane powyżej \(E\) zapisać równoważnie jako
\begin{equation*}
\begin{split} E(v_1,v_2)=\sum_p \left ( y_o^{(p)}-y_t^{(p)}\right )^2,\end{split}
\end{equation*}
\sphinxAtStartPar
co jest wzorem zaprogramowanym w kodzie. Rzeczywiście, kiedy \(y_o^{(p)}=y_t^{(p)}\) (prawidłowa odpowiedź), wkład punktu wynosi 0, a kiedy \(y_o^{(p)}\neq y_t^{(p) }\) (błędna odpowiedź), wkład wynosi \((\pm 1)^2=1\).

\sphinxAtStartPar
Powtarzamy teraz symulacje z podrozdziału {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}}, aby wygenerować etykietowaną próbkę danych \sphinxstylestrong{samp2} o 200 punktach (próbka jest utworzona z \(w_0=-0.25\), \(w_1=-0.52\) i \(w_2=1\), co odpowiada \(v_1=2.08\) i \(v_2=-4\), przy czym \({\rm sgn}(w_0)=-1\)).

\sphinxAtStartPar
Potrzebujemy teraz ponownie użyć algorytmu perceptronu z rozdz. {\hyperref[\detokenize{docs/perceptron:lab-pa}]{\sphinxcrossref{\DUrole{std,std-ref}{Algorytm perceptronu}}}}. W naszym szczególnym przypadku działa on na próbce dwuwymiarowych danych etykietowanych. Dla wygody, pojedyncza runda algorytmu może zostać zebrana w funkcję w następujący sposób:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{teach\PYGZus{}perceptron}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{w\PYGZus{}in}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Supervised learning for a single perceptron (single MCP neuron) }
\PYG{l+s+sd}{    for a sample of 2\PYGZhy{}dim. labeled data}
\PYG{l+s+sd}{       }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    sample \PYGZhy{} array of two\PYGZhy{}dimensional labeled data points p}
\PYG{l+s+sd}{             p is an array in the format [x1,x2,label]}
\PYG{l+s+sd}{             label = 0 or 1}
\PYG{l+s+sd}{    eps    \PYGZhy{} learning speed}
\PYG{l+s+sd}{    w\PYGZus{}in   \PYGZhy{} initial weights in the format [[w0], [w1], [w2]]}
\PYG{l+s+sd}{    f      \PYGZhy{} activation function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: updated weights in the format [[w0], [w1], [w2]]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}\PYG{o}{=}\PYG{n}{w\PYGZus{}in}         \PYG{c+c1}{\PYGZsh{} define w0, w1, and w2}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} loop over the whole sample}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} repeat 10 times  }
            
            \PYG{n}{yo}\PYG{o}{=}\PYG{n}{f}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} output from the neuron, f(x.w)}
            
            \PYG{c+c1}{\PYGZsh{} update of weights according to the perceptron algorithm formula}
            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{1}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
            
    \PYG{k}{return} \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} updated weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie prześledzimy działanie algorytmu perceptronu, obserwując jak modyfikuje on wartości wprowadzonej powyżej funkcji błędu \(E(v_1,v_2)\). Zaczynamy od losowych wag, a następnie wykonujemy 10 rund zdefiniowanej powyżej funkcji \sphinxstylestrong{teach\_perceptron}, wypisując zaktualizowane wagi i odpowiadający im błąd:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} initial random weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.30195756389159656], [0.3291266701139506], [0.04932223362936672]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Optimum:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0  w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                 \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{o}{*}\PYG{n}{eps}         \PYG{c+c1}{\PYGZsh{} decrease the learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}perceptron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)} 
                        \PYG{c+c1}{\PYGZsh{} see the top of this chapter}
        
    \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} updated weights and ratios}
    \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
          \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}             
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Optimum:
   w0  w1/w0  w2/w0 error
\PYGZhy{}0.958 0.786 \PYGZhy{}2.956 22.0
\PYGZhy{}0.958 1.618 \PYGZhy{}3.619 3.0
\PYGZhy{}0.958 1.765 \PYGZhy{}3.823 3.0
\PYGZhy{}0.958 1.876 \PYGZhy{}4.006 6.0
\PYGZhy{}0.958 1.918 \PYGZhy{}4.159 10.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\PYGZhy{}0.958 2.15 \PYGZhy{}4.073 0.0
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważamy, że w kolejnych rundach błąd stopniowo maleje (w zależności od symulacji, może czasem nieco podskoczyć, jeśli szybkość uczenia się jest zbyt duża, ale nie stanowi to problemu, o ile koniec końców możemy zejść do minimum), osiągając ostatecznie wartość bardzo małą lub dokładnie 0 (w zależności od konkretnego przypadku symulacji). W związku z tym
algorytm perceptronu, jak już widzieliśmy w rozdziale {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}}, \sphinxstylestrong{minimalizuje błąd dla próbki treningowej}.

\sphinxAtStartPar
Pouczające jest spojrzenie na mapę konturową funkcji błędu \(E(v_1, v_2)\) w pobliżu optymalnych parametrów:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{3.7}\PYG{p}{,}\PYG{l+m+mf}{3.7}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{delta} \PYG{o}{=} \PYG{l+m+mf}{0.02}  \PYG{c+c1}{\PYGZsh{} grid step in v1 and v2 for the contour map}
\PYG{n}{ran}\PYG{o}{=}\PYG{l+m+mf}{0.8}       \PYG{c+c1}{\PYGZsh{} plot range around (v1\PYGZus{}o, v2\PYGZus{}o)}

\PYG{n}{v1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,}\PYG{n}{v1\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} grid for v1}
\PYG{n}{v2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,}\PYG{n}{v2\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} grid for v2}
\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{v1}\PYG{p}{,} \PYG{n}{v2}\PYG{p}{)}               \PYG{c+c1}{\PYGZsh{} mesh for the contour plot}

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{error}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{n}{v1}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{n}{v2}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)} 
             \PYG{c+c1}{\PYGZsh{} we use the scaling property of the error function here }
             \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{v1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{v2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} values of E(v1,v2) }

\PYG{n}{CS} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{35}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
                        \PYG{c+c1}{\PYGZsh{} explicit contour level values}
    
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{clabel}\PYG{p}{(}\PYG{n}{CS}\PYG{p}{,} \PYG{n}{inline}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}1.0f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} contour label format}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}aspect}\PYG{p}{(}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}v\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}v\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{found minimum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} our found optimal point}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_18_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Uzyskane minimum znajduje się wewnątrz (lub blisko, w zależności od symulacji) wydłużonego obszaru w \(v_1\) i \(v_2\), gdzie błąd znika.


\section{Ciągła funkcja aktywacji}
\label{\detokenize{docs/backprop:ciagla-funkcja-aktywacji}}
\sphinxAtStartPar
Przyglądając się uważniej powyższej mapie konturowej, widzimy, że linie są „ząbkowane”. Dzieje się tak, ponieważ funkcja błędu, z oczywistego powodu, przyjmuje wartości całkowite. Jest zatem nieciągła, a zatem nieróżniczkowalna. Nieciągłości wynikają z nieciągłej funkcji aktywacji, mianowicie funkcji schodkowej. Mając na uwadze techniki, które poznamy niebawem, korzystne jest stosowanie funkcji aktywacji, która jest różniczkowalna. Historycznie tzw. \sphinxstylestrong{sigmoid}
\begin{equation*}
\begin{split} \sigma(s)=\frac{1}{1+e^{-s}}\end{split}
\end{equation*}
\sphinxAtStartPar
był wykorzystywany w wielu praktycznych zastosowaniach dla ANN.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} sigmoid, a.k.a. the logistic function, or simply (1+arctanh(\PYGZhy{}s/2))/2 }
\PYG{k}{def} \PYG{n+nf}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_23_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Funkcja ta jest oczywiście różniczkowalna. Ponadto
\begin{equation*}
\begin{split} \sigma '(s) = \sigma (s) [1- \sigma (s)], \end{split}
\end{equation*}
\sphinxAtStartPar
co jest szczególna własnością sigmoidu.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} derivative of sigmoid}
\PYG{k}{def} \PYG{n+nf}{dsig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
     \PYG{k}{return} \PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_26_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Wprowadza się również sigmoid z „temperaturą” \(T \) (nomenklatura ta jest związana z podobnymi wyrażeniami dla funkcji termodynamicznych w fizyce):
\$\(\sigma(s;T)=\frac{1}{1+e^{-s/T}}.\)\$

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} sigmoid with temperature T}
\PYG{k}{def} \PYG{n+nf}{sig\PYGZus{}T}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{T}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{o}{/}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_29_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Dla coraz mniejszych \(T\) sigmoid zbliża się do poprzednio używanej funkcji schodkowej.

\sphinxAtStartPar
Zauważ, że argumentem sigmoidu jest iloraz
\begin{equation*}
\begin{split}
s/T = (w_0 + w_1 x_1 + w_2 x_2) / T = w_0 / T + w_1 / T \, x_1 + w_2 / T \, x_2 = \xi_0 + \xi_1 x_1 + \xi_2 x_2,
\end{split}
\end{equation*}
\sphinxAtStartPar
co oznacza, że zawsze możemy przyjąć \(T = 1\) bez utraty ogólności (\(T \) to „skala”). Jednak teraz mamy trzy niezależne argumenty \( \xi_0 \), \( \xi_1 \) i \(\xi_2\), więc nie można zredukować obecnej sytuacji do tylko dwóch niezależnych parametrów, jak miało to miejsce w poprzednim podrozdziale.

\sphinxAtStartPar
Powtórzymy teraz nasz przykład z klasyfikatorem, ale z funkcją aktywacji daną przez sigmoid. Funkcja błędu
\begin{equation*}
\begin{split}y_o^{(p)}=\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)}), \end{split}
\end{equation*}
\sphinxAtStartPar
staje się teraz
\begin{equation*}
\begin{split}E(w_0,w_1,w_2)=\sum_p \left [\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)} \right] ^2.\end{split}
\end{equation*}
\sphinxAtStartPar
Algorytm perceptronu z funkcją aktywacji sigmoidu wykonujemy 1000 razy, wypisując co 100 krok:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} random weights from [\PYGZhy{}0.5,0.5]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0   w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                       \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9995}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}perceptron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update weights}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99:
        \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} updated weights }
        \PYG{n}{w1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{w2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{w1\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}                   \PYG{c+c1}{\PYGZsh{} ratios of weights}
        \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{w2\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}                             
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   w0   w1/w0  w2/w0 error
\PYGZhy{}21.633 1.908 \PYGZhy{}3.966 2.35679
\PYGZhy{}27.248 1.962 \PYGZhy{}3.994 1.77834
\PYGZhy{}30.854 1.99 \PYGZhy{}4.008 1.45645
\PYGZhy{}33.559 2.007 \PYGZhy{}4.014 1.23376
\PYGZhy{}35.751 2.017 \PYGZhy{}4.017 1.06907
\PYGZhy{}37.604 2.022 \PYGZhy{}4.016 0.94205
\PYGZhy{}39.211 2.024 \PYGZhy{}4.013 0.84068
\PYGZhy{}40.629 2.025 \PYGZhy{}4.009 0.75752
\PYGZhy{}41.896 2.025 \PYGZhy{}4.005 0.68783
\PYGZhy{}43.037 2.024 \PYGZhy{}4.0 0.62847
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Obserwujemy, zgodnie z oczekiwaniami, stopniowy spadek błędu w miarę postępu symulacji. Ponieważ funkcja błędu ma teraz trzy niezależne argumenty, nie można jej narysować w dwóch wymiarach. Możemy jednak pokazać jej rzut, np. dla ustalonej wartości \( w_0 \), co robimy poniżej:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{3.7}\PYG{p}{,}\PYG{l+m+mf}{3.7}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{delta} \PYG{o}{=} \PYG{l+m+mf}{0.5}
\PYG{n}{ran}\PYG{o}{=}\PYG{l+m+mi}{40} 
\PYG{n}{r1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{w1\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{w1\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} 
\PYG{n}{r2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{w2\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{w2\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} 
\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{r1}\PYG{p}{,} \PYG{n}{r2}\PYG{p}{)} 

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{n}{r1}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{r2}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)} 
             \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{r1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{r2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}  

\PYG{n}{CS} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{35}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{clabel}\PYG{p}{(}\PYG{n}{CS}\PYG{p}{,} \PYG{n}{inline}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}1.0f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error function for \PYGZdl{}w\PYGZus{}0\PYGZdl{}=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}aspect}\PYG{p}{(}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}w\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}w\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{w1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w2\PYGZus{}o}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{found minimum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} our found optimal point}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_34_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
W miarę jak wykonujemy coraz więcej iteracji, zauważamy, że wielkość wag rośnie, podczas gdy błąd naturalnie się zmniejsza. Powodem jest to, że nasza próbka danych jest separowalna, więc w przypadku użycia schodkowej funkcji aktywacji możliwe jest rozdzielenie próbki linią podziału i zejście z błędem aż do zera. W przypadku sigmoidu, zawsze istnieje pewien (niewielki) wkład do błędu, ponieważ wartości funkcji mieszczą się w sposób ciągły w przedziale (0,1). Jak omówiliśmy powyżej, w sigmoidzie, którego argumentem jest \( (w_0 + w_1 x_1 + w_2 x_2) / T\), zwiększanie wag jest równoznaczne ze zmniejszaniem temperatury \(T\). W moare postępu symulacji sigmoid zbliża się zatem do funkcji schodkowej, a błąd dąży do zera. Zachowanie to jest widoczne w powyższych symulacjach.
\end{sphinxadmonition}


\section{Najstromszy spadek}
\label{\detokenize{docs/backprop:najstromszy-spadek}}
\sphinxAtStartPar
Powodem dla powyższych symulacji było doprowadzenie czytelnika do wniosku, że zagadnienie optymalizacji wag można sprowadzić do ogólnego problemu minimalizacji funkcji wielu zmiennych. Jest to standardowy (choć na ogół trudny) problem w analizie matematycznej i metodach numerycznych. Problemy związane ze znalezieniem minimum funkcji wielu zmiennych są dobrze znane:
\begin{itemize}
\item {} 
\sphinxAtStartPar
mogą istnieć minima lokalne, dlatego znalezienie minimum globalnego może być bardzo trudne;

\item {} 
\sphinxAtStartPar
minimum może być w nieskończoności (czyli matematycznie nie istnieć);

\item {} 
\sphinxAtStartPar
Funkcja wokół minimum może być bardzo płaska, tj. jej gradient jest bardzo mały. Wówczas znajdowanie minimum z pomocą metod gradientowych jest bardzo powolne;

\end{itemize}

\sphinxAtStartPar
Ogólnie rzecz biorąc, minimalizacja numeryczna funkcji to sztuka! Opracowano tu wiele metod, a właściwy dobór do danego problemu ma kluczowe znaczenie dla sukcesu. Poniżej zastosujemy najprostszy wariant, tzw. metodę \sphinxstylestrong{najstromszego spadku}.

\sphinxAtStartPar
Dla różniczkowalnej funkcji wielu zmiennych \( F (z_1, z_2, ..., z_n) \), lokalnie najbardziej strome nachylenie jest okreslone przez minus gradient funkcji \( F \),
\$\(-\left (\frac{\partial F}{\partial z_1}, \frac{\partial F}{\partial z_2}, ..., 
\frac{\partial F}{\partial z_n} \right ), \)\$

\sphinxAtStartPar
gdzie pochodne cząstkowe definiuje się jako granice
\begin{equation*}
\begin{split}\frac{\partial F}{\partial z_1} = \lim _ {\Delta \to 0} \frac {F (z_1 + \Delta, z_2, ..., z_n) -F (z_1, z_2, ..., z_n)} { \Delta } \end{split}
\end{equation*}
\sphinxAtStartPar
i podobnie dla pozostałych \( z_i \).

\sphinxAtStartPar
Metoda znajdowania minimum funkcji poprzez najstromszy spadek zadana jest przez algorytm iteracyjny, w którym aktualizujemy współrzędne (wyszukiwanego minimum) w każdym kroku iteracji \(m\) (górny wskaźnik) w nastepujacy sposób:
\begin{equation*}
\begin{split}z_{i}^{(m+1)} = z_i^{(m)} - \epsilon  \, \frac{\partial F}{\partial z_i}. \end{split}
\end{equation*}
\sphinxAtStartPar
W naszym zagadnieniu potrzebujemy zminimalzować funcję błedu
\begin{equation*}
\begin{split}E(w_0,w_1,w_2)= \sum_p [y_o^{(p)}-y_t^{(p)}]^2=\sum_p [\sigma(s^{(p)})-y_t^{(p)}]^2=\sum_p [\sigma(w_0  x_0^{(p)}+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)}]^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Aby obliczyć pochodne, stosujemy \sphinxstylestrong{twierdzenie o pochodnej funkcji złożonej}.

\begin{sphinxadmonition}{note}{Tw. o pochodnej funkcji złożonej}

\sphinxAtStartPar
Dla funkcji złożonej

\sphinxAtStartPar
\([f(g(x))]' = f'(g(x)) g'(x)\).

\sphinxAtStartPar
Dla złożenia większej liczby funkcji \([f(g(h(x)))]' = f'(g(h(x))) \,g'(h(x)) \,h'(x)\) itp.
\end{sphinxadmonition}

\sphinxAtStartPar
Prowadzi to do wzoru
\begin{equation*}
\begin{split} \frac{\partial E}{\partial w_i} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma'(s^{(p)}) \,x_i^{(p)} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma(s^{(p)})\, [1-\sigma(s^{(p)})] \,x_i^{(p)}\end{split}
\end{equation*}
\sphinxAtStartPar
(pochodna funkcji kwadratowej \( \times \) pochodna sigmoidu \( \times \) pochodna \( s ^ {(p)} \)), gdzie w ostatniej równości użyliśmy specjalnej własności pochodnej sigmoidu. Metoda najstromszego spadku aktualizuje zatem wagi w następujący sposób:
\begin{equation*}
\begin{split}w_i \to w_i - \varepsilon (y_o^{(p)} -y_t^{(p)}) y_o^{(p)} (1-y_o^{(p)}) x_i.\end{split}
\end{equation*}
\sphinxAtStartPar
Zauważmy, że aktualizacja zawsze występuje, ponieważ odpowiedź \( y_o^ {(p)} \) nigdy nie jest ściśle równa 0 lub 1, podczas gdy
prawdziwa wartość (etykieta) \( y_t ^ {(p)} \) wynosi 0 lub 1.

\sphinxAtStartPar
Ponieważ \( y_o ^ {(p)} (1-y_o ^ {(p)}) = \sigma (s ^ {(p)}) [1- \sigma (s ^ {(p)})] \) jest istotnie różne od zera tylko w okolicy \( s ^ {(p)} = 0\) (patrz wcześniejszy wykres pochodnej sigmoidu), znacząca aktualizacja następuje tylko w pobliżu progu. To cecha jest odpowiednia, ponieważ problemy z błędną klasyfikacją zdarzają się właśnie w pobliżu linii podziału.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Dla porównania, wcześniejszy algorytm perceptronu jest strukturalnie bardzo podobny,
\begin{equation*}
\begin{split}w_i \to w_i - \varepsilon \,(y_o^{(p)} - y_t^{(p)}) \, x_i,\end{split}
\end{equation*}
\sphinxAtStartPar
ale tutaj aktualizacja następuje dla wszystkich punktów próbki, a nie tylko tych w pobliżu linii podziału.
\end{sphinxadmonition}

\sphinxAtStartPar
Kod algorytmu uczenia naszego perceptronu metodą najstromszyego spadku jest następujący:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{teach\PYGZus{}sd}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{w\PYGZus{}in}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Steepest descent for the perceptron}
    
    \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}\PYG{o}{=}\PYG{n}{w\PYGZus{}in}              \PYG{c+c1}{\PYGZsh{} initial weights}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} loop over the data sample}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}            \PYG{c+c1}{\PYGZsh{} repeat 10 times }
            
            \PYG{n}{yo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} obtained answer for pont i}

            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{1}            \PYG{c+c1}{\PYGZsh{} update of weights}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{k}{return} \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jego wydajność jest podobna do oryginalnego algorytmu perceptronu badanego powyżej:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} random weights from [\PYGZhy{}0.5,0.5]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0   w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                       \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9995}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}sd}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update weights}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99:
        \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} updated weights }
        \PYG{n}{w1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{w2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{w1\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{w2\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}                                          
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   w0   w1/w0  w2/w0 error
\PYGZhy{}10.373 1.939 \PYGZhy{}3.972 2.70531
\PYGZhy{}13.308 1.966 \PYGZhy{}3.971 1.96428
\PYGZhy{}15.208 1.983 \PYGZhy{}3.975 1.62302
\PYGZhy{}16.622 1.995 \PYGZhy{}3.978 1.41622
\PYGZhy{}17.754 2.003 \PYGZhy{}3.979 1.27377
\PYGZhy{}18.699 2.008 \PYGZhy{}3.98 1.16816
\PYGZhy{}19.51 2.011 \PYGZhy{}3.979 1.08604
\PYGZhy{}20.22 2.014 \PYGZhy{}3.978 1.02005
\PYGZhy{}20.85 2.015 \PYGZhy{}3.976 0.96571
\PYGZhy{}21.414 2.016 \PYGZhy{}3.974 0.92013
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Podsumowując dotychczasowy materiał, wykazaliśmy, że można skutecznie uczyć jednowarstwy perceptron (pojedynczy neuronu MCP) za pomocą metody najstromszego spadku, minimalizując funkcję błędu generowaną przez badaną próbkę. W następnym podrozdziale uogólnimy ten pomysł na dowolny wielowarstwową sieć typu feed\sphinxhyphen{}forward.


\section{Algorytm propagacji wstecznej (backprop)}
\label{\detokenize{docs/backprop:algorytm-propagacji-wstecznej-backprop}}\label{\detokenize{docs/backprop:bpa-lab}}
\sphinxAtStartPar
Materiał tego podrozdziału jest absolutnie \sphinxstylestrong{kluczowy} dla zrozumienia idei uczenia sieci neuronowych poprzez uczenie nadzorowane. Jednocześnie dla czytelnika mniej zaznajomionego z analizą matematyczną może być dość trudny, ponieważ pojawiają się wyprowadzenia i wzory z bogatą notacją. Nie udało się jednak znaleźć sposobu na przedstawienie materiału w prostszy sposób niż poniżej, z jednoczesnym zachowaniem niezbędnego rygoru.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Formuły, które wyprowadzamy tutaj krok po kroku, stanowią słynny \sphinxstylestrong{algorytm wstecznej propagacji (backprop)} {[}\hyperlink{cite.docs/conclusion:id12}{BH69}{]} dla aktualizacji wag perceptronu wielowarstwowego. Wykorzystujemy tylko dwa podstawowe fakty:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{tw. o pochodnej funkcji złożonej} do obliczania pochodnej, oraz

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{metodę najstromszego spadku}, wyjaśnioną w poprzednim podrozdziale.

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
Rozważmy perceptron z dowolną liczbą warstw neuronowych, \(l\). Neurony w warstwach pośrednich \(j=1,\dots,l-1\) są ponumerowane odpowiednimi wskaźnikami \(\alpha_j=0,\dots,n_j\), gdzie 0 oznacza węzeł progowy. W warstwie wyjściowej, nie zawierającej węzła progowego, wskaźnik przyjmuje wartości \(\alpha_l=1,\dots,n_l\). Na przykład sieć z wykresu poniżej ma
\begin{equation*}
\begin{split}l=4, \; \; \alpha_1=0,\dots,4, \;\; \alpha_2=0,\dots,5, \;\; \alpha_3=0,\dots,3, \;\; \alpha_4=1,\dots,2,\end{split}
\end{equation*}
\sphinxAtStartPar
ze wskaźnikami w każdej warstwie liczonymi od dołu.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_49_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Funkcja błędu to suma po punktach próbki treningowej oraz dodatkowo po węzłach w warstwie wyjściowej:
\begin{equation*}
\begin{split}
E(\{w\})=\sum_p \sum_{\alpha_l=1}^{n_l} \left[ y_{o,{\alpha_l}}^{(p)}(\{w\})-y_{t,{\alpha_l}}^{(p)}\right]^2,
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \( \{w \} \) reprezentują wszystkie wagi sieci.
Pojedynczy wkład punktu \(p\) do \(E\), oznaczony jako \(e\), to
suma po wszystkich neuronach w warstwie wyjściowej:
\begin{equation*}
\begin{split}
e(\{w\})= \sum_{{\alpha_l}=1}^{n_l}\left[ y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right]^2. 
\end{split}
\end{equation*}
\sphinxAtStartPar
Dla zwięzłości, opuściliśmy górny wskaźnik \((p)\).
Dla neuronu \(\alpha_j\) w warstwie \(j\) sygnałem wejściowym jest
\begin{equation*}
\begin{split}
s_{\alpha_j}^{j}=\sum_{\alpha_{j-1}=0}^{n_{j-1}} x_{\alpha_{j-1}}^{j-1} w_{\alpha_{j-1} \alpha_j}^{j}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Sygnały w warstwie wyjściowej mają postać
\begin{equation*}
\begin{split}
y_{o,{\alpha_l}}=f\left( s_{\alpha_l}^{l} \right)
\end{split}
\end{equation*}
\sphinxAtStartPar
natomiast sygnały wyjściowe w warstwach pośrednich \(j=1,\dots,l-1\) to
\begin{equation*}
\begin{split}
x_{\alpha_j}^{j}=f \left ( s_{\alpha_j}^{j}\right ),\;\;\;\alpha_{j}=1,\dots,n_j, \;\; \; {\rm i} \;\;\; x_0^{j}=1,
\end{split}
\end{equation*}
\sphinxAtStartPar
z węzłem progowym mającym wartość 1.

\sphinxAtStartPar
Kolejne podstawienia powyższych formuł do \(e\) są następujące:

\sphinxAtStartPar
\(e = \sum_{{\alpha_l}=1}^{n_l}\left( y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( f \left (\sum_{\alpha_{l-1}=0}^{n_{l-1}} x_{\alpha_{l-1}}^{l-1} w_{\alpha_{l-1} {\alpha_l}}^{l} \right )-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f \left( \sum_{\alpha_{l-2}=0}^{n_{l-2}} x_{\alpha_{l-2}}^{l-2} w_{\alpha_{l-2} \alpha_{l-1}}^{l-1}\right) w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 \gamma}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\sum_{\alpha_{l-2}=1}^{n_{l-2}} f\left( \sum_{\alpha_{l-3}=0}^{n_{l-3}} x_{\alpha_{l-3}}^{l-3} w_{\alpha_{l-3} \alpha_{l-2}}^{l-2}\right) w_{\alpha_{l-2} \alpha_{l-1}}^{l-1} + 
x_{0}^{l-2} w_{0 \alpha_{l-1}}^{l-1}
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\dots f\left( \sum_{\alpha_{0}=0}^{n_{0}} x_{\alpha_{0}}^{0} w_{\alpha_{0} \alpha_{1}}^{1}\right) w_{\alpha_{1} \alpha_{2}}^{2} + 
x_{0}^{1} w_{0 \alpha_{2}}^{2} \dots
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
Obliczając kolejne pochodne względem wag idąc wstecz, tj. od \(j=l\) do 1, otrzymujemy (patrz ćwiczenia)
\begin{equation*}
\begin{split}
\frac{\partial e}{\partial w^j_{\alpha_{j-1} \alpha_j}} = x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j} , \;\;\; \alpha_{j-1}=0,\dots,n_{j-1}, \;\; \alpha_{j}=1,\dots,n_{j},
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie

\sphinxAtStartPar
\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f'(s_{\alpha_l}^{l})\),

\sphinxAtStartPar
\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\).

\sphinxAtStartPar
Ostatnie wyrażenie to rekurencja wstecz. Zauważamy, że aby uzyskać \(D^j\), potrzebujemy \(D^{j+1}\), które uzyskaliśmy już w poprzednim kroku, oraz sygnał \(s^j\), który znamy z propagacji sygnału do przodu. Ta rekurencja prowadzi do uproszczenia obliczania pochodnych i aktualizacji wag.

\sphinxAtStartPar
Przy najstromszym spadku wagi są aktualizowane jako
\begin{equation*}
\begin{split} w^j_{\alpha_{j-1} \alpha_j} \to  w^j_{\alpha_{j-1} \alpha_j} -\varepsilon x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j}, \end{split}
\end{equation*}
\sphinxAtStartPar
W przypadku sigmoidu możemy użyć
\begin{equation*}
\begin{split}
\sigma'(s_A^{(i)})=\sigma'(s_A^{(i)}) (1-\sigma'(s_A^{(i)})) =x_A^{(i)}(1-x_A^{(i)}).
\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Powyższe formuły wyjaśniają nazwę \sphinxstylestrong{propagacja wsteczna}, ponieważ w aktualizacji wag zaczynamy od ostatniej warstwy, a następnie posuwamy się rekurencyjnie do początku sieci. Na każdym kroku potrzebujemy tylko sygnału w danej warstwie i właściwości kolejnej warstwy! Te cechy wynikają z
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
charakteru feed\sphinxhyphen{}forward sieci oraz

\item {} 
\sphinxAtStartPar
tw. o pochodnej funkcji złożonej.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Praktyczne znaczenie cofania się warstwa po warstwie polega na tym, że w jednym kroku aktualizuje się znacznie mniej wag: tylko te, które wchodzą do danej warstwy, a nie wszystkie naraz. Ma to znaczenie dla zbieżności metody najstromszego spadku, zwłaszcza dla sieci głębokich (o wielu warswach).
\end{sphinxadmonition}

\sphinxAtStartPar
Jeżeli funkcje aktywacji są różne w różnych warstwach (oznaczamy je \(f_j\) dla warstwy \(j\)), to zachodzi oczywista modyfikacja:

\sphinxAtStartPar
\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f_l'(s_{\alpha_l}^{l})\),

\sphinxAtStartPar
\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f_j'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\).

\sphinxAtStartPar
Nie jest to rzadkie, ponieważ w wielu zastosowaniach wybiera się różne funkcje aktywacji dla warstw pośrednich i warswy wyjściowej.


\subsection{Kod dla algorytmu backprop}
\label{\detokenize{docs/backprop:kod-dla-algorytmu-backprop}}
\sphinxAtStartPar
Następnie przedstawimy prosty kod realizujący algorytm backprop. Jest to bezpośrednia implementacja wyprowadzonych powyżej formuł. W kodzie zachowujemy jak najwięcej notacji z powyższego wyprowadzenia.

\sphinxAtStartPar
Kod ma tylko 12 linijek, nie licząc komentarzy!

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} disctionary of weights}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed }
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivaive of f}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
 
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer  }
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                 
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                    \PYG{n}{df}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}   
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} 
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}           
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)} 
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}          
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)} 
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}      
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Przykład z kołem}
\label{\detokenize{docs/backprop:przyklad-z-kolem}}\label{\detokenize{docs/backprop:circ-lab}}
\sphinxAtStartPar
Kod ilustrujemy na przykładzie klasyfikatora binarnego punktów wewnątrz okręgu.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{cir}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} coordinate 1}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} coordinate 2}
    \PYG{k}{if}\PYG{p}{(}\PYG{p}{(}\PYG{n}{x1}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} inside circle, radius 0.4, center (0.5,0.5)}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}                                  \PYG{c+c1}{\PYGZsh{} outside}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Do przyszłego użytku \sphinxstylestrong{(nowa konwencja)} podzielimy próbkę na oddzielne tablice \sphinxstylestrong{cech} (dwie współrzędne) i \sphinxstylestrong{etykiet} (1, jeśli punkt znajduje się wewnątrz okręgu, 0 w przeciwnym razie):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{cir}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} sample}
\PYG{n}{features\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{labels\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{,}\PYG{n}{norm}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{colors}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_62_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dobieramy następującą architekturę i początkowe parametry:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch\PYGZus{}c}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                  \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} scaled random initial weights in [\PYGZhy{}2,2]}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.7}                            \PYG{c+c1}{\PYGZsh{} initial learning speed }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_65_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Symulacja zabiera kilka minut.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.995}\PYG{o}{*}\PYG{n}{eps}        \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                       \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} backprop}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  400  500  600  700  800  900  1000  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zmniejszenie szybkości uczenia się w każdej rundzie daje końcową wartość \(\varepsilon\), która powinna być niewielka, ale nie za mała:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.004657778005182377
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
(zbyt mała wartość aktualizowałaby wagi w znikomy sposób, więc dalsze rundy byłyby bezużyteczne).

\sphinxAtStartPar
Podczas gdy faza nauki była dość długa, testowanie i używanie wytrenowanej sieci przebiega bardzo szybko:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{test}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]} 

\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{po}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} 
    \PYG{n}{xt}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{po}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}   
    \PYG{n}{test}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{[}\PYG{n}{po}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{po}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{xt}\PYG{p}{[}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{tt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{test}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} drawing the circle}
\PYG{n}{ax}\PYG{o}{=}\PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{circ}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{Circle}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{radius}\PYG{o}{=}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{add\PYGZus{}patch}\PYG{p}{(}\PYG{n}{circ}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{,}\PYG{n}{norm}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{colors}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_72_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Wytrenowana sieć wygląda następująco:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fnet}\PYG{o}{=}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{l+m+mf}{.1}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
To fascynujące, że nauczyliśmy sieć rozpoznawać, czy punkt znajduje się w okręgu, a nie ma ona żadnego pojęcia o geometrii, odległości euklidesowej, równaniu okręgu itp. Sieć właśnie nauczyła się „empirycznie”, jak postępować, za pomocą próbki szkoleniowej!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Wynik przedstawiony na rysunku jest całkiem niezły, może z wyjątkiem, jak zwykle, punktów blisko granicy. Biorąc pod uwagę naszą dyskusję w rozdz. {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Więcej warstw}}}}, w którym wyznaczyliśmy wagi sieci z trzema warstwami neuronów na podstawie rozważań geometrycznych, jakość prezentowanego wyniku jest oszałamiająca. Nie widzimy żadnych prostych boków wielokąta, ale ładnie zaokrągloną granicę. Dalsza poprawa wyniku wymagałaby większej liczebności próbki szkoleniowej i dłuższego treningu, co jest czasochłonne.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Lokalne minima}

\sphinxAtStartPar
Wspomnieliśmy wcześniej o pojawianiu się minimów lokalnych w optymalizacji wielowymiarowej jako o potencjalnym problemie. Na poniższym rysunku pokazujemy trzy różne wyniki kodu backprop dla naszego klasyfikatora punktów w okręgu. Zauważamy, że każdy z nich ma radykalnie inny zestaw optymalnych wag, podczas gdy spawdzenie na próbce testowej jest, przynajmniej na oko, równie dobre dla każdego przypadku. To pokazuje, że optymalizacja backprop prowadzi, zgodnie z przewidywaniami, do różnych minimów lokalnych. Jednak każde z nich działa wystarczająco i równie dobrze. To jest właśnie powód, dla którego algorytm backprop można wykorzystać w praktycznych problemach: istnieją miliony lokalnych minimów, ale to naprawdę nie ma znaczenia!
\end{sphinxadmonition}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_76_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\section{Ogólne uwagi}
\label{\detokenize{docs/backprop:ogolne-uwagi}}
\sphinxAtStartPar
Należy poczynić kilka istotnych i ogólnych obserwacji:

\begin{sphinxadmonition}{note}{Informacja:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Uczenie nadzorowane zajmuje bardzo dużo czasu, ale użycie wytrenowanej sieci trwa mgnienie oka. Asymetria wynika z prostego faktu, że optymalizacja wieloparametrowa wymaga bardzo wielu wywołań funkcji (tutaj \sphinxstylestrong{feed\sphinxhyphen{}forward}) i obliczneia pochodnych w wielu rundach (użyliśmy 1000 rund dla przykładu okręgu), ale użycie sieci dla przypadku jednego punktu wymaga tylko jednego wywołania funkcji.

\item {} 
\sphinxAtStartPar
Klasyfikator wyszkolony algorytmem backprop może działać niedokładnie dla punktów w pobliżu linii granicznych. Środkiem zaradczym jest dłuższe trenowanie i/lub zwiększenie
liczebności próbki szkoleniowej, w szczególności w pobliżu granicy.

\item {} 
\sphinxAtStartPar
Jednak zbyt długa nauka na tej samej próbce treningowej nie ma sensu, ponieważ w pewnym momencie dokładność przestaje się poprawiać.

\item {} 
\sphinxAtStartPar
Lokalne minima są powszecjne, ale w żadnym wypadku nie stanowi to przeszkody w stosowaniu algorytmu. To ważna praktyczna cecha.

\item {} 
\sphinxAtStartPar
Można stosować różne ulepszenia metody najstromszego spadku lub zupełnie inne metody minimalizacji (patrz ćwiczenia). Mogą one znacznie zwiększyć wydajność algorytmu.

\item {} 
\sphinxAtStartPar
Cofając się z aktualizacją wag w kolejnych warstwach, można wprowadzić współczynnik zwiększający uaktualnianie (patrz ćwiczenia). To pomaga w wydajności.

\item {} 
\sphinxAtStartPar
Wreszcie, inne funkcje aktywacji mogą być używane do poprawy wydajności (patrz kolejne wykłady).

\end{itemize}
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/backprop:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Udowodnij (analitycznie), obliczając pochodną, że \( \sigma '(s) = \sigma (s) [1- \sigma (s)]\). Pokaż, że sigmoid jest \sphinxstylestrong{jedyną} funkcją z tą właściwością.

\item {} 
\sphinxAtStartPar
Wyprowadź jawnie wzory algorytmu backprop dla sieci z jedną i dwiema warstwami pośrednimi. Zwróć uwagę na pojawiającą się prawidłowość (powtarzalność) i udowodnij ogólne wzory z wykładu dla dowolnej liczby warstw pośrednich.

\item {} 
\sphinxAtStartPar
Zmodyfikuj przykład z wykładu dla klasyfikatora punktów w okręgu dla:
\begin{itemize}
\item {} 
\sphinxAtStartPar
półkola;

\item {} 
\sphinxAtStartPar
dwóch rozłącznych okręgów;

\item {} 
\sphinxAtStartPar
pierścienia;

\item {} 
\sphinxAtStartPar
dowolnego z twoich ulubionych kształtów.

\end{itemize}

\item {} 
\sphinxAtStartPar
Powtórz 3, eksperymentując z liczbą warstw i neuronów, ale pamiętaj, że duża ich liczba wydłuża czas obliczeń i niekoniecznie poprawia wynik. Uszereguj każdy przypadek według liczby błędnie sklasyfikowanych punktów w próbce testowej. Znajdź optymalną/praktyczną architekturę dla każdego z rozważanych obszarów.

\item {} 
\sphinxAtStartPar
Jeśli sieć ma dużo neuronów i połączeń, przez każdą synapsę przepływa mało sygnału, stąd sieć jest odporna na niewielkie przypadkowe uszkodzenia. Tak dzieje się w naszym mózgu, który jest nieustannie „uszkadzany” (promienie kosmiczne, alkohol,…). Poza tym taką sieć po zniszczeniu można (już przy mniejszej liczbie połączeń) dodatkowo doszkolić. Weź wytrenowaną sieć z problemu 3. i usuń jedno z jej \sphinxstylestrong{słabych} połączeń (najpierw znajdź je, sprawdzając wagi), zmieniając odpowiednią wagę na 0. Przetestuj taką uszkodzoną sieć na próbce testowej i wyciągnij wnioski.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Skalowanie wag w propagacji wstecznej.}
Wadą zastosowania sigmoidu w algorytmie backprop jest bardzo powolna aktualizacja wag w warstwach odległych od warstwy wyjściowej (im bliżej początku sieci, tym wolniej). Remedium jest tutaj przeskalowanie wag, gdzie szybkość uczenia się warstw, licząc od tyłu, jest sukcesywnie zwiększana o pewien współczynnik. Pamiętamy, że kolejne pochodne wnoszą do szybkości aktualizacji współczynniki postaci \( \sigma '(s) = \sigma (s) [1- \sigma (s)] = y (1-y) \), gdzie \( y \) wynosi w zakresie \( (0, 1) \). Zatem wartość \( y (1-y \) nie może przekraczać 1/4, a w kolejnych warstwach (licząc od tyłu) czynnika \( [y (1-y] ^ n \le 1/4 ^ n\)).
Aby zapobiec temu „kurczeniu się”, wskaźnik uczenia się można przemnażać przez współczynniki kompensacyjne \(4 ^ n: 4, 16, 64, 256, ... \). Kolejny argument heurystyczny {[}\hyperlink{cite.docs/conclusion:id13}{RIV91}{]} sugeruje jeszcze szybciej rosnące czynniki  postaci \(6^n\):\(6,36,216,1296,...\)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Wprowadź powyższe dwie receptury do kodu backprop.

\item {} 
\sphinxAtStartPar
Sprawdź, czy rzeczywiście poprawiają wydajność algorytmu dla głębszych sieci, na przykład dla klasyfikatora punktów okręgu itp.

\item {} 
\sphinxAtStartPar
W celu oceny wydajności wykonaj pomiar czasu wykonania (np. za pomocą pakietu biblioteki Python \sphinxstylestrong{time}).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Najstromsze spadek.}
Zastosowana w wykładzie metoda najstromszego spadku do wyznaczania minimum funkcji wielu zmiennych zależy od gradientu lokalnego. Istnieją znacznie lepsze podejścia, które zapewniają szybszą zbieżność do (lokalnego) minimum. Jednym z nich jest przepis \sphinxhref{https://en.wikipedia.org/wiki/Gradient\_descent}{Barzilai\sphinxhyphen{}Borwein} wyjaśniony poniżej. Zaimplementuj tę metodę w algorytmie wstecznej propagacji. Wektory \(x\) w przestrzeni \(n\)\sphinxhyphen{}wymiarowej są aktualizowane w kolejnych iteracjach jako \( x^{(m + 1)} = x^{(m)} - \gamma_m \nabla F (x^{(m)} )\),
gdzie \(m\) numeruje iterację, a szybkość uczenia się zależy od zachowania w dwóch (bieżącym i poprzednim) punktach:

\end{enumerate}
\begin{equation*}
\begin{split} \gamma _ {m} = \frac {\left | \left (x^{(m)}-x^{(m-1)} \right) \cdot
\left [\nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right] \right |}
{\left \| \nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right \| ^ {2}}.
\end{split}
\end{equation*}\end{sphinxadmonition}


\chapter{Interpolacja}
\label{\detokenize{docs/interpol:interpolacja}}\label{\detokenize{docs/interpol::doc}}

\section{Symulowane dane}
\label{\detokenize{docs/interpol:symulowane-dane}}
\sphinxAtStartPar
Do tej pory zajmowaliśmy się \sphinxstylestrong{klasyfikacją}, czyli rozpoznawaniem przez sieci, czy dany obiekt (w naszym przykładzie punkt na płaszczyźnie) ma określone cechy. Teraz przechodzimy do innego praktycznego zastosowania, a mianowicie do \sphinxstylestrong{interpolacji} funkcji. To zastosowanie ANN stało się bardzo popularne w analizie danych naukowych. Zilustrujemy tę metodę na prostym przykładzie, który wyjaśni podstawową ideę i pokaże, jak ona działa.

\sphinxAtStartPar
Wyobraźmy sobie, że dysponujemy pewnymi danymi eksperymentalnymi. W tym przypadku symulujemy je w sztuczny sposób, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{fi}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mf}{0.2}\PYG{o}{+}\PYG{l+m+mf}{0.8}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3} \PYG{c+c1}{\PYGZsh{} a function}

\PYG{k}{def} \PYG{n+nf}{data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:} 
    \PYG{n}{x} \PYG{o}{=} \PYG{l+m+mf}{7.}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random x coordinate}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{fi}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} y coordinate = the function value + noise from [\PYGZhy{}0.2,0.2]}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powinniśmy teraz myśleć w kategoriach uczenia nadzorowanego: \(x\) to „cecha”, a \(y\) to „etykieta”.

\sphinxAtStartPar
Tablicujemy nasze zaszumione punkty danych i wykreślamy je wraz z funkcją \sphinxstylestrong{fi(x)}, wokół której się wahają. Jest to imitacja pomiaru eksperymentalnego, który zawsze obarczony jest pewnym błędem, tutaj naśladowanym przez losowy szum.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{data}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} data sample}
\PYG{n}{features}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}                   \PYG{c+c1}{\PYGZsh{} x coordinate}
\PYG{n}{labels}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}                     \PYG{c+c1}{\PYGZsh{} y coordinate}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_8_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
W języku ANN mamy zatem próbkę treningową składającą się z punktów o danych wejściowych (cechach) \(x\) i „prawdziwych” danych wyjściowych (etykietach) \(y\). Tak jak poprzednio, minimalizujemy funkcję błędu odpowiedniej sieci neuronowej,
\begin{equation*}
\begin{split}E(\{w \}) = \sum_p (y_o^{(p)} - y^{(p)})^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Ponieważ generowane \(y_o\) jest pewną (zależną od wag) funkcją \(x\), metoda ta jest odmianą \sphinxstylestrong{dopasowania najmniejszych kwadratów}, powszechnie stosowaną w analizie danych. Różnica polega na tym, że w standardowej metodzie najmniejszych kwadratów funkcja modelu, którą dopasowujemy do danych, ma pewną prostą postać analityczną (np. \( f(x) = A + B x\)), podczas gdy teraz jest to pewna „zakamuflowana” funkcja zależna od wag, dostarczona przez sieć neuronową.


\section{Interpolacja z pomocą ANN}
\label{\detokenize{docs/interpol:interpolacja-z-pomoca-ann}}
\sphinxAtStartPar
Aby zrozumieć podstawową ideę, rozważmy sieć z tylko dwoma neuronami w warstwie pośredniej, z sigmoidalną funkcją aktywacji:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_12_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Sygnały docierające do dwóch neuronów w warstwie środkowej to, w notacji z rozdz. {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Więcej warstw}}}},
\begin{equation*}
\begin{split} s_1^{1}=w_{01}^{1}+w_{11}^{1} x, \end{split}
\end{equation*}\begin{equation*}
\begin{split} s_2^{1}=w_{02}^{1}+w_{12}^{1} x, \end{split}
\end{equation*}
\sphinxAtStartPar
a sygnały wychodzące to odpowiednio,
\begin{equation*}
\begin{split} \sigma \left( w_{01}^{1}+w_{11}^{1} x \right), \end{split}
\end{equation*}\begin{equation*}
\begin{split} \sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \end{split}
\end{equation*}
\sphinxAtStartPar
Zatem połączony sygnał wchodzący do neuronu wyjściowego ma postać
\begin{equation*}
\begin{split} s_1^{2}=w_{01}^{2}+ w_{11}^{2}\sigma \left( w_{01}^{1}+w_{11}^{1} x \right)
+  w_{21}^{2}\sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \end{split}
\end{equation*}
\sphinxAtStartPar
Przyjmując, dla ilustracji, przykładowe wartości wag
\begin{equation*}
\begin{split} w_{01}^{2}=0, w_{11}^{2}=1, w_{21}^{2}=-1, w_{21}^{2},
w_{11}^{1}=w_{12}^{1}=1, \, w_{01}^{1}=-x_1, \, w_{02}^{1}=-x_2, \end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(x_1\) i \(x_2\) to notacja skrótowa, otrzymujemy
\begin{equation*}
\begin{split} s_1^{2}=\sigma(x-x_1)-\sigma(x-x_2). \end{split}
\end{equation*}
\sphinxAtStartPar
Funkcja ta jest przedstawiona na poniższym wykresie, gdzie \(x_1=-1\) i \(x_2=4\).
Dąży ona do 0 w \(-\infty\), potem rośnie wraz z \(x\), osiągając maksimum w punkcie
\((x_1+x_2)/2\), a następnie maleje, dążąc do 0 przy \(+\infty\). W punktach \(x=x_1\) i \(x=x_2\) jej wartości wynoszą około 0.5, można więc powiedzieć, że przedział znaczących wartości funkcji zawiera się między \(x_1\) i \(x_2\).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Jest to prosty, ale ważny wniosek:
Jesteśmy w stanie utworzyć, za pomocą pary neuronów z sigmoidami, sygnał „garbowy”, zlokalizowany wokół danej wartości, tutaj \( (x_1 + x_2) / 2 = 2\), i o danym rozrzucie rzędu \(|x_2-x_1|\). Zmieniając wagi, możemy modyfikować jej kształt, szerokość i wysokość.

\sphinxAtStartPar
Można teraz pomyśleć w następujący sposób: Wyobraźmy sobie, że mamy do dyspozycji wiele neuronów w warstwie pośredniej. Możemy je łączyć w pary, tworząc garby „specjalizujące się” w określonych regionach współrzędnych. Następnie, dostosowując wysokości garbów, możemy łatwo aproksymować daną funkcję.

\sphinxAtStartPar
W rzeczywistej procedurze dopasowania nie musimy „łączyć neuronów w pary”, lecz dokonać łącznego dopasowania wszystkich parametrów jednocześnie, tak jak to miało miejsce w przypadku klasyfikatorów. Poniższy przykład przedstawia kompozycję 8 sigmoidów,
\begin{equation*}
\begin{split}
f = \sigma(z+3)-\sigma(z+1)+2 \sigma(z)-2\sigma(z-4)+
      \sigma(z-2)-\sigma(z-8)-1.3 \sigma(z-8)-1.3\sigma(z-10). 
\end{split}
\end{equation*}
\sphinxAtStartPar
Na rysunku funkcje składowe (cienkie linie oznaczające pojedyncze garby) sumują się do funkcji o dość skomplikowanym kształcie, oznaczonej grubą linią.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_16_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Jeśli dopasowana funkcja jest regularna, można ją aproksymować za pomocą kombinacji liniowej sigmoidów. W przypadku większej liczby sigmoidów można uzyskać lepszą dokładność.
\end{sphinxadmonition}

\sphinxAtStartPar
Istnieje istotna różnica między ANN używanymi do aproksymacji funkcji w porównaniu z omawianymi wcześniej klasyfikatorami binarnymi. Tam odpowiedzi były równe 0 lub 1, więc w warstwie wyjściowej stosowaliśmy skokową funkcję aktywacji, a raczej jej gładką odmianę sigmoidalną. W przypadku aproksymacji funkcji odpowiedzi stanowią zazwyczaj kontinuum w zakresie wartości funkcji. Z tego powodu w warstwie wyjściowej używamy po prostu funkcji \sphinxstylestrong{identycznościowej}, czyli przepuszczamy przez nią bez zmian przychodzący sygnał. Oczywiście sigmoidy pozostają w warstwach pośrednich. Wówczas wzory używane do algorytmu \sphinxstylestrong{backprop} z sekcji {\hyperref[\detokenize{docs/backprop:bpa-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Algorytm propagacji wstecznej (backprop)}}}} mają w warstwie wyjściowej \(f_l(s)=s\).

\begin{sphinxadmonition}{note}{Warstwa wyjściowa dla aproksymacji funkcji}

\sphinxAtStartPar
W sieciach ANN używanych do aproksymacji funkcji, funkcja aktywacji w warstwie wyjściowej jest \sphinxstylestrong{identycznościowa}.
\end{sphinxadmonition}


\subsection{Algorytm backprop dla funkcji jednowymiarowych}
\label{\detokenize{docs/interpol:algorytm-backprop-dla-funkcji-jednowymiarowych}}
\sphinxAtStartPar
Weźmy architekturę

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
i losowe wagi

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jak właśnie wspomniano, wartość wyjściowa nie zawiera się teraz w przedziale od 0 do 1, co widać poniżej.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{n}{weights}\PYG{p}{,}\PYG{n}{features}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{ffo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{n}{weights}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_26_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
W module biblioteki \sphinxstylestrong{func} mamy funkcję dla algorytmu backprop, która pozwala na zastosowanie jednej funkcji aktywacji w warstwach pośrednich (przyjmujemy sigmoidę) i innej w warstwie wyjściowej (przyjmujemy funkcję identycznościową). Trening jest przeprowadzany w dwóch etapach: w pierwszych 30 rundach pobieramy punkty z próbki treningowej w losowej kolejności, a następnie w kolejnych 1500 rundach przecodzimy kolejno przez wszystkie punkty, zmniejszając również szybkość uczenia \sphinxstylestrong{eps}. Strategia ta jest jedną z wielu możliwych, ale w tym przypadku dobrze spełnia swoje zadanie.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.02}                           \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over the data sample points}
        \PYG{n}{pp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{pp}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.999}\PYG{o}{*}\PYG{n}{eps}                  \PYG{c+c1}{\PYGZsh{} dicrease of the learning speed}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over points taken in sequence}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_31_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Zauważmy, że otrzymana czerwona krzywa jest bardzo bliska funkcji użytej do wygenerowania próbki danych (czarna linia). Świadczy to o tym, że aproksymacja działa poprawnie. Konstrukcja miary ilościowej (sumy najmniejszych kwadratów) jest tematem ćwiczenia.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Funkcja aktywacji w warstwie wyjściowej może być dowolną gładką funkcją o wartościach zawierających wartości interpolowanej funkcji, niekoniecznie liniową.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Więcej wymiarów}

\sphinxAtStartPar
Aby interpolować funkcje dwóch lub więcej argumentów, należy użyć sieci ANN z co najmniej trzema warstwami neuronów.
\end{sphinxadmonition}

\sphinxAtStartPar
Możemy to rozumieć następująco {[}\hyperlink{cite.docs/conclusion:id7}{MullerRS12}{]}: dwa neurony w pierwszej warstwie neuronowej mogą tworzyć garb we współrzędnej \(x_1\), dwa inne \sphinxhyphen{} garb we współrzędnej \(x_2\), i tak dalej dla wszystkich pozostałych wymiarów. Tworząc koniunkcję tych \(n\) garbów w drugiej warstwie neuronów, otrzymujemy funkcję bazową specjalizującą się w obszarze wokół pewnego punktu w wielowymiarowej przestrzeni wejściowej. A zatem odpowiednio duża liczba takich funkcji bazowych może być użyta do aproksymacji w \(n\) wymiarach, w pełnej analogii do przypadku jednowymiarowego.

\begin{sphinxadmonition}{tip}{Wskazówka:}
\sphinxAtStartPar
Liczba neuronów potrzebnych w procedurze aproksymacji odzwierciedla zachowanie interpolowanej funkcji. Jeśli funkcja ulega licznym znacznym wahaniom, potrzeba więcej neuronów. W jednym wymiarze jest ich zwykle co najmniej dwa razy więcej niż liczba ekstremów funkcji.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Nadmierne dopasowanie (overfitting)}

\sphinxAtStartPar
Aby uniknąć tak zwanego \sphinxstylestrong{problemu nadmiernego dopasowania}, danych użytych do aproksymacji musi być znacznie więcej niż parametrów sieci. W przeciwnym razie moglibyśmy dopasować bardzo dokładnie dane treningowe za pomocą funkcji „wahającej się od punktu do punktu”. Jednocześnie, działanie takiej sieci na danych testowych byłoby bardzo kiepskie.
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/interpol:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Dopasuj punkty danych wygenerowane przez Twoją ulubioną funkcję (jednej zmiennej) z szumem. Pobaw się architekturą sieci i wyciągnij wnioski.

\item {} 
\sphinxAtStartPar
Oblicz sumę kwadratów odległości między wartościami punktów danych treningowych a odpowiadającą im funkcją aproksymującą i wykorzystaj ją jako miarę jakości dopasowania. Sprawdź, jak liczba neuronów w sieci wpływa na wynik.

\item {} 
\sphinxAtStartPar
Użyj sieci o większej liczbie warstw (co najmniej 3 warstwy neuronów) do dopasowania punktów danych wygenerowanych za pomocą ulubionej funkcji dwóch zmiennych. Wykonaj dwuwymiarowe wykresy konturowe dla tej funkcji oraz dla funkcji uzyskanej z sieci neuronowej i porównaj wyniki (oczywiście powinny być podobne, jeśli wszystko działa dobrze).

\end{enumerate}
\end{sphinxadmonition}


\chapter{Rektyfikacja}
\label{\detokenize{docs/rectification:rektyfikacja}}\label{\detokenize{docs/rectification::doc}}
\sphinxAtStartPar
W poprzednim rozdziale utworzyliśmy z dwóch sigmoidów funkcję o kształcie garbu, która, jak pokazaliśmy, mogła stanowić funkcję bazową dla aproksymacji. Możemy teraz zadać nastepujące pytanie: czy możemy skonstruować sam sigmoid jako kombinację liniową (różnicę) pewych innych funkcji? Wtedy moglibyśmy użyć tychże funkcji do aktywacji neuronów zamiast sigmoidu. Odpowiedź brzmi \sphinxstylestrong{tak}. Na przykład funkcja \sphinxstylestrong{Rectified Linear Unit (ReLU)}
\begin{equation*}
\begin{split}
{\rm ReLU}(x) = \left \{ \begin{array}{l} x {\rm ~~~ for~} x \ge 0 \\
                                          0 {\rm ~~~ for~} x < 0 \end{array}    \right . = {\rm max}(x,0)
\end{split}
\end{equation*}
\sphinxAtStartPar
wykonuje (w przybliżeniu) to zadanie. Ta nieco niezręczna nazwa pochodzi z elektrotechniki (rektyfikacja oznacza prostowanie), w której prostownik służy do odcinania ujemnych wartości sygnału elektrycznego. Wykres funkcji ReLU wygląda następująco:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_6_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Różnica dwóch funkcji ReLU o przesuniętych argumentach daje przykładowy wynik

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_8_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
która jakościowo wygląda jak sigmoid, z wyjątkiem ostrych rogów. Aby uzyskać gładkość, można skorzystać z innej funkcji \sphinxhyphen{} \sphinxstylestrong{softplus},
\begin{equation*}
\begin{split}
{\rm softplus}(x)=\log \left( 1+e^x \right ),
\end{split}
\end{equation*}
\sphinxAtStartPar
która ma nastepujący wykres:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_10_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Różnica dwóch funkcji \sphinxstylestrong{softplus} o przesuniętym argumencie daje wynik bardzo podobny do sigmoidu:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_12_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Do aktywacji można użyć ReLU, softplus lub wielu innych podobnych funkcji.
\end{sphinxadmonition}

\sphinxAtStartPar
Dlaczego właściwie należy to robić, zostanie omówione później.


\section{Interpolacja z ReLU}
\label{\detokenize{docs/rectification:interpolacja-z-relu}}
\sphinxAtStartPar
Nasze symulowane dane możemy aproksymować za pomocą sieci ANN z aktywacją ReLU w warstwach pośrednich (w warstwie wyjściowej funcja aktywacji jest identycznościowa, tak jak w poprzednim rozdziale). W poniższych kodach funkcje zostały zaczerpnięte z modułu \sphinxstylestrong{func}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}fff=func.softplus    \PYGZsh{} short\PYGZhy{}hand notation}
\PYG{c+c1}{\PYGZsh{}dfff=func.dsoftplus}

\PYG{n}{fff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{relu}    \PYG{c+c1}{\PYGZsh{} short\PYGZhy{}hand notation}
\PYG{n}{dfff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{drelu}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Sieć musi mieć teraz więcej neuronów, ponieważ sigmoid „rozpada się” na dwie funkcje ReLU:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                    \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} initialize weights randomly in [\PYGZhy{}2.5,2.5]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Symulacje przeprowadzamy dokładnie tak samo jak w poprzednim przypadku. Doświadczenie mówi, że należy startować z małymi szybkościami uczenia się. Dwa zestawy rund (podobnie jak w poprzednim rozdziale)

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.0003}         \PYG{c+c1}{\PYGZsh{} small learning speed}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} loop over the data sample points}
        \PYG{n}{pp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{pp}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{fff}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{dfff}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} teaching}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
\PYG{c+c1}{\PYGZsh{}    eps=eps*.995}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} points in sequence}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{fff}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{dfff}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} teaching}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  400  500  600  700  800  900  1000  1100  1200  1300  1400  1500  1600  1700  1800  1900  2000  2100  2200  2300  2400  2500  2600  2700  2800  2900  3000  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.995}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} points in sequence}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{fff}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{dfff}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} teaching}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  400  500  600  700  800  900  1000  1100  1200  1300  1400  1500  1600  1700  1800  1900  2000  2100  2200  2300  2400  2500  2600  2700  2800  2900  3000  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
dają wynik

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_24_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Ponownie uzyskujemy całkiem zadowalający wynik (linia czerwona), zauważając, że wykres funkcji dopasowania jest ciągiem linii prostych, co odzwierciedla właściwości użytej funkcji aktywacji ReLU. Gładki wynik można uzyskać z pomocą funkcji softplus.


\section{Klasyfikatory z rektyfikacją}
\label{\detokenize{docs/rectification:klasyfikatory-z-rektyfikacja}}
\sphinxAtStartPar
Istnieją techniczne powody przemawiające za stosowaniem w algorytmie backprop \sphinxhref{https://en.wikipedia.org/wiki/Rectifier\_(neural\_networks)}{funkcji rektyfikowanych} zamiast sigmoidalnych. Pochodne sigmoidu są bowiem bardzo bliskie zera, z wyjątkiem wąskiego obszaru w pobliżu progu. Sprawia to, że aktualizacja wag jest mało prawdopodobna, zwłaszcza gdy cofamy się o wiele warstw wstecz, ponieważ wtedy bardzo małe liczby (określone przez pochodne funkcji) są przemnażane i w zasadzie nie prowadzą do żadnej aktualizacji (zjawsko to jest znane jako \sphinxstylestrong{problem zanikającego gradientu}). W przypadku funkcji rektyfikowanych zakres, w którym pochodna jest istotnie różna od zera, jest duży (w przypadku ReLU dotyczy to wszystkich współrzędnych dodatnich), dlatego problem zanikającego gradientu się nie pojawia. Właśnie z tego powodu funkcje rektyfikowane są stosowane w głębokich sieciach ANN, w których jest wiele warstw, niemożliwych do wytrenowania przy funkcjach aktywacji jest typu sigmoidalnego.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Zastosowanie rektyfikowanych funkcji aktywacji było jednym z kluczowych trików, które umożliwiły przełom w rozwoju głębokich ANN około 2011 roku.
\end{sphinxadmonition}

\sphinxAtStartPar
Z drugiej strony, w przypadku ReLU może się zdarzyć, że niektóre wagi przyjmą takie wartości, że wiele neuronów stanie się nieaktywnych, tzn. nigdy, dla żadnego inputu, nie zadziałają – zostaną de facto wyeliminowane. Nazywa się to problemem „martwego neuronu” lub „trupa”, który pojawia się zwłaszcza wtedy, gdy parametr szybkości uczenia jest zbyt duży. Sposobem na ograniczenie tego problemu jest zastosowanie funkcji aktywacji, która w ogóle nie ma przedziału o zerowej pochodnej, np. \sphinxhref{https://en.wikipedia.org/wiki/Activation\_function}{Leaky ReLU}. Tutaj przyjmiemy jej nasepującą postać
\begin{equation*}
\begin{split}
{\rm Leaky~ReLU}(x) = \left \{ \begin{array}{ll} x &{\rm ~~~ for~} x \ge 0 \\
                                          0.1 \, x &{\rm ~~~ for~} x < 0 \end{array}    \right . .
\end{split}
\end{equation*}
\sphinxAtStartPar
Dla ilustracji powtórzymy nasz przykład z rozdz. {\hyperref[\detokenize{docs/backprop:circ-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Przykład z kołem}}}} z klasyfikacją punktów w kole z wykorzystaniem funkcji Leaky ReLU.

\sphinxAtStartPar
Przyjmujemy następującą architekturę i parametry początkowe:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch\PYGZus{}c}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                   \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} scaled random initial weights in [\PYGZhy{}1.5,1.5]}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.01}                           \PYG{c+c1}{\PYGZsh{} initial learning speed }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
i uruchamiamy algorytm w dwóch etapach: z Leaky ReLU, a następnie z ReLU:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.999}\PYG{o}{*}\PYG{n}{eps}       \PYG{c+c1}{\PYGZsh{} decrease the learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
            \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lrelu}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlrelu}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} 
                    \PYG{c+c1}{\PYGZsh{} backprop with leaky ReLU}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{700}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.995}\PYG{o}{*}\PYG{n}{eps}       \PYG{c+c1}{\PYGZsh{} decrease the learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
            \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{relu}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{drelu}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} 
                    \PYG{c+c1}{\PYGZsh{} backprop with ReLU}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  400  500  600  700  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Wynik jest całkiem zadowalający, co pokazuje, że metoda działa. Przy obecnej architekturze i funkcjach aktywacji, co nie jest zaskakujące, na poniższym wykresie można zauważyć ślady wielokąta przybliżającego koło.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_38_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\section{Ćwiczenia}
\label{\detokenize{docs/rectification:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Zastosuj różne rektywikowane funkcje aktywacji dla klasyfikatorów binarnych i przetestuj je na różnych kształtach (analogicznie do przykładu z kołem powyżej).

\item {} 
\sphinxAtStartPar
Przekonaj się, że uruchomienie algorytmu backprop (z funkcją ReLU) ze zbyt dużą początkową szybkością uczenia prowadzi do problemu „martwego neuronu” i niepowodzenia algorytmu.

\end{enumerate}
\end{sphinxadmonition}


\chapter{Uczenie nienadzorowane}
\label{\detokenize{docs/unsupervised:uczenie-nienadzorowane}}\label{\detokenize{docs/unsupervised:un-lab}}\label{\detokenize{docs/unsupervised::doc}}
\begin{sphinxadmonition}{note}{Motto}

\sphinxAtStartPar
\sphinxstyleemphasis{teachers! leave those kids alone!}

\sphinxAtStartPar
          (Pink Floyd, Another Brick In The Wall)
\end{sphinxadmonition}

\sphinxAtStartPar
Uczenie nadzorowane, omawiane w poprzednich wykładach, wymaga nauczyciela lub próbki treningowej z etykietami, gdzie znamy \sphinxstylestrong{a priori} cechy danych (np. jak w jednym z naszych przykładów, czy dany punkt jest wewnątrz czy na zewnątrz okręgu).
Jest to jednak dość szczególna sytuacja, ponieważ najczęściej dane, z którymi się stykamy, nie mają przypisanych etykiet i ,,są, jakie są”. Ponadto, z neurobiologicznego czy metodologicznego punktu widzenia, wielu faktów i czynności uczymy się ,,na bieżąco”, klasyfikując je, a następnie rozpoznając, przy czym proces ten przebiega bez żadnego zewnętrznego nadzoru czy etykietek ,,unoszących się” w powietrzu nad obiektami.

\sphinxAtStartPar
Wyobraźmy sobie botanika\sphinxhyphen{}kosmitę, który wchodzi na łąkę i napotyka różne gatunki kwiatów. Nie ma zielonego pojęcia, czym one są i czego się spodziewać, ponieważ nie ma żadnej wiedzy o sprawach ziemskich. Po znalezieniu pierwszego kwiatu zapisuje jego cechy: kolor, wielkość, liczbę płatków, zapach itd. Idzie dalej, znajduje inny kwiat, zapisuje jego cechy, i tak dalej i dalej dla kolejnych kwiatami. W pewnym momencie trafia jednak na kwiat, który już wcześniej poznał. Dokładniej mówiąc, jego cechy są bardzo zbliżone, choć nie identyczne (wielkość może się nieco różnić, kolor itd.), do poprzedniego przypadku. Stąd wniosek, że należy on do jednej kategorii. Poszukiwania trwają dalej, a nowe kwiaty albo tworzą nową kategorię, albo dołączają do już istniejącej. Na koniec poszukiwań kosmina ma utworzony katalog kwiatów i może przypisać nazwy (etykiety) poszczególnym gatunkom: mak kukurydziany, krwawnik pospolity, dziewanna…  Etykiety te, czyli nazwy, są przydatne w dzieleniu się swoją wiedzą z innymi, ponieważ podsumowują cechy kwiatu. Należy jednak pamiętać, że etykiety te nigdy nie były używane w procesie eksploracji (uczenia się) łąki.

\sphinxAtStartPar
Formalnie rzecz biorąc, opisywany problem \sphinxstylestrong{uczenia nienadzorowanego} dotyczy klasyfikacji danych (podziału na kategorie, lub \sphinxstylestrong{klastry}, czyli podzbiory próbki danych, w których odpowiednio zdefiniowane odległości między poszczególnymi danymi są małe, mniejsze od przyjętych odległości między klastrami). Mówiąc kolokwialnie, szukamy podobieństw między poszczególnymi punktami danych i staramy się podzielić próbkę na grupy podobnych obiektów.


\section{Klastry}
\label{\detokenize{docs/unsupervised:klastry}}
\sphinxAtStartPar
Oto nasza uproszczona wersja eksploracji botanika\sphinxhyphen{}kosmity:
Rozważmy punkty na płaszczyźnie, które są generowane losowo. Ich rozkład nie jest jednorodny, lecz rozłożony w czterech skupiskach: A, B, C i D. Możemy na przykład zadać odpowiednie granice dla współrzędnych \(x_1\) i \(x_2\) przy losowym generowaniu punktów danej kategorii. Używamy do tego funkcji numpy \sphinxstylestrong{random.uniform(a,b)}, dającej równomiernie rozłożoną liczbę zmiennoprzecinkową pomiędzy a i b:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{pA}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.75}\PYG{p}{,} \PYG{l+m+mf}{.95}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.7}\PYG{p}{,} \PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pB}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{l+m+mf}{.6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.6}\PYG{p}{,} \PYG{l+m+mf}{.75}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pC}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.1}\PYG{p}{,} \PYG{l+m+mf}{.3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{l+m+mf}{.5}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pD}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.7}\PYG{p}{,} \PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{.2}\PYG{p}{)}\PYG{p}{]} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Utwórzmy próbkę danych zawierającą po kilka punktów z każdej kategorii:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samA}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pA}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samB}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pB}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samC}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pC}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samD}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pD}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dane te wyglądają następująco:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_11_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Jeśli pokażemy komuś powyższy rysunek, to z pewnością stwierdzi, że są na nim cztery klastry. Ale jaka metoda jest używany, aby to stwierdzić? Wkrótce skonstruujemy odpowiedni algorytm i będziemy mogli przeprowadzić klasteryzację. Na razie jednak skoczmy w przód i załóżmy, że \sphinxstylestrong{wiemy}, czym są klastry. W naszym przykładzie klastry są dobrze zdefiniowane, tzn. widocznie oddzielone od siebie.

\sphinxAtStartPar
Można reprezentować klastry za pomocą \sphinxstylestrong{punktów reprezentatywnych}, które leżą gdzieś w obrębie klastra. Można na przykład wziąć element należący do danego klastra jako jego reprezentanta, lub też dla każdego klastra oszacować średnie położenie jego punktów i użyć je jako punkt reprezentatywny:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rA}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rB}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rC}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rD}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
(do obliczenia średniej użyliśmy modułu \sphinxstylestrong{statistics}). Tak zdefiniowane punkty reprezentatywne dołączamy do powyższej grafiki. Dla wygody wizualnej, każdej kategorii przypisujemy kolor (po ustaleniu klastów możemy bowiem nadać im etykiety, a kolor w tym przypadku służy właśnie temu celowi).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{col}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magenta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Clusters with representative points}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_16_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Komórki Woronoja}
\label{\detokenize{docs/unsupervised:komorki-woronoja}}\label{\detokenize{docs/unsupervised:vor-lab}}
\sphinxAtStartPar
W sytuacji jak na rysunku powyżej, tzn. mając wyznaczone punkty reprezentatywne, możemy podzielić całą płaszczyznę na komórki (obszary) według następującego kryterium Woronoja, które jest prostym pojęciem geometrycznym:

\begin{sphinxadmonition}{note}{Komórki Woronoja}

\sphinxAtStartPar
Rozważmy przestrzeń metryczną, w której istnieje pewna liczba punktów reprezentatywnych (punktów Woronoja) \(R\). Dla danego punktu \(P\) wyznaczamy odległości do wszystkich punktów \(R\). Jeśli wśród tych odległości istnieje ścisłe minimum (najbliższy punkt \(R_m\)), to z definicji punkt \(P\) należy do komórki Woronoja \(R_m\). Jeśli nie ma ścisłego minimum, to \(P\) należy do granicy między pewnymi komórkami. Konstrukcja ta dzieli całą przestrzeń na kopmórki Woronoja i granice pomiędzy nimi.
\end{sphinxadmonition}

\sphinxAtStartPar
Wracając do naszego przykładu, zdefiniujmy kolor punktu P jako kolor najbliższego punktu reprezentatywnego. W tym celu potrzebujemy (kwadratu) odległości (tutaj euklidesowej) między dwoma punktami w przestrzeni dwuwymiarowej:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{eucl}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} square of the Euclidean distance}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie z pomocą \sphinxstylestrong{np.argmin} znajdujemy najbliższy reprezentatywny punkt i określamy jego kolor:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{col\PYGZus{}char}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rA}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rB}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rC}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rD}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} array of distances}
    \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}                          \PYG{c+c1}{\PYGZsh{} index of the nearest point}
    \PYG{k}{return} \PYG{n}{col}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}                                \PYG{c+c1}{\PYGZsh{} color of the nearest point}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{col\PYGZus{}char}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{.5}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}blue\PYGZsq{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Wynikiem przeprowadzenia tego kolorowania dla punktów z naszej przestrzeni (bierzemy tu wystarczająco gęstą próbkę \(70 \times 70\) punktów) jest jej następujący podział na komórki Woronoja:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_26_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Łatwo jest udowodnić, że granice między sąsiednimi obszarami są liniami prostymi.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Praktyczne przesłanie jest takie, że po wyznaczeniu punktów reprezentatywnych możemy zastosować kryterium Woronoja do klasyfikacji danych.
\end{sphinxadmonition}


\section{Naiwna klasteryzacja}
\label{\detokenize{docs/unsupervised:naiwna-klasteryzacja}}
\sphinxAtStartPar
Wracamy teraz do problemu botanika\sphinxhyphen{}kosmity: wyobraźmy sobie, że mamy naszą próbkę, ale nie wiemy nic o tym, jak zostały wygenerowane jej punkty (nie mamy etykiet A, B, C, D ani kolorów punktów). Co więcej, dane są zmieszane, tzn. punkty danych występują w przypadkowej kolejności. Łączymy więc nasze punkty za pomocą \sphinxstylestrong{np.concatenate}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{alls}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{,} \PYG{n}{samB}\PYG{p}{,} \PYG{n}{samC}\PYG{p}{,} \PYG{n}{samD}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
i tasujemy je z pomocą \sphinxstylestrong{np.random.shuffle}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Wizualizacja danych wygląda tak, jak na pierwszym wykresie w tym rozdziale.

\sphinxAtStartPar
Chcemy teraz w jakiś sposób utworzyć punkty reprezentatywne, ale a priori nie wiemy, gdzie powinny się one znajdować, ani nawet ile ich powinno być. Do osiągnięcia celu możliwe są bardzo różne strategie. Ich wspólną cechą jest to, że położenie punktów reprezentatywnych jest aktualizowane w miarę przetwarzania (czytania) danych próbki.

\sphinxAtStartPar
Zacznijmy od przypadku tylko jednego punktu reprezentatywnego, \(\vec{R}\). Nie jest to zbyt ambitne, ale przynajmniej będziemy znali pewne uśrednione charakterystyki próbki. Początkowa pozycja to \( R=(R_1, R_2) \), dwuwymiarowy wektor w przestrzeni \([0,1]\times [0,1]\). Po odczytaniu punktu danych \(P\) o współrzędnych \( (x_1 ^ P, x_2 ^ P) \), wektor \(R\) zmienia się w następujący sposób:
\begin{equation*}
\begin{split} (R_1, R_2) \to (R_1, R_2) + \varepsilon (x_1 ^P-R_1, x_2 ^P-R_2), \end{split}
\end{equation*}
\sphinxAtStartPar
lub w notacji wektorowej
\begin{equation*}
\begin{split} \vec {R} \to \vec {R} + \varepsilon (\vec {x}^P - \vec {R}). \end{split}
\end{equation*}
\sphinxAtStartPar
Czynność tę powtarzamy dla wszystkich punktów próbki, a następnie można wykonać wiele rund. Podobnie jak w poprzednich rozdziałach, \( \varepsilon \) jest współczynnikiem uczenia, który maleje
wraz z postępem algorytmu. Powyższy wzór realizuje „przyciąganie” punktu \(\vec{R}\) przez punkt danych \(vec{P}\).

\sphinxAtStartPar
Poniższy kod implementuje przepis:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} initial location}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{initial location:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{round   location}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}                         \PYG{c+c1}{\PYGZsh{} initial learning speed}

\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{:}            \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}               \PYG{c+c1}{\PYGZsh{} decrease the learning speed }
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} reshuffle the sample}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over points of the whole sample}
        \PYG{n}{R}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} update/learning}
    \PYG{k}{if} \PYG{n}{j}\PYG{o}{\PYGZpc{}}\PYG{k}{5}==4: print(j+1, \PYGZdq{}    \PYGZdq{},np.round(R,3))  \PYGZsh{} print every 5th step
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
initial location:
[0.458 0.603]
round   location
5      [0.447 0.537]
10      [0.457 0.47 ]
15      [0.604 0.453]
20      [0.607 0.478]
25      [0.603 0.484]
30      [0.602 0.484]
35      [0.602 0.484]
40      [0.602 0.484]
45      [0.602 0.485]
50      [0.602 0.485]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Można zauważyć, że położenie punktu reprezentatywnego jest zbieżne do pewnej granicy. W rzeczywistości dąży ono do średniego położenia punktów próbki,

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R\PYGZus{}mean}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R\PYGZus{}mean}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[0.602 0.485]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zdecydowaliśmy a priori, że będziemy mieli tylko jedną kategorię. Oto więc wykres z wynikiem dla punktu reprezentatywnenego, oznaczonego szarą plamą:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_39_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Powyższe rozwiązanie oczywiście nas nie zadowala (na oko, obiekty nie należą do jednej kategorii), spróbujmy więc uogólnić algorytm na przypadek kilku (\(n_R> 1\)) punktów reprezentatywnych:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Inicjalizujemy losowo punkty reprezentatywne \( \vec{R}^i \), \(i = 1, \dots, n_R \).

\item {} 
\sphinxAtStartPar
Runda: Bierzemy po kolei przykładowe punkty P i aktualizujemy tylko \sphinxstylestrong{najbliższy} punkt reprezentatywny \(R^m\) do punktu P w danym kroku:

\end{itemize}
\begin{equation*}
\begin{split} \vec{R}^m \to \vec{R}^m + \varepsilon (\vec{x} - \vec{R}^m). \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Położenie pozostałych punktów reprezentatywnych pozostaje niezmienione. Strategia taka nazywana jest \sphinxstylestrong{zwycięzca bierze wszystko} (winner\sphinxhyphen{}take\sphinxhyphen{}all).

\item {} 
\sphinxAtStartPar
Powtarzamy rundy, za każdym razem zmniejszając \(\varepsilon\).

\end{itemize}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Strategia \sphinxstylestrong{zwycięzca bierze wszystko} jest ważnym pojęciem w trenowaniu ANN. Konkurujące neurony w warstwie walczą o ,,nagrodę”, a ten, który wygra, bierze ją w całości (jego wagi są aktualizowane), podczas gdy przegrani nie dostają nic.
\end{sphinxadmonition}

\sphinxAtStartPar
Rozważmy teraz dwa punkty reprezentatywne, które inicjalizujemy losowo:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{R2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie wykonujemy nasz algorytm. Dla każdego punktu danych znajdujemy najbliższy reprezentatywny punkt spośród dwóch i aktualizujemy tylko ten, który jest zwycięzcą:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{initial locations:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rounds  locations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}

\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{40}\PYG{p}{)}\PYG{p}{:}             
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)} 
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}   
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R2}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} squares of distances}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}               \PYG{c+c1}{\PYGZsh{} index of the minimum}
        \PYG{k}{if} \PYG{n}{ind\PYGZus{}min}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} if R1 closer to the new data point}
            \PYG{n}{R1}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R1}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} update R1                }
        \PYG{k}{else}\PYG{p}{:}                  \PYG{c+c1}{\PYGZsh{} if R2 closer ... }
            \PYG{n}{R2}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R2}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} update R2       }

    \PYG{k}{if} \PYG{n}{j}\PYG{o}{\PYGZpc{}}\PYG{k}{5}==4: print(j+1,\PYGZdq{}    \PYGZdq{}, np.round(R1,3), np.round(R2,3))  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
initial locations:
[0.887 0.077] [0.916 0.311]
rounds  locations
5      [0.793 0.139] [0.475 0.636]
10      [0.795 0.117] [0.468 0.614]
15      [0.8   0.112] [0.528 0.647]
20      [0.796 0.113] [0.525 0.643]
25      [0.796 0.114] [0.521 0.642]
30      [0.796 0.114] [0.52  0.642]
35      [0.796 0.114] [0.52  0.642]
40      [0.796 0.114] [0.52  0.642]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Wynik jest następujący:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_47_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Jeden z punktów charakterystycznych ,,specjalizuje się” w prawym dolnym klastrze, a drugi w pozostałych punktach próbki.

\sphinxAtStartPar
Następnie kontynuujemy, całkiem analogicznie, z czterema punktami reprezentatywnymi.

\sphinxAtStartPar
Wynik dla dwóch różnych warunków początkowych dla punktów reprezentatywnych (i nieco innej próbki) jest
pokazany na \hyperref[\detokenize{docs/unsupervised:p-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:p-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{cl4_2}.png}
\caption{Po lewej: właściwe punkty reprezentatywne. Po prawej: jeden ,,trup”.}\label{\detokenize{docs/unsupervised:p-fig}}\end{figure}

\sphinxAtStartPar
Zauważamy, że procedura nie zawsze daje poprawną/oczekiwaną odpowiedź. Dość często jeden z punktów reprezentatywnych nie jest w ogóle aktualizowany i staje się tak zwanym \sphinxstylestrong{trupem}. Dzieje się tak dlatego, że pozostałe punkty reprezentatywne zawsze wygrywają, tzn. jeden z nich jest zawsze bliżej każdego punktu danych w próbce niż „trup”. Oczywiście, jest to sytuacja niezadowalająca.

\sphinxAtStartPar
Gdy bierzemy pięć punktów reprezentatywnych, to w zależności od losowej inicjalizacji może wystąpić kilka sytuacji, jak pokazano na \hyperref[\detokenize{docs/unsupervised:id1}]{Rys.\@ \ref{\detokenize{docs/unsupervised:id1}}}. Czasami jakoś klaster rozpada się na dwa mniejsze, czasami pojawiają się trupy.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=870\sphinxpxdimen]{{cl5}.jpg}
\caption{Od lewej do prawej: 5 punktów charakterystycznych z jednym wcześniejszym klastrem podzielonym na dwa, z innym klasterm podzielonym na dwa, jednym trupem i dwoma trupami.}\label{\detokenize{docs/unsupervised:id1}}\end{figure}

\sphinxAtStartPar
Branie większej liczby punktów reprezentacyjnych prowadzi do jeszcze częstszego powstawania trupów. Oczywiście możemy je lekceważyć, ale przykład ten pokazuje, że obecna strategia klastrowanioa danych jest wysoce problematyczna i potrzebujemy czegoś lepszego.


\section{Skala klastrowania}
\label{\detokenize{docs/unsupervised:skala-klastrowania}}
\sphinxAtStartPar
W poprzednim rozdziale staraliśmy się od początku odgadnąć, ile klastrów znajduje się w danych. Prowadziło to do problemów, gdyż zazwyczaj nie wiemy nawet, ile jest klastrów. Właściwie do tej pory nie zdefiniowaliśmy, czym dokładnie jest klaster, i posługiwaliśmy się jedynie intuicją. Ta intuicja podpowiadała nam, że punkty w tym samym skupisku muszą być blisko siebie lub blisko punktu charakterystycznego, ale jak blisko? Tak naprawdę definicja musi zawierać \sphinxstylestrong{skalę} (charakterystyczną odległość), która mówi nam, „jak blisko jest blisko”. Na przykład w naszym przykładzie możemy przyjąć skalę około 0,2, gdzie są 4 klastry, ale możemy też przyjąć mniejszą skalę i rozdzielić większe klastry na mniejsze, jak w dwóch lewych panelach \hyperref[\detokenize{docs/unsupervised:id1}]{Rys.\@ \ref{\detokenize{docs/unsupervised:id1}}}.

\begin{sphinxadmonition}{note}{Definicja klastra}

\sphinxAtStartPar
Klaster o skali \(d\) związany z punktem charakterystycznym \(R\) to zbiór punktów danych \(P\), których odległość od \(R\) jest mniejsza niż \(d\), natomiast odległość od innych punktów reprezentatywnych jest  \(\ge d\). Punkty charakterystyczne muszą być wybrane w taki sposób, aby każdy punkt danych należał do klastra, a żaden punkt charakterystyczny nie był martwy (tzn. jego klaster musi zawierać co najmniej jeden punkt danych).
\end{sphinxadmonition}

\sphinxAtStartPar
Do realizacji tej recepty można wykorzystać różne strategie. Tutaj użyjemy \sphinxstylestrong{dynamicznej klasteryzacji}, w której nowy klaster/punkt reprezentatywny jest tworzony za każdym razem, gdy napotkany punkt danych znajduje się dalej niż \(d\) od dowolnego punktu reprezentatywnego zdefiniowanego do tej pory.

\begin{sphinxadmonition}{note}{Dynamiczna klasteryzacja}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Ustaw skalę klasteryzacji \(d\) i początkową szybkość uczenia \(varepsilon\). Potasuj próbkę.

\item {} 
\sphinxAtStartPar
Odczytaj pierwszy punkt danych \(P_1\) i wyznacz pierwszy punkt charakterystyczny jako \(R^1=P_1\). Dodaj go do tablicy \(R\) zawierającej wszystkie punkty charakterystyczne. Oznacz \(P_1\) jako należący do klastra \(1\).

\item {} 
\sphinxAtStartPar
Odczytaj kolejny punkt danych \(P\). Jeśli odległość \(P\) od \sphinxstylestrong{najbliższego} punktu charakterystycznego, \(R^m\), jest \(\le d\), to
\begin{itemize}
\item {} 
\sphinxAtStartPar
oznacz \(P\) jako należący do klastra \(m\).

\item {} 
\sphinxAtStartPar
przesuń \(R^m\) w kierunku \(P\) z prędkością uczenia \(varepsilon\).W przeciwnym razie dodaj do \(R\) nowy punkt charakterystyczny w położeniu punktu \(P\).

\end{itemize}

\item {} 
\sphinxAtStartPar
Powtórz od \(2\), aż wszystkie punkty danych zostaną przetworzone.

\item {} 
\sphinxAtStartPar
Powtórz od \(2\) w pewnej liczbie rund, zmniejszając za każdym razem \(\varepsilon\). Wynikiem jest podział próbki na pewną liczbę klastrów oraz położenia odpowiadających im punktów reprezentatywnych. Wynik może zależeć od losowego tasowania, a więc nie musi być taki sam przy powtarzaniu procedury.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
Poniżej przedstawiono implementację w języku Python, dynamiczne znajdującą punkty reprezentatywne:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{d}\PYG{o}{=}\PYG{l+m+mf}{0.2}   \PYG{c+c1}{\PYGZsh{} clustering scale}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} initial learning speed}

\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}                  \PYG{c+c1}{\PYGZsh{} decrease the learning speed }
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} shuffle the sample}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}                      \PYG{c+c1}{\PYGZsh{} in the first round}
        \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{alls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} R \PYGZhy{} array of representative points}
                                  \PYG{c+c1}{\PYGZsh{} initialized to the first data point}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} loop over the sample points}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}                 \PYG{c+c1}{\PYGZsh{} new data point}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} 
         \PYG{c+c1}{\PYGZsh{} array of squares of distances of p from the current repr. points in R}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} index of the closest repr. point}
        \PYG{k}{if} \PYG{n}{dist}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{d}\PYG{o}{*}\PYG{n}{d}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} if its distance square \PYGZgt{} d*d}
                                  \PYG{c+c1}{\PYGZsh{} dynamical creation of a new category}
            \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,} \PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} add new repr. point to R}
        \PYG{k}{else}\PYG{p}{:}   
            \PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} otherwise, apdate the \PYGZdq{}old\PYGZdq{} repr. point}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of representative points: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of representative points:  4
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Wynik działania algorytmu dla różnych wartości skali klasteryzacji \(d\) pokazano na \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}. Przy bardzo małych wartościach \(d\), mniejszych od minimalnej separacji między punktami, klastrów jest tyle, ile punktów danych. Następnie, wraz ze wzrostem wartości \(d\), liczba klasrów maleje. Przy bardzo dużych wartościach \(d\), rzędu rozpiętości całej próbki, występuje tylko jeden klaster.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=770\sphinxpxdimen]{{cd}.jpg}
\caption{Dynamiczne klastrowanie dla różnych wartości skali \(d\).}\label{\detokenize{docs/unsupervised:dyn-fig}}\end{figure}

\sphinxAtStartPar
Oczywiście sam algorytm nie powie nam, jaką skalę klasteryzacji należy zastosować. Właściwa wartość zależy od natury problemu. Przypomnijmy sobie naszego botanika\sphinxhyphen{}kosmitę. Gdyby użył bardzo małej wartości \(d\), otrzymałby tyle kategorii, ile jest kwiatów na łące, ponieważ wszystkie kwiaty, nawet tego samego gatunku, różnią się od siebie nieznacznie. Byłoby to bezużyteczne. Z drugiej strony, jeśli \(d\) jest zbyt duże, to klasyfikacja jest zbyt zgrubna. Coś pomiędzy jest w sam raz!

\begin{sphinxadmonition}{note}{Labels}

\sphinxAtStartPar
Po utworzeniu klastrów możemy dla wygody nadać im \sphinxstylestrong{etykiety}. Nie są one wykorzystywane w procesie uczenia (tworzenia klastrów).
\end{sphinxadmonition}

\sphinxAtStartPar
Po określeniu klastrów mamy \sphinxstylestrong{klasyfikator}. Możemy go używać w dwojaki sposób:
\begin{itemize}
\item {} 
\sphinxAtStartPar
kontynuować dynamiczną aktualizację w miarę napływu nowych danych lub

\item {} 
\sphinxAtStartPar
,,zamknąć” klasyfikator i sprawdzić, gdzie wpadają nowe dane.

\end{itemize}

\sphinxAtStartPar
W pierwszym przypadku przyporządkowujemy nowemu punktowi danych odpowiednią etykietę klastra (nasz botanik wie, jaki nowy kwiat znalazł) lub tworzymy nową kategorię, jeśli punkt nie należy do żadnego z istniejących klastrów. Jest to po prostu kontynuacja opisanego powyżej algorytmu dla nowych przychodzących danych.

\sphinxAtStartPar
W drugim przypadku (kupiliśmy gotowy i zamknięty katalog botanika\sphinxhyphen{}kosmity) punkt danych może
\begin{itemize}
\item {} 
\sphinxAtStartPar
należeć do klastra (znamy jego etykietę),

\item {} 
\sphinxAtStartPar
nie należeć do żadnego klastra, wtedy nie wiemy, co to jest, lub

\item {} 
\sphinxAtStartPar
znaleźć się w obszarze nakładania się dwóch lub więcej klastrów (por. \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}, kiedy otrzymujemy tylko „częściową” lub niejednoznaczną klasyfikację.

\end{itemize}

\sphinxAtStartPar
Alternatywnie, możemy zastosować klasyfikację z pomocą komórek Woronoja, aby pozbyć się niejednoznaczności.


\subsection{Interpretacja poprzez najstromszy spadek}
\label{\detokenize{docs/unsupervised:interpretacja-poprzez-najstromszy-spadek}}
\sphinxAtStartPar
Oznaczmy dany klaster symbolem \(C_i\), \(i = 1, ..., n\), gdzie \( n\) jest całkowitą liczbą klastrów. Suma kwadratów odległości punktów danych w klastrze \( C_i \) od jego punktu reprezentatywnego \( R ^ i \) wynosi
\begin{equation*}
\begin{split}
\sum_{P \in C_i} | \vec{R}^i- \vec{x}^P|^2.
\end{split}
\end{equation*}
\sphinxAtStartPar
Sumując po wszystkich klastrach, otrzymujemy funkcję analogiczną do omówionej wcześniej funkcji błędu:
\begin{equation*}
\begin{split}E (\{R \}) = \sum_{i = 1}^ n \sum_ {P \in C_i} |\vec{R}^i- \vec{x}^P |^2 .\end{split}
\end{equation*}
\sphinxAtStartPar
Jej pochodna po \( \vec{R}_i \) wynosi
\begin{equation*}
\begin{split} \frac{\partial E (\{R \})}{\partial \vec{R}^i}
= 2 \sum_{P \in C_i} (\vec{R}^i- \vec{x}^P). \end{split}
\end{equation*}
\sphinxAtStartPar
Metoda najstromszego spadku daje w rezultacie \sphinxstylestrong{dokładnie} taką samą receptę, jaką zastosowano w przedstawionym powyżej algorytmie dynamicznej klasteryzacji, tj.
\begin{equation*}
\begin{split} \vec{R} \to \vec{R} - \varepsilon (\vec{R} - \vec {x}^P). \end{split}
\end{equation*}
\sphinxAtStartPar
Podsumowując, zastosowany algorytm polega w istocie na zastosowaniu metody najstromszego zejścia dla funkcji \( E (R) \), co zostało dokładniej omówione w poprzednich rozdziałach.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Należy jednak zauważyć, że minimalizacja stosowana w obecnym algorytmie uwzględnia również różne kombinatoryczne podziały punktów na klastry. W szczególności, dany punkt danych może zmienić swoje przyporządkowanie do klastra w trakcie wykonywania algorytmu. Dzieje się tak, gdy zmienia się jego najbliższy punkt reprezentatywny.
\end{sphinxadmonition}


\chapter{TUTAJ}
\label{\detokenize{docs/unsupervised:tutaj}}

\section{Interpretacja jako ANN}
\label{\detokenize{docs/unsupervised:interpretacja-jako-ann}}\label{\detokenize{docs/unsupervised:inn-sec}}
\sphinxAtStartPar
Zinterpretujemy teraz zastosowany powyżej algorytm uczenia nienadzorowanego ze strategią ,,zwycięzca bierze wszystko” w języku sieci neuronowych. Weźmy następującą przykładową sieć:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_75_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Składa się ona z czterech neuronów w warstwie pośredniej, z których każdy odpowiada jednemu punktowi charakterystycznemu \(\vec{R}^i\). Wagi są współrzędnymi punktu \(\vec{R}^i\). W warstwie wyjściowej znajduje się jeden węzeł. Zauważamy istotne różnice w stosunku do omawianego wcześniej perceptronu.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Nie ma węzłów progowych.

\item {} 
\sphinxAtStartPar
W warstwie pośredniej sygnał jest równy kwadratowi odległości inputu od odpowiedniego punktu reprezentatywnego. Nie jest to suma ważona.

\item {} 
\sphinxAtStartPar
Węzeł w ostatniej warstwie (MIN) wskazuje, w którym neuronie warstwy pośredniej sygnał jest najmniejszy, tzn. gdzie mamy najmniejszą odległość. Działa on zatem jako jednostka sterująca wybierająca minimum.

\end{itemize}

\sphinxAtStartPar
Podczas uczenia (nienadzorowanego) punkt wejściowy P ,,przyciąga»» najbliższy punkt charakterystyczny, którego wagi są aktualizowane w kierunku współrzędnych P.

\sphinxAtStartPar
Zastosowanie powyższej sieci klasyfikuje punkt o współrzędnych \((x_1, x_2)\), przypisując mu wskaźnik najbliższego punktu reprezentatywnego dla danej kategorii (tutaj jest to etykieta 1, 2, 3 lub 4).


\subsection{Reprezentacja z pomocą  współrzędnych sferycznych}
\label{\detokenize{docs/unsupervised:reprezentacja-z-pomoca-wspolrzednych-sferycznych}}
\sphinxAtStartPar
Nawet przy naszej ogromnej „swobodzie matematycznej” nazwanie powyższego systemu siecią neuronową byłoby sporym nadużyciem, ponieważ wydaje się on bardzo daleki od jakiegokolwiek wzorca neurobiologicznego. W szczególności, użycie (nieliniowego) sygnału w postaci \(\left( \vec{R}^i-\vec{x}\right)^2\) kontrastuje z perceptronem, w którym sygnał wchodzący do neuronów jest (liniową) sumą ważoną inputów, tzn.
\begin{equation*}
\begin{split} s^ i = x_1 w_1 ^ i + x_2 w_2 ^ i + ... + w_1 ^ m x_m = \vec {x} \cdot \vec {w} ^ i. \end{split}
\end{equation*}
\sphinxAtStartPar
Możemy zmienić nasz problem, stosując prostą konstrukcję geometryczną tak, aby upodobnić go do zasady działania perceptronu. W tym celu wprowadzamy (fikcyjną, pomocniczą) trzecią współrzędną zdefiniowaną jako
\begin{equation*}
\begin{split} x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2}, \end{split}
\end{equation*}
\sphinxAtStartPar
gdie \( r \) jest dobrane tak, aby dla wszystkich punktów danych \( r ^ 2 \ge x_1 ^ 2 + x_2 ^ 2 \). Z konstrukcji, \( \vec {x} \cdot \vec {x} = x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 = r ^ 2 \), więc punkty danych leżą na półkuli (\( x_3 \ge 0 \)) o promieniu \( r \). Podobnie, dla punktów reprezentatywnych wprowadzamy
\begin{equation*}
\begin{split} w_1 ^ i = R_1 ^ i,  \; w_2 ^ i = R_2 ^ i,  \; 
w_3 ^ i = \sqrt {r ^ 2-(R_1 ^i)^2 -(R_2 ^i)^2}. \end{split}
\end{equation*}
\sphinxAtStartPar
Jest geometrycznie oczywiste, że dwa punkty na płaszczyźnie są sobie bliskie wtedy i tylko wtedy, gdy ich rozszerzenia ndo półkuli są sobie bliskie. Stwierdzenie to poprzemy prostym rachunkiem:

\sphinxAtStartPar
Iloczyn skalarny dwóch punktów \(\vec{x} \) i \(\vec{y} \) na półkuli można zapisać jako
\begin{equation*}
\begin{split} \vec {x} \cdot \vec {y} = x_1 y_1 + x_2 y_2 + \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \sqrt {r ^ 2-y_1 ^ 2-y_2 ^ 2}. \end{split}
\end{equation*}
\sphinxAtStartPar
Dla uproszczenia rozważmy sytuację, w której \( x_1 ^ 2 + x_2 ^ 2 \ll r ^ 2 \) and \( y_1 ^ 2 + y_2 ^ 2 \ll r ^ 2 \), tj. obydwa punkty leżą w pobliżu bieguna półkuli. Korzystając z wiedzy z zakresu analizy matematycznej
\begin{equation*}
\begin{split} \sqrt{r^2-a^2} \simeq r - \frac{a^2}{2r},  \;\;\;a \ll r, \end{split}
\end{equation*}
\sphinxAtStartPar
zatem
\begin{equation*}
\begin{split} \vec{x} \cdot \vec{y} \simeq x_1 y_1 + x_2 y_2 + \left( r -\frac{x_1^2+x_2^2}{2r} \right) \left( r -\frac{y_1^2+y_2^2}{2r} \right) \\ 
\;\;\;\simeq r^2 - \frac{1}{2} (x_1^2+x_2^2 +y_1^2+y_2^2) + x_1 y_1+x_2 y_2 \\ 
\;\;\; = r^2 - \frac{1}{2}[ (x_1-x_2)^2 +(y_1-y_2)^2]. \end{split}
\end{equation*}
\sphinxAtStartPar
Iloczyn skalarny jest równy (dla punktów położonych blisko bieguna) stałej \( r ^ 2 \) minus połowa kwadratu odległości między punktami \( (x_1, x_2) \) i \( (y_1, y_2) \) na płaszczyźnie! Wynika z tego, że zamiast znajdować minimalną odległość dla punktów na płaszczyźnie, jak w poprzednim algorytmie, możemy znaleźć maksymalny iloczyn skalarny dla ich 3\sphinxhyphen{}wymiarowych rozszerzeń do półkuli.

\sphinxAtStartPar
Po rozszerzeniu danych do półkuli, odpowiednią sieć neuronową można przedstawić w następujący sposób:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_81_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Dzięki naszym staraniom sygnał w warstwie pośredniej jest teraz po prostu iloczynem skalarnym inputu i wag, dokładnie tak jak powinno być w sztucznym neuronie. Neuron w ostatniej warstwie (MAX) wskazuje, gdzie iloczyn skalarny jest największy.

\sphinxAtStartPar
Interpretacja funkcji MAX w naszych obecnych ramach jest nadal nieco problematyczna. W rzeczywistości jest to możliwe, ale wymaga wyjścia poza sieci typu feed\sphinxhyphen{}forward. Gdy neurony w warstwie mogą się komunikować (sieci rekurencyjne, \sphinxhref{https://en.wikipedia.org/wiki/Hopfield\_network}{sieci Hopfielda}), konkurować, a przy odpowiednim sprzężeniu zwrotnym można wymusić mechanizm ,,zwycięzca bierze wszystko”. Aspekty te będą wspomniane w rozdz. \{ref\}`lat\sphinxhyphen{}lab»».

\begin{sphinxadmonition}{note}{Reguła Hebba}

\sphinxAtStartPar
Od strony koncepcyjnej dotykamy tutaj bardzo ważnej i intuicyjnej zasady w biologicznych sieciach neuronowych, znanej jako {[}reguła Hebba{]} (\sphinxurl{https://en.wikipedia.org/wiki/Hebbian\_theory}). Zasadniczo odnosi się ona do stwierdzenia „To, co jest używane, staje się silniejsze” w odniesieniu do połączeń synaptycznych. Wielokrotne użycie połączenia sprawia, że staje się ono silniejsze.
\end{sphinxadmonition}

\sphinxAtStartPar
W naszym sformułowaniu, jeśli sygnał przechodzi przez dane połączenie, jego waga odpowiednio się zmienia, podczas gdy inne połączenia pozostają bez zmian. Proces ten odbywa się w sposób nienadzorowany, a jego realizacja jest dobrze umotywowana biologicznie.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Z drugiej strony, trudno jest znaleźć biologiczne uzasadnienie dla uczenia nadzorowanego metodą backprop, w której wszystkie wagi są aktualizowane, także w warstwach bardzo odległych od wyjścia. Zdaniem wielu badaczy jest to raczej koncepcja matematyczna (niemniej niezwykle użyteczna).
\end{sphinxadmonition}


\subsection{Maksymalizacja iloczynu skalarnego}
\label{\detokenize{docs/unsupervised:maksymalizacja-iloczynu-skalarnego}}
\sphinxAtStartPar
Obecny algorytm klasteryzacji jest następujący:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Przedłużamy punkty próbki o trzecią współrzędną, \( x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \), wybierając odpowiednio duże \( r \), aby \( r ^ 2> x_1 ^ 2 + x_2 ^ 2 \) dla wszystkich punktów próbki.

\item {} 
\sphinxAtStartPar
Inicjalizujemy wagi w taki sposób, że \( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \).

\end{itemize}

\sphinxAtStartPar
Następnie wykonujemy pętlę po punktach danych:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Znajdujemy neuron w warstwie pośredniej, dla którego iloczyn skalarny \( x \cdot \vec {w} _i \) jest największy. Zmieniamy wagi tego neuronu wg. wzoru

\end{itemize}
\begin{equation*}
\begin{split} \vec {w} ^ i \to \vec {w} ^ i + \varepsilon (\vec {x} - \vec {w} ^ i). \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Renormalizujemy uaktualnione wagi \( \vec {w_i} \) tak, aby \( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \):

\end{itemize}
\begin{equation*}
\begin{split} \vec {w} ^ i \to \vec {w} ^ i \frac {r} {\sqrt {\vec {w} _i \cdot \vec {w} _i}}. \end{split}
\end{equation*}
\sphinxAtStartPar
Pozostałe kroki algorytmu, takie jak wyznaczanie początkowych położeń punktów reprezentatywnych, ich dynamiczne tworzenie w miarę napotykania kolejnych punktów danych itp. pozostają dokładnie takie same, jak w poprzednio omawianej procedurze.

\sphinxAtStartPar
Uogólnienie dla \(n\) wymiarów jest oczywiste: wprowadzamy dodatkową współrzędną
\begin{equation*}
\begin{split} x_ {n + 1} = \sqrt {r ^ 2 - x_1 ^ 2 -...- x_n ^ 2},\end{split}
\end{equation*}
\sphinxAtStartPar
mamy więc punkt na hiperhemisferze
\( x_1 ^ 2 + \dots + x_n ^ 2 + x_ {n + 1} ^ 2 = r ^ 2 \),  \(x_ {n + 1} >0\).

\sphinxAtStartPar
W Pythonie odpowiedni kod jest następujący:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{d}\PYG{o}{=}\PYG{l+m+mf}{0.25}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}

\PYG{n}{rad}\PYG{o}{=}\PYG{l+m+mi}{2}    \PYG{c+c1}{\PYGZsh{} radius of the hypersphere}

\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
                                        \PYG{c+c1}{\PYGZsh{} extension of R to the hypersphere}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} 
                    \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
                                        \PYG{c+c1}{\PYGZsh{} extension of p to the hypersphere}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} array of dot products}
        \PYG{n}{ind\PYGZus{}max} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}                         \PYG{c+c1}{\PYGZsh{} maximum}
        \PYG{k}{if} \PYG{n}{dist}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{:}
             \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,} \PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}   
            \PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of representative points: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of representative points:  4
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_88_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Można łatwo zauważyć, że algorytm maksymalizacji iloczynu skalarnego daje niemal dokładnie taki sam wynik jak minimalizacja kwadratu odległości (por. \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}).


\section{Ćwiczenia}
\label{\detokenize{docs/unsupervised:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Metrykę miejską (Manhattanu) definiuje się jako

\end{enumerate}

\sphinxAtStartPar
\( d (\vec {x}, \vec {y}) = | x_1-y_1 | + | x_2 - y_2 | \) dla punktów \( \vec {x} \) i \( \vec {y} \).
Powtórz symulacje z tego rozdziału, stosując tę metrykę. Wyciągnij wnioski.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Uruchom algorytm klasyfikacyjne dla większej liczby kategorii w próbce danych (wygeneruj własną próbkę).

\item {} 
\sphinxAtStartPar
Rozszerz algorytm dynamicznej klasteryzacji na trójwymiarową przestrzeń danych.

\end{enumerate}
\end{sphinxadmonition}


\chapter{Mapy samoorganizujące się}
\label{\detokenize{docs/som:mapy-samoorganizujace-sie}}\label{\detokenize{docs/som::doc}}
\sphinxAtStartPar
A very important and ingenious application of unsupervised learning are the so\sphinxhyphen{}called \sphinxstylestrong{Kohonen networks} (\sphinxhref{https://en.wikipedia.org/wiki/Teuvo\_Kohonen}{Teuvo Kohonen}, a class of \sphinxstylestrong{self\sphinxhyphen{}organizing mappings (SOM)}. Consider firs a mapping \(f\) between a \sphinxstylestrong{discrete} \(k\)\sphinxhyphen{}dimensional set (we call it a \sphinxstylestrong{grid} in this chapter) of neurons and \(n\)\sphinxhyphen{}dimensional input data \(D\) (continuous or discrete),
\begin{equation*}
\begin{split}
f: N \to D
\end{split}
\end{equation*}
\sphinxAtStartPar
(note that \sphinxstylestrong{this is not a Kohonen mapping yet!}).
Since \(N\) is discrete, each neuron carries an index consisting of \(k\) natural numbers, denoted as \(\bar {i} = (i_1, i_2, ..., i_k)\). Typically, the dimensions in Kohonen’s networks satisfy \(n \ge k\). When \(n > k\), one talks about \sphinxstylestrong{reduction of dimensionality}, as then the input space \(D\) has more dimensions than the dimensionaiy of the grid of neurons \(N\).

\sphinxAtStartPar
Two examples of such networks are visualized in \hyperref[\detokenize{docs/som:koh-fig}]{Rys.\@ \ref{\detokenize{docs/som:koh-fig}}}. The left panel shows a 2\sphinxhyphen{}dim. input space \(D\), and a one dimensional grid on neurons labeled with \(i\). The input point \((x_1,x_2)\) enters all the neurons in the grid, and one of the neurons (the one with best\sphinxhyphen{}suited weights) becomes the \sphinxstylestrong{winner} (red dot). The gray oval indicates the \sphinxstylestrong{neighborhood} of the winner, to be defined accurately in the following.

\sphinxAtStartPar
The right panel shows an analogous situation for the case of a 3\sphinxhyphen{}dim. input and 2\sphinxhyphen{}dim. grid of neurons, now labeled with a double index \(\bar {i} = (i_1, i_2)\). Here, for clarity, we only indicate the edges entering the winner, but they also enter all the other neurons in the grid, similarly to the left panel.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{koha}.png}
\caption{Example of Kohonen’s networks. Left: 1\sphinxhyphen{}dim. grid of neurons \(N\) and 2\sphinxhyphen{}dim. input space \(D\). Right: 2\sphinxhyphen{}dim. grid of neurons \(N\) and 3\sphinxhyphen{}dim. input space \(D\). The red dot indicates the winner, and the gray oval marks its neighborhood.}\label{\detokenize{docs/som:koh-fig}}\end{figure}

\sphinxAtStartPar
Next, one defines the neuron \sphinxstylestrong{proximity function}, \(\phi (\bar {i}, \bar {j})\), which assigns, to a pair of neurons, a real number depending on their relative position in the grid. This function must decrease with the distance between the neuron indices. A popular choice is a Gaussian,
\begin{equation*}
\begin{split} \phi(\bar{i}, \bar{j})=\exp\left [ -\frac{(i_1-j_1)^2+...+(i_k-j_k)^2}{2 \delta^2} \right ] ,\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\delta\) is the \sphinxstylestrong{neighborhood radius}. For a 1\sphinxhyphen{}dim. grid we have \( \phi(i,j)=\exp\left [ -\frac{(i-j)^2}{2 \delta^2} \right ]\).


\section{Kohonen’s algorithm}
\label{\detokenize{docs/som:kohonen-s-algorithm}}
\sphinxAtStartPar
The set up for Kohonen’s algorithm is similar to the unsupervised learning discussed in the previous chapter. Each neuron \(\bar{i}\) obtains weights \(f\left(\bar{i}\right)\), which are elements of \(D\), i.e. form \(n\)\sphinxhyphen{}dimensional vectors. One may simply think of this procedure as placing the neurons in some locations in \(D\).

\sphinxAtStartPar
When an input point \(P\) from \(D\) is fed into the network, one looks for the closest neuron, which becomes the \sphinxstylestrong{winner}, exactly as in the unsupervised learning algorithm from section {\hyperref[\detokenize{docs/unsupervised:inn-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Interpretacja jako ANN}}}}. However, now comes a \sphinxstylestrong{crucial difference}: Not only the winner is attracted (updated) a bit towards \(P\), but also its neighbors, to a lesser and lesser extent the farther they are from the winner, as quantified by the proximity function.

\begin{sphinxadmonition}{note}{Winner\sphinxhyphen{}take\sphinxhyphen{}most strategy}

\sphinxAtStartPar
Kohonen’s algorithm involves the „winner take most” strategy, where not only the winner neuron is updated (as in the winner\sphinxhyphen{}take\sphinxhyphen{}all case), but also its neighbors. The neighbors update is strongest for the nearest neighbors, and gradually weakens with the distance from the winner, as given by the proximity function.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Kohnen’s algorithm}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Initialize (for instance randomly) \(n\)\sphinxhyphen{}dimensional weight vectors \(w_i\), \(i-1,\dots,m\) for all the \(m\) neurons in the grid. Set an an initial neighborhood radius \( \delta \) and an initial learning speed \( \varepsilon \).

\item {} 
\sphinxAtStartPar
Choose (for instance, randomly) a data point \(P\) with coordinates \(x\) from the input space (possibly with an appropriate probability distribution).

\item {} 
\sphinxAtStartPar
Find the neuron (the winner) for which the distance from \(P\) is the smallest. Denote its index as \( \bar {l} \).

\item {} 
\sphinxAtStartPar
The weights of the winner and its neighbors are updated according to the \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}most} recipe:

\end{enumerate}
\begin{equation*}
\begin{split}w_{\bar{i}} \to w_{\bar{i}} + \varepsilon \phi(\bar{i}, \bar{l})(x - w_{\bar{i}}), \hspace{1cm} i=1, \dots , m. 
\end{split}
\end{equation*}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Loop from \(1.\) for a specified number of points.

\item {} 
\sphinxAtStartPar
Repeat from \(1.\) in rounds, until a satisfactory result is obtained or a stopping criterion is reached. In each round  \sphinxstylestrong{reduce} \( \varepsilon \) and \( \delta \) according to a chosen policy.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
The way the reduction of \( \varepsilon \) and \( \delta \) is done is very important for the desired outcome of the algorithm (see exercises).
\end{sphinxadmonition}


\subsection{2\sphinxhyphen{}dim. data and 1\sphinxhyphen{}dim. neuron grid}
\label{\detokenize{docs/som:dim-data-and-1-dim-neuron-grid}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{num}\PYG{o}{=}\PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} number of neurons}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
and the Gaussian proximity function

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{phi}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{k}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}                       \PYG{c+c1}{\PYGZsh{} proximity function}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{n}{k}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Gaussian}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
This function looks as follows around the middle neuron (\(k=50\)) and for the width parameter \(\delta=5\):

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_17_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
As a feature of a Gaussian, at \(|k-i|=\delta\) the function drops to \(~60\%\) of the central value, and at \(|k-i|=3\delta\) to \(~1\%\), a tiny fraction. Hence \(\delta\) controls the size of the neighborhood of the winner. The neurons farther away from the winner than, say, \(3\delta\) are practically left uncharged.

\sphinxAtStartPar
We initiate the network by by placing the grid inside the circle, with a random location of each neuron. As said, this amounts to assigning weights to the neuron equal to its location. An auxiliary line is drawn to guide the eye sequentially along the neuron indices: \(1,2,3,\dots m\). The line has no other meaning.

\sphinxAtStartPar
The weights (neuron locations) are stored in array \sphinxstylestrong{W}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random initialization of weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
As a result of the initial randomness, the neurons are, of course, „chaotically” distributed:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_21_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Next, we initialize the parameters \sphinxstylestrong{eps} amd \sphinxstylestrong{delta} and run the algorithm. Its structure is analogous to the previously discussed codes and is a straightforward implementation of the steps spelled out in the previous section. For that reason, we only provide the comments in the code.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial learning speed }
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{10}  \PYG{c+c1}{\PYGZsh{} initial neighborhood distance}
\PYG{n}{ste}\PYG{o}{=}\PYG{l+m+mi}{0}    \PYG{c+c1}{\PYGZsh{} inital number of caried out steps}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Kohonen\PYGZsq{}s algorithm}
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}              \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.98}                   \PYG{c+c1}{\PYGZsh{} dicrease learning speed}
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.95}                     \PYG{c+c1}{\PYGZsh{} ... and the neighborhood distance}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{150}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{ste}\PYG{o}{=}\PYG{n}{ste}\PYG{o}{+}\PYG{l+m+mi}{1}                 \PYG{c+c1}{\PYGZsh{} count steps}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{]} 
         \PYG{c+c1}{\PYGZsh{} array of squares of Euclidean disances between p and the neuron locations}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} index of the winner}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{:}      \PYG{c+c1}{\PYGZsh{} loop over all the neurons}
            \PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{,}\PYG{n}{k}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} 
             \PYG{c+c1}{\PYGZsh{} update of the neuron locations (weights), depending on proximity}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
As the above algorithm progresses (see \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}) the neuron grid first disentangles, and then gradually fills the whole space \(D\) (circle) in such a way that the neurons with adjacent indices are located close to each other.
Figuratively speaking, a new point \(P\) attracts towards itself the nearest neuron (the winner), but also, to a weaker extent, its neighbors. At the beginning of the algorithm the neighborhood distance \sphinxstylestrong{de} is large, so large chunks of the neighboring neurons in the input grid are pulled together towards \(P\), and the arrangement looks as in the top right corner of \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}. At later stages \sphinxstylestrong{de} reduces, so only the winner and possibly its very immediate neighbors are attracted to a new point.
After completion (bottom right panel), individual neurons „specialize” (are close to) in a certain data area.

\sphinxAtStartPar
In the present example, after about 20000 steps the result practically stops to change.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{kaall}.png}
\caption{Progress of Kohonen’s algorithm. The line, drawn to guide the eye, connects neurons with adjacent indices.}\label{\detokenize{docs/som:kohstory-fig}}\end{figure}

\begin{sphinxadmonition}{note}{Kohonen’s network as a classifier}

\sphinxAtStartPar
Having the trained network, we may use it as a classifier similarly as in chapter {\hyperref[\detokenize{docs/unsupervised:un-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Uczenie nienadzorowane}}}}. We label a point from \(D\) with the index of the nearest neuron. One can interpret this as a Voronoi construction, see section {\hyperref[\detokenize{docs/unsupervised:vor-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Komórki Woronoja}}}}.
\end{sphinxadmonition}

\sphinxAtStartPar
The plots in \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}} are made in coordinates \((x_1,x_2)\), that is, from the „point of view” of the input \(D\)\sphinxhyphen{}space. One may also look at the result from the point of view of the \(N\)\sphinxhyphen{}space, i.e. plot \(x_1\) and \(x_2\) as functions of the neuron index \(i\).

\begin{sphinxadmonition}{note}{Caution}

\sphinxAtStartPar
When presenting results of Kohonen’s algorithm, one sometimes makes plots in \(D\)\sphinxhyphen{}space, and sometimes in \(N\)\sphinxhyphen{}space, which may lead to some confusion.
\end{sphinxadmonition}

\sphinxAtStartPar
The plots in the \(N\)\sphinxhyphen{}space, fully equivalent in information to the plot in, e.g., the bottom right panel of \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}, are following:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_30_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We note that the jumps in the above plotted curves are small, since the subsequent neurons are close to each other. This feature can be presented quantitatively as in the histogram below, where we can see that the average distance between the neurons is about 0.07, and the spread is between 0.05 and 0.10.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dd}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{p}{(}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} array of distances between subsequent neurons in the grid}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.8}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{dd}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} histogram}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_32_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Remarks}
\begin{itemize}
\item {} 
\sphinxAtStartPar
We took a situation in which the data space with the dimension \(n = 2\) is „sampled” by a discrete set of neurons forming  \(k=1\)\sphinxhyphen{}dimensional grid. Hence we encounter dimensional reduction.

\item {} 
\sphinxAtStartPar
The outcome of the algorithm is a network in which a given neuron „focuses” on data from its vicinity. In a general case, where the data can be non\sphinxhyphen{}uniformly distributed, the neurons would fill the area containing more data more densely.

\item {} 
\sphinxAtStartPar
The policy of choosing initial \(\delta\) and \(\varepsilon \) parameters and reducing them appropriately in subsequent rounds is based on experience and is non\sphinxhyphen{}trivial. The results depend significantly on this choice.

\item {} 
\sphinxAtStartPar
The final result, even with the same \(\delta\) and \(\varepsilon \) strategy, is not unequivocal, i.e. running the algorithm with a different initialization of the weights (initial positions of neurons) yields different outcomes, usually equally „good”.

\item {} 
\sphinxAtStartPar
Finally, the progress and the result of the algorithm is reminiscent of the construction of the \sphinxhref{https://en.wikipedia.org/wiki/Peano\_curve}{Peano curve} in mathematics, which fills densely an area with a line.
As we increase the number of neurons, the analogy gets closer and closer.

\end{itemize}
\end{sphinxadmonition}


\subsection{2 dim. color map}
\label{\detokenize{docs/som:dim-color-map}}
\sphinxAtStartPar
Now we pass to a case of 3\sphinxhyphen{}dim. data and 2\sphinxhyphen{}dim. neuron grid, which is a situation from the right panel of \hyperref[\detokenize{docs/som:koh-fig}]{Rys.\@ \ref{\detokenize{docs/som:koh-fig}}} (hence also with dimensionality reduction). As we know, an RGB color is described with three numbers \([r,g,b]\) from \([0,1]\), so it can nicely serve as input in our example.

\sphinxAtStartPar
The distance squared between two colors (this is just a distance between two points in the 3\sphinxhyphen{}dim. space) is taken in the Euclidean form:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{dist3}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:} 
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Square of the Euclidean distance between points p1 and p2}
\PYG{l+s+sd}{    in 3 dimensions.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dist3}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{.3}\PYG{p}{,}\PYG{l+m+mf}{.3}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1.08
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The proximity function is now a Gaussian in two dimensions:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{phi2}\PYG{p}{(}\PYG{n}{ix}\PYG{p}{,}\PYG{n}{iy}\PYG{p}{,}\PYG{n}{kx}\PYG{p}{,}\PYG{n}{ky}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} proximity function for 2\PYGZhy{}dim. grid}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{p}{(}\PYG{n}{ix}\PYG{o}{\PYGZhy{}}\PYG{n}{kx}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{iy}\PYG{o}{\PYGZhy{}}\PYG{n}{ky}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Gaussian}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We also decide to normalize the RGB colors such that \(r^2+g^2+b^2=1\). This makes the perceived intensity of colors similar (this normalization could be dropped, as irrelevant for the method to work).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{rgbn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{r}\PYG{p}{,}\PYG{n}{g}\PYG{p}{,}\PYG{n}{b}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random RGB}
    \PYG{n}{norm}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{r}\PYG{o}{*}\PYG{n}{r}\PYG{o}{+}\PYG{n}{g}\PYG{o}{*}\PYG{n}{g}\PYG{o}{+}\PYG{n}{b}\PYG{o}{*}\PYG{n}{b}\PYG{p}{)}                                      \PYG{c+c1}{\PYGZsh{} norm}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{r}\PYG{p}{,}\PYG{n}{g}\PYG{p}{,}\PYG{n}{b}\PYG{p}{]}\PYG{o}{/}\PYG{n}{norm}\PYG{p}{)}                                  \PYG{c+c1}{\PYGZsh{} normalized RGB}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we generate and plot a sample of \sphinxstylestrong{ns} points with (normalized) RGB colors:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns}\PYG{o}{=}\PYG{l+m+mi}{10}                            \PYG{c+c1}{\PYGZsh{} number of colors in the sample}
\PYG{n}{samp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} random sample}

\PYG{n}{pls}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_43_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We use a 2\sphinxhyphen{}dim. \sphinxstylestrong{size} x \sphinxstylestrong{size} grid of neurons. Each neuron’s position (that is its color) in the 3\sphinxhyphen{}dim. \(D\)\sphinxhyphen{}space is initialized randomly:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{40}                        \PYG{c+c1}{\PYGZsh{} neuron array of size x size (40 x 40)}
\PYG{n}{tab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{p}{,}\PYG{n}{size}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} create array tab with zeros  }

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} i index in the grid    }
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}      \PYG{c+c1}{\PYGZsh{} j index in the grid}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} RGB: k=0\PYGZhy{}red, 1\PYGZhy{}green, 2\PYGZhy{}blue}
            \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random number form [0,1]}
            \PYG{c+c1}{\PYGZsh{} 3 RGB components for neuron in the grid positin (i,j)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_46_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Now we are ready to run Kohonen’s algorithm:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial parameters}
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{20}   
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{150}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.995}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.99}           \PYG{c+c1}{\PYGZsh{} de shrinks a bit faster than eps     }
    \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over the points in the data sample       }
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} point from the sample}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{]} 
                        \PYG{c+c1}{\PYGZsh{} distance of p from all neurons}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} the winner index}
        \PYG{n}{ind\PYGZus{}1}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{/}\PYG{o}{/}\PYG{n}{size}       \PYG{c+c1}{\PYGZsh{} a trick to get a 2\PYGZhy{}dim index}
        \PYG{n}{ind\PYGZus{}2}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{\PYGZpc{}}\PYG{k}{size}
\PYG{c+c1}{\PYGZsh{}        print(\PYGZdq{}winner:\PYGZdq{},ind\PYGZus{}1,ind\PYGZus{}2)}

        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:} 
            \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi2}\PYG{p}{(}\PYG{n}{ind\PYGZus{}1}\PYG{p}{,}\PYG{n}{ind\PYGZus{}2}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update         }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
A word of explanation is in place here, concerning the numpy \sphinxstylestrong{argmin} function. For a 2\sphinxhyphen{}dim. array it provides the index of the minimum in the corresponding \sphinxstylestrong{flattened} array (cf. section {\hyperref[\detokenize{docs/memory:het-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Pamieć skojarzeniowa (heteroasocjacyjna)}}}}). Hence, to get the indices in the two dimensions, we need to apply the operations \sphinxstylestrong{//} (integer division) and \sphinxstylestrong{\%} (remainder). For instance, in an array \sphinxstylestrong{ind\_min=53}, then \sphinxstylestrong{ind\_1=ind\_min//size=53//10=5} and \sphinxstylestrong{ind\_2=ind\_min\%size=53//10=3}.

\sphinxAtStartPar
As a result of the above code, we get an arrangement of our color sample in two dimensions in such a way that the neighboring areas in the grid have a similar color „specializing” on the color of a given sample point (note the plot is in the \(N\)\sphinxhyphen{}space):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Kohonen color map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_52_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Remarks}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The areas for the individual colors of the sample have a comparable area. Generally, the area is proportional to the frequency of the data point in the sample.

\item {} 
\sphinxAtStartPar
To get sharper boundaries between the regions, \sphinxstylestrong{de} would have to shrink even faster compared to \sphinxstylestrong{eps}. Then, in the final stage of learning, the neuron update process takes place within a smaller neighborhood radius and more resolution in the boundaries can be achieved.

\end{itemize}
\end{sphinxadmonition}


\section{\protect\(U\protect\)\sphinxhyphen{}matrix}
\label{\detokenize{docs/som:u-matrix}}
\sphinxAtStartPar
A convenient way to present the results of Kohonen’s algorithm when the grid is 2\sphinxhyphen{}dimensional is via the \sphinxstylestrong{unified distance matrix} (shortly \sphinxstylestrong{\(U\)\sphinxhyphen{}matrix}). The idea is to plot a 2\sphinxhyphen{}dimensional grayscale map in \(N\)\sphinxhyphen{}space with the intensity given by the averaged distance (in \(D\)\sphinxhyphen{}space) of the given neuron to its immediate neighbors, and not a neuron property itself (such as its color in the figure above). This is particularly useful when the dimension of the input space is large, hence it is difficult to visualize the results directly.

\sphinxAtStartPar
The definition of a \(U\)\sphinxhyphen{}matrix element \(U_{ij}\) is explained in \hyperref[\detokenize{docs/som:udm-fig}]{Rys.\@ \ref{\detokenize{docs/som:udm-fig}}}. Let \(d\) be the distance in \(D\)\sphinxhyphen{}space and \([i,j]\) denote the neuron of indices \(i,j\) . We take
\begin{equation*}
\begin{split}
U_{ij}=\sqrt{d\left([i,j],[i+1,j]\right)^2+d\left([i,j],[i-1,j]\right)^2+
        d\left([i,j],[i,j+1]\right)^2+d\left([i,j],[i,j-1]\right)^2 }.
\end{split}
\end{equation*}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=150\sphinxpxdimen]{{udm}.png}
\caption{Construction of \(U_{ij}\): a geometric average of the distances along the indicated links.}\label{\detokenize{docs/som:udm-fig}}\end{figure}

\sphinxAtStartPar
The Python implementation of the above definition is following:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{udm}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} initiaize U\PYGZhy{}matrix with elements set to 0}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loops over the neurons in the grid}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}
                            \PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
                                 \PYG{c+c1}{\PYGZsh{} U\PYGZhy{}matrix as explained above}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The result, corresponding one\sphinxhyphen{}to\sphinxhyphen{}one to the color map above, can be presented in a contour plot:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{U\PYGZhy{}matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)} 

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loops over indices, excluding the boundaries of the grid}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 
                        \PYG{c+c1}{\PYGZsh{} color format: [R,G,B,intensity], 2 just scales up}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_60_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The white regions in the above figure show the clusters (they correspond one\sphinxhyphen{}to\sphinxhyphen{}one to the regions of the same color in the previously shown color map). There, the elements \(U_{ij} \simeq 0\). The clusters are separated with darker boundaries. The higher the dividing ridge between clusters, the darker the intensity.

\sphinxAtStartPar
The result may also be visualized with a 3\sphinxhyphen{}dim. plot:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{axes1} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{111}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{3d}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{xx\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{xx\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{x\PYGZus{}1}\PYG{p}{,} \PYG{n}{x\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{xx\PYGZus{}1}\PYG{p}{,} \PYG{n}{xx\PYGZus{}2}\PYG{p}{)}

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{x\PYGZus{}1}\PYG{p}{,}\PYG{n}{x\PYGZus{}2}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{gray}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{U\PYGZhy{}matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_62_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can now classify a given (new) data point according to the obtained map. We generate a new (normalized) RGB color:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nd}\PYG{o}{=}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_65_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
It is useful to obtain a map of distances of our grid neurons from this point:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tad}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{p}{,}\PYG{n}{size}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{tad}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{=}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{nd}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        

\PYG{n}{ind\PYGZus{}m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{tad}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} winner}
\PYG{n}{in\PYGZus{}x}\PYG{o}{=}\PYG{n}{ind\PYGZus{}m}\PYG{o}{/}\PYG{o}{/}\PYG{n}{size}      
\PYG{n}{in\PYGZus{}y}\PYG{o}{=}\PYG{n}{ind\PYGZus{}m}\PYG{o}{\PYGZpc{}}\PYG{k}{size} 

\PYG{n}{da}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{tad}\PYG{p}{[}\PYG{n}{in\PYGZus{}x}\PYG{p}{]}\PYG{p}{[}\PYG{n}{in\PYGZus{}y}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Closest neuron grid indices: (}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{in\PYGZus{}x}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{in\PYGZus{}y}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Distance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{da}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Closest neuron grid indices: ( 21 , 39 )
Distance:  0.133
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_68_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The lightest region in the above figure indicates the cluster, to which the new point belongs. The darker the region, the larger is the distance from the corresponding neuron.

\sphinxAtStartPar
One should stress that we have obtained a classifier which not only assigns a closest cluster to a probed point, but also provides its distances from all other clusters.


\subsection{Mapping colors on a line}
\label{\detokenize{docs/som:mapping-colors-on-a-line}}
\sphinxAtStartPar
In this subsection we present an example of a mapping of 3\sphinxhyphen{}dim. data into a 1\sphinxhyphen{}dim. neuron grid, hence a reduction of three dimensions into one. This proceeds exactly along the lines of the previous subsection, so we are very brief in comments.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns}\PYG{o}{=}\PYG{l+m+mi}{8}
\PYG{n}{samp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample colors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)} 

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{)}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_72_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{si}\PYG{o}{=}\PYG{l+m+mi}{50}                    \PYG{c+c1}{\PYGZsh{} 1\PYGZhy{}dim. grid of si neurons, 3 RGB components}
\PYG{n}{tab2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{si}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} neuron gri}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{:}      
    \PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{=}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} random initialization}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_74_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}    
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{20}   
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{:} 
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.99}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.96}        
    \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}       
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{]} 
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}          
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_77_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
As expected, we note smooth transitions between colors. The formation of clusters can be seen with the \(U\)\sphinxhyphen{}matrix, which now is, of course, one\sphinxhyphen{}dimensional:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ta2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{si}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{si}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ta2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_80_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The minima (there are 8 of them, equal to the multiplicity of the sample) indicate the clusters. The height of the separating peaks shows how much the neighboring colors differ. Again, we see a nicely produced classifier, this time with two dimensions „hidden away”, as we reduce from three to one.


\subsection{Large reduction of dimensionality}
\label{\detokenize{docs/som:large-reduction-of-dimensionality}}
\sphinxAtStartPar
In many situations the input space may have a very large dimension. In the \sphinxhref{https://en.wikipedia.org/wiki/Self-organizing\_map}{Wikipedia example} quoted here, one takes articles from various fields and computes frequencies of used words (for instance, in a given article how  many times the word „goalkeeper” has been used, divided by the total number of words in the article). Essentially, the dimensionality of \(D\) is of the order of the number of all English words, a huge number \(\sim 10^5\)! Then, with a properly defined distance depending on these frequencies, one uses Kohonen’s algorithm to carry out a reduction into a 2\sphinxhyphen{}dim. grid of neurons. The resulting \(U\)\sphinxhyphen{}matrix can be drawn as follows:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_84_0}.jpg}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Not surprisingly, we notice that articles on sports are special and form a very well defined cluster. The reason is that the sport’s jargon is very specific. The Media are also distinguished, whereas other fields are more\sphinxhyphen{}less uniformly distributed. The example shows how we can, with a very simple method, comprehend data in a multidimensional space and see specific correlations/clusters. Whereas some conclusions may be obvious, such as the fact that sport has a unique jargon, other are less transparent, for instance the emergence of the media cluster and lack of well\sphinxhyphen{}defined clusters for other fields, e.g. for mathematics.


\section{Mapping 2\sphinxhyphen{}dim. data into a 2\sphinxhyphen{}dim. grid}
\label{\detokenize{docs/som:mapping-2-dim-data-into-a-2-dim-grid}}
\sphinxAtStartPar
Finally, we come to a very important case of mapping 2\sphinxhyphen{}dim. data in a 2\sphinxhyphen{}dim. grid, i.e. with no dimensionality reduction. In particular, this case is realized in our vision system between the retina and the visual cortex.

\sphinxAtStartPar
The algorithm proceeds analogously to the previous cases. We initialize an \(n \times n\) grid of neurons and place them randomly in the square \([0,1]\times [0,1]\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}
\PYG{n}{sam}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The lines, again drawn to guide the eye, join the adjacent index pairs in the grid: {[}i,j{]} and {[}i+1,j{]}, or {[}i,j{]} and {[}i,j+1{]} (the neurons in the interior of the grid have 4 nearest neighbors, those at the boundary 3, except for the corners, which have only 2).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_90_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We note a total initial „chaos”, as the neurons are located randomly. Now comes Kohonen’s miracle:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{3}   \PYG{c+c1}{\PYGZsh{} initial neighborhood distance}
\PYG{n}{nr} \PYG{o}{=} \PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} number of rounds}
\PYG{n}{rep}\PYG{o}{=} \PYG{l+m+mi}{300} \PYG{c+c1}{\PYGZsh{} number of points in each round}
\PYG{n}{ste}\PYG{o}{=}\PYG{l+m+mi}{0}    \PYG{c+c1}{\PYGZsh{} inital number of caried out steps}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} completely analogous to the previous codes of this chapter}
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nr}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.97}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.98}         
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{rep}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} repeat for rep points}
        \PYG{n}{ste}\PYG{o}{=}\PYG{n}{ste}\PYG{o}{+}\PYG{l+m+mi}{1}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} 
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{l} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} 
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} 
        \PYG{n}{ind\PYGZus{}i}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{\PYGZpc{}}\PYG{k}{n}
        \PYG{n}{ind\PYGZus{}j}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{/}\PYG{o}{/}\PYG{n}{n}       
        
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:} 
            \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{sam}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{n}{n}\PYG{o}{*}\PYG{n}{j}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi2}\PYG{p}{(}\PYG{n}{ind\PYGZus{}i}\PYG{p}{,}\PYG{n}{ind\PYGZus{}j}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{n}{n}\PYG{o}{*}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Here is the history of a simulation:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{kball}.png}
\caption{Progress of Kohonen’s algorithm. The lines, drawn to guide the eye, connects neurons with adjacent indices.}\label{\detokenize{docs/som:kohstory2-fig}}\end{figure}

\sphinxAtStartPar
As the algorithm progresses, the initial „chaos” gradually changes into a nearly perfect order, with the grid placed uniformly in the square of the data, with only slight displacements from a regular arrangement. On the way, near 40 steps, we notice a phenomenon called „twist”, where the grid is crumpled. In the twist region, many neurons, also of distant indices, have a close location in \((x_1,x_2)\).


\section{Topology}
\label{\detokenize{docs/som:topology}}
\sphinxAtStartPar
Recall the Voronoi construction of categories introduced in section {\hyperref[\detokenize{docs/unsupervised:vor-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Komórki Woronoja}}}}. One can use it now again, treating the neurons from a grid as the Voronoi points. The Voronoi construction provides a mapping \(v\) from the data space \(D\) to the neuron space \(N\),
\begin{equation*}
\begin{split} 
v: D \to N 
\end{split}
\end{equation*}
\sphinxAtStartPar
(note that this goes in the opposite direction than function \(f\) defined at the beginning of this chapter).

\sphinxAtStartPar
The procedure is as follows:
We take the final outcome of the algorith, such as in the  bottom right panel of \hyperref[\detokenize{docs/som:kohstory2-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory2-fig}}}, construct the Voronoi areas for all the neurons, and thus obtain a mapping \(v\) for all the points in the \((x_1,x_2)\) square. The reader may notice that there is an ambiguity for points lying exactly at the boundaries between the neighboring areas, but this can be taken care of by using an additional prescription (for instance, selecting a neuron lying at a direction which has the lowest azimuthal angle, etc.)

\sphinxAtStartPar
Now a key observation:

\begin{sphinxadmonition}{note}{Topological property}

\sphinxAtStartPar
For situations without twists, such as in the bottom right panel of \hyperref[\detokenize{docs/som:kohstory2-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory2-fig}}}, mapping \(v\) has the property that when \(d_1\) and \(d_2\) from \(D\) are close to each other, then also their corresponding neurons are close, i.e. the indices \(v(d_1)\) and \(v(d_2)\) are close.
\end{sphinxadmonition}

\sphinxAtStartPar
This observation is straightforward to prove: Since \(d_1\) and \(d_2\) are close (and we mean very close, closer than the grid spacing), they must belong either to
\begin{itemize}
\item {} 
\sphinxAtStartPar
the same Voronoi area, where \(v(d_1)=v(d_2)\), or

\item {} 
\sphinxAtStartPar
a pair of neighboring Voronoi areas.

\end{itemize}

\sphinxAtStartPar
Since for the considered situation (without twists) the neighboring areas have the grid indices differing by 1, the conclusion that \(v(d_1)\) and \(v(d_2)\) are close follows immediately.

\sphinxAtStartPar
Note that this feature of Kohonen’s maps is far from trivial and does not hold for a general mapping. Imagine for instance that we stop our simulations for \hyperref[\detokenize{docs/som:kohstory2-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory2-fig}}} after 40 steps (top central panel) and are left with a „twisted” grid. In the vicinity of the twist, the indices of the adjacent Voronoi areas differ largely, and the advertised topological property no longer holds.

\sphinxAtStartPar
The discussed topological property has mathematically general and far\sphinxhyphen{}reaching consequences. First, it allows to carry over „shapes” from \(D\) to \(N\). We illustrate it on an example.

\sphinxAtStartPar
Imagine that we have a circle \(C\) in \(D\)\sphinxhyphen{}space, of radius \sphinxstylestrong{rad} centered at \sphinxstylestrong{cent}. We need to find the winners in the \(N\) space for any point in \(C\). For this purpose we go around \(C\) in \sphinxstylestrong{npoi} points equally spaced in the azimuthal angle, and for each one find a winner.

\sphinxAtStartPar
\(C\) is parametrized with polar coordinates:
\begin{equation*}
\begin{split}
x_1=r \cos \left( \frac{2\pi \phi}{N}  \right)+c_1, \;\;\;
x_2=r \sin \left( \frac{2\pi \phi}{N}  \right)+c_2.
\end{split}
\end{equation*}
\sphinxAtStartPar
Going to the mathematical notation to Python we use
\(r=\)\sphinxstylestrong{rad}, \(\phi\)=\sphinxstylestrong{ph}, \(N=\)\sphinxstylestrong{npoi}, \((c_1,c_2)=\)\sphinxstylestrong{{[}cent{]}}.
The loop over \sphinxstylestrong{ph} goes around the circle.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rad}\PYG{o}{=}\PYG{l+m+mf}{0.35}                      \PYG{c+c1}{\PYGZsh{} radius of a circle}
\PYG{n}{cent}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} center of the circle}
\PYG{n}{npoi}\PYG{o}{=}\PYG{l+m+mi}{400}                      \PYG{c+c1}{\PYGZsh{} number of points in the circle}

\PYG{n}{wins}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}                       \PYG{c+c1}{\PYGZsh{} table of winners}

\PYG{k}{for} \PYG{n}{ph} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npoi}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} go around the circle}
    \PYG{n}{p}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{rad}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{/}\PYG{n}{npoi}\PYG{o}{*}\PYG{n}{ph}\PYG{p}{)}\PYG{p}{,}\PYG{n}{rad}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{/}\PYG{n}{npoi}\PYG{o}{*}\PYG{n}{ph}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{cent}
                              \PYG{c+c1}{\PYGZsh{} the circle in polar coordinates}
    \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{l} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} 
      \PYG{c+c1}{\PYGZsh{} distances from the point on the circle to the neurons in the nxn grid}
    \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} winner}
    \PYG{n}{wins}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} add winner to the table}
        
\PYG{n}{ci}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{n}{wins}\PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} remove duplicates from the table      }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The result of Kohonen’s algorithm is as follows:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_104_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The red neurons are the winners for certain sections of the circle. When we draw these winners alone in the \(N\) space (keep in mind we are going from \(D\) to \(N\)), we get

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{ci}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{n}{ci}\PYG{o}{\PYGZpc{}}\PYG{k}{10},c=\PYGZsq{}red\PYGZsq{},s=5)

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_106_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This looks pretty much as a (rough and discrete) circle. Note that in our example we only have \(n^2=100\) pixels to our disposal \sphinxhyphen{} a very low resolution. The image would look better and better with an increasing \(n\). At some point one would reach the 10M pixel resolution of typical camera, and then the image would seem smooth! We have carried over our circle from \(D\) into \(N\).

\begin{sphinxadmonition}{note}{Vision}

\sphinxAtStartPar
The topological property, such as the one in the discussed Kohonen mappings, has a prime importance in our vision system and the perception of objects. Shapes are carried over from the retina to the visual cortex and are not „warped up” on the way!
\end{sphinxadmonition}

\sphinxAtStartPar
Another key topological feature is the preservation of \sphinxstylestrong{connectedness}. If an area \(A\) in \(D\) is connected (so to speak, is in one piece), then its image \(v(A)\) in \(N\) is also connected (we ignore the desired rigor here as to what „connected” means in a discrete space and rely on intuition). So things do not get „torn into pieces” when transforming from \(D\) to \(N\).

\sphinxAtStartPar
Note that the discussed topological features need not be present when the dimensionality is reduced, as in our previous examples. Take for instance the bottom right panel of \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}. There, many neighboring pairs of the Voronoi areas correspond to distant indices, so it is no longer true that \(v(d_1)\) and \(v(d_2)\) in \(N\) are close for close \(d_1\) and \(d_2\) in \(D\), as these points may belong to different Voronoi areas with \sphinxstylestrong{distant} indices.

\sphinxAtStartPar
For that case, our example with the circle looks like this:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_112_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
When we go subsequently along the \sphinxstylestrong{grid indices} (i.e. along the blue connecting line), taking \(i=1,2,\dots,100\), we obtain the plot below. We can see the image of our circle (red dots) as a bunch of \sphinxstylestrong{disconnected} red sections. The circle is torn into pieces, the \sphinxstylestrong{topology is not preserved!}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_114_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Here is the summarizing statement (NB not made sufficiently clear in the literature):

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Topological features of Kohonen’s maps hold for equal dimensionalities of the input space and the neuron grid, \(n=k\), and in general do not hold for the reduced dimensionality cases, \(k<n\).
\end{sphinxadmonition}


\section{Lateral inhibition}
\label{\detokenize{docs/som:lateral-inhibition}}\label{\detokenize{docs/som:lat-lab}}
\sphinxAtStartPar
In the last topic of these lectures, we return to the issue of how the competition for the „winner” is realized in ANNs. Up to now (cf. section {\hyperref[\detokenize{docs/unsupervised:inn-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Interpretacja jako ANN}}}}), we have just been using the minimum (or maximum, when the signal was extended to a hyperphere) in the output, though this is embarrassingly outside of the neural framework. Such an inspection of which neuron yields the strongest signal would require an „external wizard”, or some sort of a control unit. Mathematically, it is easy to imagine, but the challenge is to build it from neurons within the rules of the game.

\sphinxAtStartPar
Actually, if the neurons in a layer „talk” to one another, we can have a „contest” from which a winner may emerge. In particular, an architecture as in \hyperref[\detokenize{docs/som:lat-fig}]{Rys.\@ \ref{\detokenize{docs/som:lat-fig}}} allows for an arrangement of competition and a natural realization of a \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}most} mechanism.

\sphinxAtStartPar
The type of models as presented below is known as the \sphinxhref{https://en.wikipedia.org/wiki/Hopfield\_network}{Hopfield networks}. Note that we depart here from the \sphinxstylestrong{feed\sphinxhyphen{}forward} limitation of \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} and allow for a recursive, or feed\sphinxhyphen{}back character.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=220\sphinxpxdimen]{{lat3}.png}
\caption{Network with inter\sphinxhyphen{}neuron couplings used for modeling lateral inhibition. All the neurons are connected to one another in both directions (lines without arrows).}\label{\detokenize{docs/som:lat-fig}}\end{figure}

\sphinxAtStartPar
Neuron number \(i\) receives the signal \(s_i = x w_i\), where \(x\) is the input (the same for all the neurons), and \(w_i\) is the weight of neuron \(i\). The neuron produces output \(y_i\), where now a part of it is sent to neurons \(j\) as \(F_{ji} y_i\). Here \(F_{ij}\) denotes the coupling strength (we assume \(F_{ii}=0\) \sphinxhyphen{} no self coupling). Reciprocally, neuron \(i\) also receives output from neurons \(j\) in the form \(F_{ij} y_j\). The summation over all the neurons yields
\begin{equation*}
\begin{split} 
y_i = s_i + \sum_{j\neq i} F_{ij} y_j, 
\end{split}
\end{equation*}
\sphinxAtStartPar
which in the matrix notation becomes \( y = s + F y\), or \(y(I-F)=s\), where \(I\) is the identity matrix. Solving for \(y\) gives formally
\begin{equation}\label{equation:docs/som:eq-lat}
\begin{split}y= (I-F)^{-1} s.\end{split}
\end{equation}
\sphinxAtStartPar
One needs to model appropriately the coupling matrix \(F\). We take

\sphinxAtStartPar
\( F_ {ii} = \) 0,

\sphinxAtStartPar
\( F_ {ij} = - a \exp (- | i-j | / b) ~~ \) for \( i \neq j \), \( ~~ a, b> 0 \),

\sphinxAtStartPar
i.e. assume attenuation (negative feedback), which is strongest for close neighbors and decreases with distance. The decrease is controlled by a characteristic scale \(b\).

\sphinxAtStartPar
The Python implementation is straightforward:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns} \PYG{o}{=} \PYG{l+m+mi}{30}\PYG{p}{;}       \PYG{c+c1}{\PYGZsh{} number of neurons}
\PYG{n}{b} \PYG{o}{=} \PYG{l+m+mi}{4}\PYG{p}{;}         \PYG{c+c1}{\PYGZsh{} parameter controlling the decrease of damping with distance}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}         \PYG{c+c1}{\PYGZsh{} magnitude of damping}

\PYG{n}{F}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{a}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{n}{j}\PYG{p}{)}\PYG{o}{/}\PYG{n}{b}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} 
                    \PYG{c+c1}{\PYGZsh{} exponential fall\PYGZhy{}off}
    
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{F}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{0}       \PYG{c+c1}{\PYGZsh{} no self\PYGZhy{}coupling}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_123_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We assume a bell\sphinxhyphen{}shaped Lorentzian input signal \(s\), with a maximum in the middle neuron. The width is controlled with \sphinxstylestrong{D}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{D}\PYG{o}{=}\PYG{l+m+mi}{3}
\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{D}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{p}{(}\PYG{p}{(}\PYG{n}{i} \PYG{o}{\PYGZhy{}} \PYG{n}{ns}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{D}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Lorentzian function}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we solve Eq. \eqref{equation:docs/som:eq-lat} via inverting the \((I-F)\) matrix, performed with the numpy \sphinxstylestrong{linalg.inv} function. Recall that \sphinxstylestrong{dot} multiplies matrices:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{invF}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{identity}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{F}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} matrix inversion}
\PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{invF}\PYG{p}{,}\PYG{n}{s}\PYG{p}{)}                      \PYG{c+c1}{\PYGZsh{} multiplication}
\PYG{n}{y}\PYG{o}{=}\PYG{n}{y}\PYG{o}{/}\PYG{n}{y}\PYG{p}{[}\PYG{l+m+mi}{15}\PYG{p}{]}                             \PYG{c+c1}{\PYGZsh{} normalization (inessential) }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
What follows is actually quite remarkable: the output signal \(y\) becomes much narrower from the input signal \(s\). This may be interpreted as a realization of the „winner\sphinxhyphen{}take\sphinxhyphen{}all” scenario. The winner „damped” he guys around him, so he puts himself on airs! The effect is smooth, with the signal visibly sharpened.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_129_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Lateral inhibition}

\sphinxAtStartPar
The damping of the response of neighboring neurons is called \sphinxstylestrong{lateral inhibition}. It was discovered in neurobiological networks {[}\hyperlink{cite.docs/conclusion:id14}{HR72}{]}.
\end{sphinxadmonition}

\sphinxAtStartPar
The presented model is certainly too simplistic to be realistic from the point of view of biological networks. Also, it yields unnatural negative signal outside of the central peak (which we can remove with rectification). Nevertheless, the setup shows a possible way to achieve the „winner competition”, essential for unsupervised learning: One needs to allow for the competing neurons to interact.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Actually, \sphinxstylestrong{pyramidal neurons}, present i.a. in the neocortex, have as many as a few thousand dendritic spines and do realize a scenario with numerous synaptic connections. They are believed \sphinxhref{https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/}{Quantamagazine} to play a crucial role in learning and cognition processes.
\end{sphinxadmonition}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{smi32-pic}.jpg}
\caption{Image of pyramidal neurons (from \sphinxhref{http://brainmaps.org/index.php?p=screenshots}{brainmaps.org})}\label{\detokenize{docs/som:pyr-fig}}\end{figure}


\section{Exercises}
\label{\detokenize{docs/som:exercises}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Construct a Kohonen mapping form a \sphinxstylestrong{disjoint} 2D shape into a 2D grid of neurons.

\item {} 
\sphinxAtStartPar
Construct a Kohonen mapping for a case where the points in the input space are not distributed uniformly, but denser in some regions.

\item {} 
\sphinxAtStartPar
Create, for a number of countries, fictitious flags which have two colors (hence are described with 6 RGB numbers). Construct a Kohonen map into a 2\sphinxhyphen{}dim. grid. Plot the resulting \(U\)\sphinxhyphen{}matrix and draw conclusions.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Lateral\_inhibition}{Lateral inhibition} has „side\sphinxhyphen{}effects” seen in optical delusions. Describe the \sphinxhref{https://en.wikipedia.org/wiki/Mach\_bands}{Mach illusion}, programming it in Python.

\end{enumerate}
\end{sphinxadmonition}


\chapter{Concluding remarks}
\label{\detokenize{docs/conclusion:concluding-remarks}}\label{\detokenize{docs/conclusion::doc}}
\sphinxAtStartPar
In a programmer’s life, building a well\sphinxhyphen{}functioning ANN, even for simple problems as used for illustrations in these lectures, can be a truly frustrating experience! There are many subtleties involved on the way. To list just a few that we have encountered in this course, one faces a choice of the network architecture and freedom in the initialization of weights (hyperparameters). Then, one has to select an initial learning speed, a neighborhood distance, in general, some parameters controlling the performance/convergence, as well as their update strategy as the algorithm progresses. Further, one frequently tackles with an emergence of a massive number of local minima in the space of hyperparameters, and many optimization methods may be applied here, way more sophisticated than our simplest steepest\sphinxhyphen{}descent method. Moreover, a proper choice of the neuron activation functions is crucial for success, which relates to avoiding the problem of „dead neurons”. And so on and on, many choices to be made before we start gazing in the screen in hopes that the results of our code converge …

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Taking the right decisions for the above issues is an \sphinxstylestrong{art} more than science, based on long experience of multitudes of code developers and piles of empty pizza boxes!
\end{sphinxadmonition}

\sphinxAtStartPar
Now, having completed this course and understood the basic principles behind the simplest ANNs inside out, the reader may safely jump to using professional tools of modern machine learning, with the conviction that inside the black boxes there sit essentially the same little codes he met here, but with all the knowledge, experience, tricks, provisions, and options built in. Achieving this conviction, through appreciation of simplicity, has been one of the guiding goals of this course.


\section{Acknowledgments}
\label{\detokenize{docs/conclusion:acknowledgments}}
\sphinxAtStartPar
The author thanks \sphinxhref{https://www.linkedin.com/in/janbroniowski}{Jan Broniowski} for priceless technical help and for remarks to the text.

\sphinxAtStartPar



\chapter{Dodatki}
\label{\detokenize{docs/appendix:dodatki}}\label{\detokenize{docs/appendix::doc}}

\section{Jak uruchamiać kody książki}
\label{\detokenize{docs/appendix:jak-uruchamiac-kody-ksiazki}}\label{\detokenize{docs/appendix:app-run}}

\subsection{Lokalnie}
\label{\detokenize{docs/appendix:lokalnie}}
\sphinxAtStartPar
Czytelnik może pobrać notebooki \sphinxhref{https://jupyter.org}{Jupytera} dla każdego rozdziału, klikając ikonę pobierania (strzałka w dół) po prawej stronie na górnym pasku podczas przeglądania książki.



\sphinxAtStartPar
Po zainstalowaniu Pythona i \sphinxhref{https://jupyter.org}{Jupytera} (najlepiej przez \sphinxhref{https://docs.conda.io/projects/conda/en/latest/user-guide/install}{conda}), Czytelnik może postępować zgodnie z instrukcjami dla danego systemu operacyjnego, aby otworzyć Jupyter i uruchomić w nim notebooki wykładu.


\subsection{Google Colab lub Binder}
\label{\detokenize{docs/appendix:google-colab-lub-binder}}
\sphinxAtStartPar
W danym rozdziale poniżej Wstępu należy kliknąć symbol rakiety w górnym prawym rogu ekranu, co uruchamia możliwość wykonywania (edycji, zabawy) programu w chmurze. Jest to podstawowa funkcjonalnośc wykonywalnej książki Jupyter Book.


\section{Pakiet \sphinxstylestrong{neural}}
\label{\detokenize{docs/appendix:pakiet-neural}}\label{\detokenize{docs/appendix:app-lab}}
\sphinxAtStartPar
Structura pakietu biblioteki jest następująca:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
lib\PYGZus{}nn
└── neural
    ├── \PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}.py
    ├── draw.py
    └── func.py
\end{sphinxVerbatim}

\sphinxAtStartPar
i składa się z dwóch modułów: \sphinxstylestrong{\sphinxhref{http://func.py}{func.py}} and \sphinxstylestrong{\sphinxhref{http://draw.py}{draw.py}}.


\subsection{Moduł \sphinxstylestrong{func.py}}
\label{\detokenize{docs/appendix:modul-func-py}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Contains functions used in the lecture}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}


\PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    step function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: 1 if s\PYGZgt{}0, 0 otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
   
   
\PYG{k}{def} \PYG{n+nf}{neuron}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    MCP neuron}

\PYG{l+s+sd}{    x: array of inputs  [x1, x2,...,xn]}
\PYG{l+s+sd}{    w: array of weights [w0, w1, w2,...,wn]}
\PYG{l+s+sd}{    f: activation function, with step as default}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: signal=f(w0 + x1 w1 + x2 w2 +...+ xn wn) = f(x.w)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{f}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)}
 
 
\PYG{k}{def} \PYG{n+nf}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{T}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    sigmoid}
\PYG{l+s+sd}{     }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    T: temperature}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: sigmoid(s)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{o}{/}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}
    
    
\PYG{k}{def} \PYG{n+nf}{dsig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{T}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of sigmoid}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    T: temperature}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: dsigmoid(s,T)/ds}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{T}
    
    
\PYG{k}{def} \PYG{n+nf}{lin}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    linear function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: constant}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: a*s}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{a}\PYG{o}{*}\PYG{n}{s}
  
  
\PYG{k}{def} \PYG{n+nf}{dlin}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of linear function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: constant}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: a}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{a}


\PYG{k}{def} \PYG{n+nf}{relu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    ReLU function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: s if s\PYGZgt{}0, 0 otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{s}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}


\PYG{k}{def} \PYG{n+nf}{drelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of ReLU function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: 1 if s\PYGZgt{}0, 0 otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
 
 
\PYG{k}{def} \PYG{n+nf}{lrelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Leaky ReLU function}
\PYG{l+s+sd}{  }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: parameter}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: s if s\PYGZgt{}0, a*s otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{s}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{a}\PYG{o}{*}\PYG{n}{s}


\PYG{k}{def} \PYG{n+nf}{dlrelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of Leaky ReLU function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: parameter}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: 1 if s\PYGZgt{}0, a otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{a}


\PYG{k}{def} \PYG{n+nf}{softplus}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    softplus function}

\PYG{l+s+sd}{    s: signal}

\PYG{l+s+sd}{    return: log(1+exp(s))}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{dsoftplus}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of softplus function}
\PYG{l+s+sd}{ }
\PYG{l+s+sd}{    s: signal}

\PYG{l+s+sd}{    return: 1/(1+exp(\PYGZhy{}s))}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}

    
\PYG{k}{def} \PYG{n+nf}{l2}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}for separating line\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{\PYGZhy{}}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{]}


\PYG{k}{def} \PYG{n+nf}{eucl}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Square of the Euclidean distance between two points in 2\PYGZhy{}dim. space}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: p1, p2 \PYGZhy{} arrays in the format [x1,x2]}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: square of the Euclidean distance}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}


\PYG{k}{def} \PYG{n+nf}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    return: random number from [\PYGZhy{}0.5,0.5]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}
 
 
\PYG{k}{def} \PYG{n+nf}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    return: array [x,y] with random point from a cirle}
\PYG{l+s+sd}{            centered at [0.5,0.5] and radius 0.4}
\PYG{l+s+sd}{            (used for examples)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{y}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{:}
            \PYG{k}{break}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}\PYG{p}{)}
 
 
\PYG{k}{def} \PYG{n+nf}{point}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    return: array [x,y] with random point from [0,1]x[0,1]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Set network weights randomly}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s \PYGZhy{} scale factor: each weight is in the range [\PYGZhy{}0.s, 0.5s]}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    w \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[}\PYG{n}{s}\PYG{o}{*}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{w}


\PYG{k}{def} \PYG{n+nf}{set\PYGZus{}val\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Set network weights to a constant value}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    a \PYGZhy{} value for each weight}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    w \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[}\PYG{n}{a} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{w}
    

\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{ff}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    ff \PYGZhy{} activation function (default: step)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over layers till before last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication}
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

    \PYG{c+c1}{\PYGZsh{} the last layer \PYGZhy{} no adding of the bias node}
    \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}


\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{df}\PYG{o}{=}\PYG{n}{dsig}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    back propagation algorithm}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights \PYGZhy{} UPDATED}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed}
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivative of f}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
 
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer}
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                    \PYG{n}{df}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{ff}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{ffo}\PYG{o}{=}\PYG{n}{lin}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation with different output activation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    f  \PYGZhy{} activation function (default: sigmoid)}
\PYG{l+s+sd}{    fo \PYGZhy{} activation function in the output layer (default: linear)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over layers till before last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication}
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

    \PYG{c+c1}{\PYGZsh{} the last layer \PYGZhy{} no adding of the bias node}
    \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ffo}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output activation function}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}                    \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}


\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{df}\PYG{o}{=}\PYG{n}{dsig}\PYG{p}{,} \PYG{n}{fo}\PYG{o}{=}\PYG{n}{lin}\PYG{p}{,} \PYG{n}{dfo}\PYG{o}{=}\PYG{n}{dlin}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    backprop with different output activation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights \PYGZhy{} UPDATED}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed}
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivative of f}
\PYG{l+s+sd}{    fo  \PYGZhy{} activation function in the output layer (default: linear)}
\PYG{l+s+sd}{    dfo \PYGZhy{} derivative of fo}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer}
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{,}\PYG{n}{ffo}\PYG{o}{=}\PYG{n}{fo}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                   \PYG{n}{dfo}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
    
\end{sphinxVerbatim}


\subsection{Moduł \sphinxstylestrong{draw.py}}
\label{\detokenize{docs/appendix:modul-draw-py}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Plotting functions used in the lecture.}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}


\PYG{k}{def} \PYG{n+nf}{plot}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{activation function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x\PYGZus{}label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y\PYGZus{}label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{start}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{stop}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{samples}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Wrapper on matplotlib.pyplot library.}
\PYG{l+s+sd}{    Plots functions passed as *args.}
\PYG{l+s+sd}{    Functions need to accept a single number argument and return a single number.}
\PYG{l+s+sd}{    Example usage:  plot(func.step,func.sig)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{stop}\PYG{p}{,} \PYG{n}{samples}\PYG{p}{)}

    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.8}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{n}{x\PYGZus{}label}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{n}{y\PYGZus{}label}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{fun} \PYG{o+ow}{in} \PYG{n}{args}\PYG{p}{:}
        \PYG{n}{data\PYGZus{}to\PYGZus{}plot} \PYG{o}{=} \PYG{p}{[}\PYG{n}{fun}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{data\PYGZus{}to\PYGZus{}plot}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}simp}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture without bias nodes}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: array of numbers of nodes in subsequent layers [n0, n1, n2,...]}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l\PYGZus{}layer}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l\PYGZus{}layer}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            
\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l\PYGZus{}layer}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw network with bias nodes}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{wid}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture with weights}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    wid \PYGZhy{} controls the width of the lines}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    
\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{th}\PYG{o}{=}\PYG{n}{wid}\PYG{o}{*}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
                \PYG{k}{if} \PYG{n}{th}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{n}{th}\PYG{o}{=}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{th}\PYG{p}{)}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{,}\PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{th}\PYG{p}{)}
 
\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{wid}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture with weights and signals}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    wid \PYGZhy{} controls the width of the lines}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x \PYGZhy{} dictionary the the signal in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    
\PYG{c+c1}{\PYGZsh{} input layer}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.27}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} intermediate layer}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} output layer}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
        \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{th}\PYG{o}{=}\PYG{n}{wid}\PYG{o}{*}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
                \PYG{k}{if} \PYG{n}{th}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{n}{th}\PYG{o}{=}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{th}\PYG{p}{)}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{,}\PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{th}\PYG{p}{)}
 
\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}
    
    
\PYG{k}{def} \PYG{n+nf}{l2}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}for separating line\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{\PYGZhy{}}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{]}

\end{sphinxVerbatim}


\section{Jak cytować}
\label{\detokenize{docs/appendix:jak-cytowac}}
\sphinxAtStartPar
Jeśli chcesz zacytować tę książkę Jupyter Book, oto wpis w formacie BibTeX do wersji angielskiej:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@book}\PYG{p}{\PYGZob{}}\PYG{n}{WB2021}\PYG{p}{,}
  \PYG{n}{title}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Explaining neural networks in raw Python: lectures in Jupiter}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{author}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Wojciech} \PYG{n}{Broniowski}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{isbn}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{978}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{83}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{962099}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{year}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{2021}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{url}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{ifj}\PYG{o}{.}\PYG{n}{edu}\PYG{o}{.}\PYG{n}{pl}\PYG{o}{/}\PYG{n}{strony}\PYG{o}{/}\PYG{o}{\PYGZti{}}\PYG{n}{broniows}\PYG{o}{/}\PYG{n}{nn}\PYG{p}{\PYGZcb{}}
  \PYG{n}{publisher}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Wojciech} \PYG{n}{Broniowski}\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\begin{sphinxthebibliography}{MullerRS}
\bibitem[Bar16]{docs/conclusion:id5}
\sphinxAtStartPar
P. Barry. \sphinxstyleemphasis{Head First Python: A Brain\sphinxhyphen{}Friendly Guide}. O'Reilly Media, 2016. ISBN 9781491919491. URL: \sphinxurl{https://books.google.pl/books?id=NIqNDQAAQBAJ}.
\bibitem[BH69]{docs/conclusion:id12}
\sphinxAtStartPar
A. E. Bryson and Y.\sphinxhyphen{}C. Ho. \sphinxstyleemphasis{Applied optimal control: Optimization, estimation, and control}. Waltham, Mass: Blaisdell Pub. Co., 1969.
\bibitem[FR13]{docs/conclusion:id8}
\sphinxAtStartPar
J. Feldman and R. Rojas. \sphinxstyleemphasis{Neural Networks: A Systematic Introduction}. Springer Berlin Heidelberg, 2013. ISBN 9783642610684.
\bibitem[Fre93]{docs/conclusion:id10}
\sphinxAtStartPar
James A. Freeman. \sphinxstyleemphasis{Simulating Neural Networks with Mathematica}. Addison\sphinxhyphen{}Wesley Professional, 1993. ISBN 9780201566291.
\bibitem[FS91]{docs/conclusion:id11}
\sphinxAtStartPar
James A. Freeman and David M. Skapura. \sphinxstyleemphasis{Neural Networks: Algorithms, Applications, and Programming Techniques (Computation and Neural Systems Series)}. Addison\sphinxhyphen{}Wesley, 1991. ISBN 9780201513769.
\bibitem[Gut16]{docs/conclusion:id3}
\sphinxAtStartPar
John Guttag. \sphinxstyleemphasis{Introduction to computation and programming using Python: With application to understanding data}. MIT Press, 2016.
\bibitem[HR72]{docs/conclusion:id14}
\sphinxAtStartPar
K. K. Hartline and F. Ratcliff. Inhibitory interactions in the retina of limulus. \sphinxstyleemphasis{Handbook of sensory physiology}, VII(2):381–447, 1972.
\bibitem[KSJ+12]{docs/conclusion:id6}
\sphinxAtStartPar
E.R. Kandel, J.H. Schwartz, T.M. Jessell, S.A. Siegelbaum, and A.J. Hudspeth. \sphinxstyleemphasis{Principles of Neural Science, Fifth Edition}. McGraw\sphinxhyphen{}Hill Education, 2012. ISBN 9780071810012. URL: \sphinxurl{https://books.google.pl/books?id=Z2yVUTnlIQsC}.
\bibitem[Mat19]{docs/conclusion:id2}
\sphinxAtStartPar
E. Matthes. \sphinxstyleemphasis{Python Crash Course, 2nd Edition: A Hands\sphinxhyphen{}On, Project\sphinxhyphen{}Based Introduction to Programming}. No Starch Press, 2019. ISBN 9781593279295. URL: \sphinxurl{https://books.google.pl/books?id=boBxDwAAQBAJ}.
\bibitem[MP43]{docs/conclusion:id9}
\sphinxAtStartPar
Warren S. McCulloch and Walter Pitts. The logical calculus of the ideas immanent in nervous activity. \sphinxstyleemphasis{The bulletin of mathematical biophysics}, 5(4):115–133, 1943. URL: \sphinxurl{https://doi.org/10.1007/BF02478259}.
\bibitem[MullerRS12]{docs/conclusion:id7}
\sphinxAtStartPar
B. Müller, J. Reinhardt, and M.T. Strickland. \sphinxstyleemphasis{Neural Networks: An Introduction}. Physics of Neural Networks. Springer Berlin Heidelberg, 2012. ISBN 9783642577604. URL: \sphinxurl{https://books.google.pl/books?id=on0QBwAAQBAJ}.
\bibitem[RIV91]{docs/conclusion:id13}
\sphinxAtStartPar
A. K. Rigler, J. M. Irvine, and Thomas P. Vogl. Rescaling of variables in back propagation learning. \sphinxstyleemphasis{Neural Networks}, 4(2):225–229, 1991. URL: \sphinxurl{https://doi.org/10.1016/0893-6080(91)90006-Q}, \sphinxhref{https://doi.org/10.1016/0893-6080(91)90006-Q}{doi:10.1016/0893\sphinxhyphen{}6080(91)90006\sphinxhyphen{}Q}.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Indeks}
\printindex
\end{document}