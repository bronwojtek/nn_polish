
<!DOCTYPE html>

<html lang="pl">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Propagacja wsteczna &#8212; Sieci neuronowe dla początkujących w Pythonie: wykłady w Jupyter Book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/koh.png"/>
    <link rel="index" title="Indeks" href="../genindex.html" />
    <link rel="search" title="Szukaj" href="../search.html" />
    <link rel="next" title="Interpolation" href="interpol.html" />
    <link rel="prev" title="Więcej warstw" href="more_layers.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="pl">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Sieci neuronowe dla początkujących w Pythonie: wykłady w Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Przeszukaj tę książkę ..." aria-label="Przeszukaj tę książkę ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Wstęp
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mcp.html">
   Neuron MCP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Modele pamięci
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="more_layers.html">
   Więcej warstw
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Propagacja wsteczna
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpol.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="som.html">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Concluding remarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Dodatki
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Przełącz nawigację" aria-controls="site-navigation"
                title="Przełącz nawigację" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Pobierz tę stronę"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/backprop.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Pobierz plik źródłowy" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Drukuj do PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bronwojtek/nn_polish/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Repozytorium źródłowe"><i
                    class="fab fa-github"></i>magazyn</button></a>
        <a class="issues-button"
            href="https://github.com/bronwojtek/nn_polish//issues/new?title=Issue%20on%20page%20%2Fdocs/backprop.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Otwórz problem"><i class="fas fa-lightbulb"></i>otwarty problem</button></a>
        <a class="edit-button" href="https://github.com/bronwojtek/nn_polish/edit/master/nn_polish/docs/backprop.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edytuj tę strone"><i class="fas fa-pencil-alt"></i>zaproponuj edycję</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Pełny ekran"
        title="Pełny ekran"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bronwojtek/nn_polish/master?urlpath=tree/nn_polish/docs/backprop.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Uruchomić Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/bronwojtek/nn_polish/blob/master/nn_polish/docs/backprop.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Uruchomić Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Zawartość
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#minimalizacja-bledu">
   Minimalizacja błędu
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ciagla-funkcja-aktywacji">
   Ciągła funkcja aktywacji
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#najstromszy-spadek">
   Najstromszy spadek
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algorytm-propagacji-wstecznej-backprop">
   Algorytm propagacji wstecznej (backprop)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kod-dla-algorytmu-backprop">
     Kod dla algorytmu backprop
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#przyklad-z-kolem">
   Przykład z kołem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ogolne-uwagi">
   Ogólne uwagi
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cwiczenia">
   Ćwiczenia
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Propagacja wsteczna</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Zawartość </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#minimalizacja-bledu">
   Minimalizacja błędu
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ciagla-funkcja-aktywacji">
   Ciągła funkcja aktywacji
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#najstromszy-spadek">
   Najstromszy spadek
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algorytm-propagacji-wstecznej-backprop">
   Algorytm propagacji wstecznej (backprop)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kod-dla-algorytmu-backprop">
     Kod dla algorytmu backprop
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#przyklad-z-kolem">
   Przykład z kołem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ogolne-uwagi">
   Ogólne uwagi
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cwiczenia">
   Ćwiczenia
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="propagacja-wsteczna">
<h1>Propagacja wsteczna<a class="headerlink" href="#propagacja-wsteczna" title="Stały odnośnik do tego nagłówka">¶</a></h1>
<p>W tym rozdziale pokażemy szczegółowo, jak przeprowadzić uczenie nadzorowane dla klasyfikatorów wielowarstwowych omówionych w rozdziale <a class="reference internal" href="more_layers.html#more-lab"><span class="std std-ref">Więcej warstw</span></a>. Ponieważ metoda opiera się na minimalizacji liczby błędnych odpowiedzi na próbce testowej, zaczynamy od dokładnego omówienia problemu minimalizacji błędów w naszej konfiguracji.</p>
<div class="section" id="minimalizacja-bledu">
<h2>Minimalizacja błędu<a class="headerlink" href="#minimalizacja-bledu" title="Stały odnośnik do tego nagłówka">¶</a></h2>
<p>Przypomnijmy, że w naszym przykładzie z punktami na płaszczyźnie z rozdziału <a class="reference internal" href="perceptron.html#perc-lab"><span class="std std-ref">Perceptron</span></a> warunek dla różowych punktów był zadany przez nierówność</p>
<p><span class="math notranslate nohighlight">\(w_0+w_1 x_1 + w_2 x_2 &gt; 0\)</span>.</p>
<p>Wspomnieliśmy już pokrótce o klasie równoważności związanej z dzieleniem obu stron tej nierówności przez dodatnią stałą <span class="math notranslate nohighlight">\(c\)</span>. Ogólnie rzecz biorąc, co najmniej jedna z wag w powyższym warunku musi być niezerowa, aby był on nietrywialny. Załóżmy zatem, że <span class="math notranslate nohighlight">\(w_0 \neq 0\)</span> (inne przypadki można potraktować analogicznie). Następnie podzielmy obie strony nierówności przez <span class="math notranslate nohighlight">\(|w_0|\)</span>, co daje</p>
<div class="math notranslate nohighlight">
\[\frac{w_0}{|w_0|}+\frac{w_1}{|w_0|} \, x_1 + \frac{w_2}{|w_0|} \, x_2 &gt; 0. \]</div>
<p>Wprowadzając notację <span class="math notranslate nohighlight">\(v_1=\frac{w_1}{w_0}\)</span> and <span class="math notranslate nohighlight">\(v_2=\frac{w_2}{w_0}\)</span>, możemy zatem zapisać</p>
<div class="math notranslate nohighlight">
\[{\rm sgn}(w_0)( 1+v_1 \, x_1 +v_2 \, x_2) &gt; 0,\]</div>
<p>gdzie znak <span class="math notranslate nohighlight">\({\rm sgn}(w_0) = \frac{w_0}{|w_0|}\)</span>. Mamy więc w efekcie system dwuparametrowy (dla ustalonego znaku <span class="math notranslate nohighlight">\(w_0\)</span>).</p>
<p>Oczywiście przy pewnych wartościach <span class="math notranslate nohighlight">\( v_1 \)</span> i <span class="math notranslate nohighlight">\( v_2 \)</span> i dla danego punktu z próbki danych, perceptron poda w wyniku poprawną lub błędną odpowiedź. Naturalne jest zatem zdefiniowanie <strong>funkcji błędu</strong> <span class="math notranslate nohighlight">\(E\)</span> w taki sposób, że dla każdego punktu <span class="math notranslate nohighlight">\(p\)</span> próbki wnosi 1, jeśli odpowiedź jest niepoprawna, a 0, jeśli jest poprawna:</p>
<div class="math notranslate nohighlight">
\[\begin{split} E(v_1,v_2)=\sum_p \left\{ \begin{array}{ll} 1 -{\rm niepoprawna,~}\\ 0 -{\rm poprawna.} \end{array}\right .\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(E\)</span> ma zatem interpretację liczby źle sklasyfikowanych punktów.</p>
<p>Możemy łatwo skonstruować tę funkcję w Pythonie:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">error</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">w1</span> <span class="p">,</span><span class="n">w2</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    error function for the perceptron (for 2-dim data with labels)</span>
<span class="sd">    </span>
<span class="sd">    inputs:</span>
<span class="sd">    w0, w1, w2 - weights</span>
<span class="sd">    sample - array of labeled data points p </span>
<span class="sd">             p in an array in the format [x1, x1, label]</span>
<span class="sd">    f - activation function</span>
<span class="sd">    </span>
<span class="sd">    returns:</span>
<span class="sd">    error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">er</span><span class="o">=</span><span class="mi">0</span>                                       <span class="c1"># initial value of the error</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)):</span>               <span class="c1"># loop over data points       </span>
        <span class="n">yo</span><span class="o">=</span><span class="n">f</span><span class="p">(</span><span class="n">w0</span><span class="o">+</span><span class="n">w1</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">w2</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># obtained answer</span>
        <span class="n">er</span><span class="o">+=</span><span class="p">(</span><span class="n">yo</span><span class="o">-</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
                      <span class="c1"># sample[i,2] is the label</span>
                      <span class="c1"># adds the square of the difference of yo and the label</span>
                      <span class="c1"># this adds 1 if the answer is incorrect, and 0 if correct</span>
    <span class="k">return</span> <span class="n">er</span>  <span class="c1"># the error</span>
</pre></div>
</div>
</div>
</div>
<p>Zastosowaliśmy tutaj małą sztuczkę, mając na uwadze przyszłe zastosowania. Oznaczając otrzymany wynik dla danego punktu danych jako <span class="math notranslate nohighlight">\(y_o^{(p)}\)</span>, a wynik prawdziwy (etykietę) jako <span class="math notranslate nohighlight">\(y_t^{(p)}\)</span> (obydwa przyjmują wartości 0 lub 1), możemy zdefiniowane powyżej <span class="math notranslate nohighlight">\(E\)</span> zapisać równoważnie jako</p>
<div class="math notranslate nohighlight">
\[ E(v_1,v_2)=\sum_p \left ( y_o^{(p)}-y_t^{(p)}\right )^2,\]</div>
<p>co jest wzorem zaprogramowanym w kodzie. Rzeczywiście, kiedy <span class="math notranslate nohighlight">\(y_o^{(p)}=y_t^{(p)}\)</span> (prawidłowa odpowiedź), wkład punktu wynosi 0, a kiedy <span class="math notranslate nohighlight">\(y_o^{(p)}\neq y_t^{(p) }\)</span> (błędna odpowiedź), wkład wynosi <span class="math notranslate nohighlight">\((\pm 1)^2=1\)</span>.</p>
<p>Powtarzamy teraz symulacje z podrozdziału <a class="reference internal" href="perceptron.html#perc-lab"><span class="std std-ref">Perceptron</span></a>, aby wygenerować etykietowaną próbkę danych <strong>samp2</strong> o 200 punktach (próbka jest utworzona z <span class="math notranslate nohighlight">\(w_0=-0.25\)</span>, <span class="math notranslate nohighlight">\(w_1=-0.52\)</span> i <span class="math notranslate nohighlight">\(w_2=1\)</span>, co odpowiada <span class="math notranslate nohighlight">\(v_1=2.08\)</span> i <span class="math notranslate nohighlight">\(v_2=-4\)</span>, przy czym <span class="math notranslate nohighlight">\({\rm sgn}(w_0)=-1\)</span>).</p>
<p>Potrzebujemy teraz ponownie użyć algorytmu perceptronu z rozdz. <a class="reference internal" href="perceptron.html#lab-pa"><span class="std std-ref">Algorytm perceptronu</span></a>. W naszym szczególnym przypadku działa on na próbce dwuwymiarowych danych etykietowanych. Dla wygody, pojedyncza runda algorytmu może zostać zebrana w funkcję w następujący sposób:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">teach_perceptron</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Supervised learning for a single perceptron (single MCP neuron) </span>
<span class="sd">    for a sample of 2-dim. labeled data</span>
<span class="sd">       </span>
<span class="sd">    input:</span>
<span class="sd">    sample - array of two-dimensional labeled data points p</span>
<span class="sd">             p is an array in the format [x1,x2,label]</span>
<span class="sd">             label = 0 or 1</span>
<span class="sd">    eps    - learning speed</span>
<span class="sd">    w_in   - initial weights in the format [[w0], [w1], [w2]]</span>
<span class="sd">    f      - activation function</span>
<span class="sd">    </span>
<span class="sd">    return: updated weights in the format [[w0], [w1], [w2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="p">[[</span><span class="n">w0</span><span class="p">],[</span><span class="n">w1</span><span class="p">],[</span><span class="n">w2</span><span class="p">]]</span><span class="o">=</span><span class="n">w_in</span>         <span class="c1"># define w0, w1, and w2</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)):</span>  <span class="c1"># loop over the whole sample</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>       <span class="c1"># repeat 10 times  </span>
            
            <span class="n">yo</span><span class="o">=</span><span class="n">f</span><span class="p">(</span><span class="n">w0</span><span class="o">+</span><span class="n">w1</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">w2</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># output from the neuron, f(x.w)</span>
            
            <span class="c1"># update of weights according to the perceptron algorithm formula</span>
            <span class="n">w0</span><span class="o">=</span><span class="n">w0</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span>
            <span class="n">w1</span><span class="o">=</span><span class="n">w1</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">w2</span><span class="o">=</span><span class="n">w2</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
            
    <span class="k">return</span> <span class="p">[[</span><span class="n">w0</span><span class="p">],[</span><span class="n">w1</span><span class="p">],[</span><span class="n">w2</span><span class="p">]]</span>       <span class="c1"># updated weights</span>
</pre></div>
</div>
</div>
</div>
<p>Następnie prześledzimy działanie algorytmu perceptronu, obserwując jak modyfikuje on wartości wprowadzonej powyżej funkcji błędu <span class="math notranslate nohighlight">\(E(v_1,v_2)\)</span>. Zaczynamy od losowych wag, a następnie wykonujemy 10 rund zdefiniowanej powyżej funkcji <strong>teach_perceptron</strong>, wypisując zaktualizowane wagi i odpowiadający im błąd:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="o">=</span><span class="p">[[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()],</span> <span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()],</span> <span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()]]</span> <span class="c1"># initial random weights</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimum:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   w0  w1/w0  w2/w0 error&quot;</span><span class="p">)</span>   <span class="c1"># header</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">0.7</span>                 <span class="c1"># initial learning speed</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>     <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.8</span><span class="o">*</span><span class="n">eps</span>         <span class="c1"># decrease the learning speed</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">teach_perceptron</span><span class="p">(</span><span class="n">samp2</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">)</span> 
                        <span class="c1"># see the top of this chapter</span>
        
    <span class="n">w0_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># updated weights and ratios</span>
    <span class="n">v1_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">v2_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">v1_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">v2_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
          <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">error</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span> <span class="n">w0_o</span><span class="o">*</span><span class="n">v1_o</span><span class="p">,</span> <span class="n">w0_o</span><span class="o">*</span><span class="n">v2_o</span><span class="p">,</span> <span class="n">samp2</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">),</span><span class="mi">0</span><span class="p">))</span>             
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimum:
   w0  w1/w0  w2/w0 error
-0.17 7.311 -13.855 40.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
-0.618 2.073 -3.999 0.0
</pre></div>
</div>
</div>
</div>
<p>Zauważamy, że w kolejnych rundach błąd stopniowo maleje (w zależności od symulacji, może czasem nieco podskoczyć, jeśli szybkość uczenia się jest zbyt duża, ale nie stanowi to problemu, o ile koniec końców możemy zejść do minimum), osiągając ostatecznie wartość bardzo małą lub dokładnie 0 (w zależności od konkretnego przypadku symulacji). W związku z tym
algorytm perceptronu, jak już widzieliśmy w rozdziale <a class="reference internal" href="perceptron.html#perc-lab"><span class="std std-ref">Perceptron</span></a>, <strong>minimalizuje błąd dla próbki treningowej</strong>.</p>
<p>Pouczające jest spojrzenie na mapę konturową funkcji błędu <span class="math notranslate nohighlight">\(E(v_1, v_2)\)</span> w pobliżu optymalnych parametrów:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">3.7</span><span class="p">,</span><span class="mf">3.7</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="n">delta</span> <span class="o">=</span> <span class="mf">0.02</span>  <span class="c1"># grid step in v1 and v2 for the contour map</span>
<span class="n">ran</span><span class="o">=</span><span class="mf">0.8</span>       <span class="c1"># plot range around (v1_o, v2_o)</span>

<span class="n">v1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">v1_o</span><span class="o">-</span><span class="n">ran</span><span class="p">,</span><span class="n">v1_o</span><span class="o">+</span><span class="n">ran</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> <span class="c1"># grid for v1</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">v2_o</span><span class="o">-</span><span class="n">ran</span><span class="p">,</span><span class="n">v2_o</span><span class="o">+</span><span class="n">ran</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> <span class="c1"># grid for v2</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>               <span class="c1"># mesh for the contour plot</span>

<span class="n">Z</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">error</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="n">v1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="o">-</span><span class="n">v2</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">samp2</span><span class="p">,</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">)</span> 
             <span class="c1"># we use the scaling property of the error function here </span>
             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v1</span><span class="p">))]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v2</span><span class="p">))])</span> <span class="c1"># values of E(v1,v2) </span>

<span class="n">CS</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">50</span><span class="p">])</span>
                        <span class="c1"># explicit contour level values</span>
    
<span class="n">ax</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">CS</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%1.0f</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span> <span class="c1"># contour label format</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Error function&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$v_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$v_2$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">v1_o</span><span class="p">,</span> <span class="n">v2_o</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;found minimum&#39;</span><span class="p">)</span> <span class="c1"># our found optimal point</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/backprop_16_0.png" src="../_images/backprop_16_0.png" />
</div>
</div>
<p>Uzyskane minimum znajduje się wewnątrz (lub blisko, w zależności od symulacji) wydłużonego obszaru w <span class="math notranslate nohighlight">\(v_1\)</span> i <span class="math notranslate nohighlight">\(v_2\)</span>, gdzie błąd znika.</p>
</div>
<div class="section" id="ciagla-funkcja-aktywacji">
<h2>Ciągła funkcja aktywacji<a class="headerlink" href="#ciagla-funkcja-aktywacji" title="Stały odnośnik do tego nagłówka">¶</a></h2>
<p>Przyglądając się uważniej powyższej mapie konturowej, widzimy, że linie są „ząbkowane”. Dzieje się tak, ponieważ funkcja błędu, z oczywistego powodu, przyjmuje wartości całkowite. Jest zatem nieciągła, a zatem nieróżniczkowalna. Nieciągłości wynikają z nieciągłej funkcji aktywacji, mianowicie funkcji schodkowej. Mając na uwadze techniki, które poznamy niebawem, korzystne jest stosowanie funkcji aktywacji, która jest różniczkowalna. Historycznie tzw. <strong>sigmoid</strong></p>
<div class="math notranslate nohighlight">
\[ \sigma(s)=\frac{1}{1+e^{-s}}\]</div>
<p>był wykorzystywany w wielu praktycznych zastosowaniach dla ANN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sigmoid, a.k.a. the logistic function, or simply (1+arctanh(-s/2))/2 </span>
<span class="k">def</span> <span class="nf">sig</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/backprop_21_0.png" src="../_images/backprop_21_0.png" />
</div>
</div>
<p>Funkcja ta jest oczywiście różniczkowalna. Ponadto</p>
<div class="math notranslate nohighlight">
\[ \sigma '(s) = \sigma (s) [1- \sigma (s)], \]</div>
<p>co jest szczególna własnością sigmoidu.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># derivative of sigmoid</span>
<span class="k">def</span> <span class="nf">dsig</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
     <span class="k">return</span> <span class="n">sig</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sig</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/backprop_24_0.png" src="../_images/backprop_24_0.png" />
</div>
</div>
<p>Wprowadza się również sigmoid z „temperaturą” <span class="math notranslate nohighlight">\(T \)</span> (nomenklatura ta jest związana z podobnymi wyrażeniami dla funkcji termodynamicznych w fizyce):
$<span class="math notranslate nohighlight">\(\sigma(s;T)=\frac{1}{1+e^{-s/T}}.\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sigmoid with temperature T</span>
<span class="k">def</span> <span class="nf">sig_T</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">T</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span><span class="o">/</span><span class="n">T</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/backprop_27_0.png" src="../_images/backprop_27_0.png" />
</div>
</div>
<p>Dla coraz mniejszych <span class="math notranslate nohighlight">\(T\)</span> sigmoid zbliża się do poprzednio używanej funkcji schodkowej.</p>
<p>Zauważ, że argumentem sigmoidu jest iloraz</p>
<div class="math notranslate nohighlight">
\[
s/T = (w_0 + w_1 x_1 + w_2 x_2) / T = w_0 / T + w_1 / T \, x_1 + w_2 / T \, x_2 = \xi_0 + \xi_1 x_1 + \xi_2 x_2,
\]</div>
<p>co oznacza, że zawsze możemy przyjąć <span class="math notranslate nohighlight">\(T = 1\)</span> bez utraty ogólności (<span class="math notranslate nohighlight">\(T \)</span> to „skala”). Jednak teraz mamy trzy niezależne argumenty <span class="math notranslate nohighlight">\( \xi_0 \)</span>, <span class="math notranslate nohighlight">\( \xi_1 \)</span> i <span class="math notranslate nohighlight">\( \xi_2\)</span>, więc nie można zredukować obecnej sytuacji do tylko dwóch niezależnych parametrów, jak miało to miejsce w poprzednim podrozdziale.</p>
<p>Powtórzymy teraz nasz przykład z klasyfikatorem, ale z funkcją aktywacji daną przez sigmoid. Funkcja błędu</p>
<div class="math notranslate nohighlight">
\[y_o^{(p)}=\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)}), \]</div>
<p>staje się teraz</p>
<div class="math notranslate nohighlight">
\[E(w_0,w_1,w_2)=\sum_p \left [\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)} \right] ^2.\]</div>
<p>Algorytm perceptronu z funkcją aktywacji sigmoidu wykonujemy 1000 razy, wypisując co 100 krok:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="o">=</span><span class="p">[[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()],[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()],[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()]]</span>      <span class="c1"># random weights from [-0.5,0.5]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   w0   w1/w0  w2/w0 error&quot;</span><span class="p">)</span>   <span class="c1"># header</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">0.7</span>                       <span class="c1"># initial learning speed</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>         <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.9995</span><span class="o">*</span><span class="n">eps</span>            <span class="c1"># decrease learning speed</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">teach_perceptron</span><span class="p">(</span><span class="n">samp2</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">)</span> <span class="c1"># update weights</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">%</span><span class="k">100</span>==99:
        <span class="n">w0_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>               <span class="c1"># updated weights </span>
        <span class="n">w1_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> 
        <span class="n">w2_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> 
        <span class="n">v1_o</span><span class="o">=</span><span class="n">w1_o</span><span class="o">/</span><span class="n">w0_o</span>                   <span class="c1"># ratios of weights</span>
        <span class="n">v2_o</span><span class="o">=</span><span class="n">w2_o</span><span class="o">/</span><span class="n">w0_o</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">v1_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">v2_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
              <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">error</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span> <span class="n">w0_o</span><span class="o">*</span><span class="n">v1_o</span><span class="p">,</span> <span class="n">w0_o</span><span class="o">*</span><span class="n">v2_o</span><span class="p">,</span> <span class="n">samp2</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">),</span><span class="mi">5</span><span class="p">))</span>                             
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   w0   w1/w0  w2/w0 error
-20.455 2.031 -3.99 0.98049
-26.414 1.977 -3.886 0.47767
-30.326 1.954 -3.844 0.36447
-33.282 1.94 -3.82 0.32368
-35.661 1.929 -3.805 0.30012
-37.646 1.921 -3.794 0.28081
-39.341 1.915 -3.787 0.26266
-40.814 1.909 -3.78 0.24521
-42.11 1.904 -3.775 0.22862
-43.263 1.899 -3.771 0.21313
</pre></div>
</div>
</div>
</div>
<p>Obserwujemy, zgodnie z oczekiwaniami, stopniowy spadek błędu w miarę postępu symulacji. Ponieważ funkcja błędu ma teraz trzy niezależne argumenty, nie można jej narysować w dwóch wymiarach. Możemy jednak pokazać jej rzut, np. dla ustalonej wartości <span class="math notranslate nohighlight">\( w_0 \)</span>, co robimy poniżej:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">3.7</span><span class="p">,</span><span class="mf">3.7</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="n">delta</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">ran</span><span class="o">=</span><span class="mi">40</span> 
<span class="n">r1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">w1_o</span><span class="o">-</span><span class="n">ran</span><span class="p">,</span> <span class="n">w1_o</span><span class="o">+</span><span class="n">ran</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> 
<span class="n">r2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">w2_o</span><span class="o">-</span><span class="n">ran</span><span class="p">,</span> <span class="n">w2_o</span><span class="o">+</span><span class="n">ran</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> 
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">)</span> 

<span class="n">Z</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">error</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span><span class="n">r1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">r2</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">samp2</span><span class="p">,</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">)</span> 
             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r1</span><span class="p">))]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r2</span><span class="p">))])</span>  

<span class="n">CS</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">45</span><span class="p">,</span><span class="mi">50</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">CS</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%1.0f</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Error function for $w_0$=&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$w_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$w_2$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">w1_o</span><span class="p">,</span> <span class="n">w2_o</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;found minimum&#39;</span><span class="p">)</span> <span class="c1"># our found optimal point</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/backprop_32_0.png" src="../_images/backprop_32_0.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Informacja</p>
<p>W miarę jak wykonujemy coraz więcej iteracji, zauważamy, że wielkość wag rośnie, podczas gdy błąd naturalnie się zmniejsza. Powodem jest to, że nasza próbka danych jest separowalna, więc w przypadku użycia schodkowej funkcji aktywacji możliwe jest rozdzielenie próbki linią podziału i zejście z błędem aż do zera. W przypadku sigmoidu, zawsze istnieje pewien (niewielki) wkład do błędu, ponieważ wartości funkcji mieszczą się w sposób ciągły w przedziale (0,1). Jak omówiliśmy powyżej, w sigmoidzie, którego argumentem jest <span class="math notranslate nohighlight">\( (w_0 + w_1 x_1 + w_2 x_2) / T\)</span>, zwiększanie wag jest równoznaczne ze zmniejszaniem temperatury <span class="math notranslate nohighlight">\(T\)</span>. W moare postępu symulacji sigmoid zbliża się zatem do funkcji schodkowej, a błąd dąży do zera. Zachowanie to jest widoczne w powyższych symulacjach.</p>
</div>
</div>
<div class="section" id="najstromszy-spadek">
<h2>Najstromszy spadek<a class="headerlink" href="#najstromszy-spadek" title="Stały odnośnik do tego nagłówka">¶</a></h2>
<p>Powodem dla powyższych symulacji było doprowadzenie czytelnika do wniosku, że zagadnienie optymalizacji wag można sprowadzić do ogólnego problemu minimalizacji funkcji wielu zmiennych. Jest to standardowy (choć na ogół trudny) problem w analizie matematycznej i metodach numerycznych. Problemy związane ze znalezieniem minimum funkcji wielu zmiennych są dobrze znane:</p>
<ul class="simple">
<li><p>mogą istnieć minima lokalne, dlatego znalezienie minimum globalnego może być bardzo trudne;</p></li>
<li><p>minimum może być w nieskończoności (czyli matematycznie nie istnieć);</p></li>
<li><p>Funkcja wokół minimum może być bardzo płaska, tj. jej gradient jest bardzo mały. Wówczas znajdowanie minimum z pomocą metod gradientowych jest bardzo powolne;</p></li>
</ul>
<p>Ogólnie rzecz biorąc, minimalizacja numeryczna funkcji to sztuka! Opracowano tu wiele metod, a właściwy dobór do danego problemu ma kluczowe znaczenie dla sukcesu. Poniżej zastosujemy najprostszy wariant, tzw. metodę <strong>najstromszego spadku</strong>.</p>
<p>Dla różniczkowalnej funkcji wielu zmiennych <span class="math notranslate nohighlight">\( F (z_1, z_2, ..., z_n) \)</span>, lokalnie najbardziej strome nachylenie jest okreslone przez minus gradient funkcji <span class="math notranslate nohighlight">\( F \)</span>,
$<span class="math notranslate nohighlight">\(-\left (\frac{\partial F}{\partial z_1}, \frac{\partial F}{\partial z_2}, ..., 
\frac{\partial F}{\partial z_n} \right ), \)</span>$</p>
<p>gdzie pochodne cząstkowe definiuje się jako granice</p>
<div class="math notranslate nohighlight">
\[\frac{\partial F}{\partial z_1} = \lim _ {\Delta \to 0} \frac {F (z_1 + \Delta, z_2, ..., z_n) -F (z_1, z_2, ..., z_n)} { \Delta } \]</div>
<p>i podobnie dla pozostałych <span class="math notranslate nohighlight">\( z_i \)</span>.</p>
<p>Metoda znajdowania minimum funkcji poprzez najstromszy spadek zadana jest przez algorytm iteracyjny, w którym aktualizujemy współrzędne (wyszukiwanego minimum) w każdym kroku iteracji <span class="math notranslate nohighlight">\(m\)</span> (górny wskaźnik) w nastepujacy sposób:</p>
<div class="math notranslate nohighlight">
\[z_{i}^{(m+1)} = z_i^{(m)} - \epsilon  \, \frac{\partial F}{\partial z_i}. \]</div>
<p>W naszym zagadnieniu potrzebujemy zminimalzować funcję błedu</p>
<div class="math notranslate nohighlight">
\[E(w_0,w_1,w_2)= \sum_p [y_o^{(p)}-y_t^{(p)}]^2=\sum_p [\sigma(s^{(p)})-y_t^{(p)}]^2=\sum_p [\sigma(w_0  x_0^{(p)}+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)}]^2. \]</div>
<p>Aby obliczyć pochodne, stosujemy <strong>twierdzenie o pochodnej funkcji złożonej</strong>.</p>
<div class="admonition-tw-o-pochodnej-funkcji-zlozonej admonition">
<p class="admonition-title">Tw. o pochodnej funkcji złożonej</p>
<p>Dla funkcji złożonej</p>
<p><span class="math notranslate nohighlight">\([f(g(x))]' = f'(g(x)) g'(x)\)</span>.</p>
<p>Dla złożenia większej liczby funkcji <span class="math notranslate nohighlight">\([f(g(h(x)))]' = f'(g(h(x))) \,g'(h(x)) \,h'(x)\)</span> itp.</p>
</div>
<p>Prowadzi to do wzoru</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial E}{\partial w_i} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma'(s^{(p)}) \,x_i^{(p)} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma(s^{(p)})\, [1-\sigma(s^{(p)})] \,x_i^{(p)}\]</div>
<p>(pochodna funkcji kwadratowej <span class="math notranslate nohighlight">\( \times \)</span> pochodna sigmoidu <span class="math notranslate nohighlight">\( \times \)</span> pochodna <span class="math notranslate nohighlight">\( s ^ {(p)} \)</span>), gdzie w ostatniej równości użyliśmy specjalnej własności pochodnej sigmoidu. Metoda najstromszego spadku aktualizuje zatem wagi w następujący sposób:</p>
<div class="math notranslate nohighlight">
\[w_i \to w_i - \varepsilon (y_o^{(p)} -y_t^{(p)}) y_o^{(p)} (1-y_o^{(p)}) x_i.\]</div>
<p>Zauważmy, że aktualizacja zawsze występuje, ponieważ odpowiedź <span class="math notranslate nohighlight">\( y_o^ {(p)} \)</span> nigdy nie jest ściśle równa 0 lub 1, podczas gdy
prawdziwa wartość (etykieta) <span class="math notranslate nohighlight">\( y_t ^ {(p)} \)</span> wynosi 0 lub 1.</p>
<p>Ponieważ <span class="math notranslate nohighlight">\( y_o ^ {(p)} (1-y_o ^ {(p)}) = \sigma (s ^ {(p)}) [1- \sigma (s ^ {(p)})] \)</span> jest istotnie różne od zera tylko w okolicy <span class="math notranslate nohighlight">\( s ^ {(p)} = 0\)</span> (patrz wcześniejszy wykres pochodnej sigmoidu), znacząca aktualizacja następuje tylko w pobliżu progu. To cecha jest odpowiednia, ponieważ problemy z błędną klasyfikacją zdarzają się właśnie w pobliżu linii podziału.</p>
<div class="admonition note">
<p class="admonition-title">Informacja</p>
<p>Dla porównania, wcześniejszy algorytm perceptronu jest strukturalnie bardzo podobny,</p>
<div class="math notranslate nohighlight">
\[w_i \to w_i - \varepsilon \,(y_o^{(p)} - y_t^{(p)}) \, x_i,\]</div>
<p>ale tutaj aktualizacja następuje dla wszystkich punktów próbki, a nie tylko tych w pobliżu linii podziału.</p>
</div>
<p>Kod algorytmu uczenia naszego perceptronu metodą najstromszyego spadku jest następujący:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">teach_sd</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">w_in</span><span class="p">):</span> <span class="c1"># Steepest descent for the perceptron</span>
    
    <span class="p">[[</span><span class="n">w0</span><span class="p">],[</span><span class="n">w1</span><span class="p">],[</span><span class="n">w2</span><span class="p">]]</span><span class="o">=</span><span class="n">w_in</span>              <span class="c1"># initial weights</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)):</span>       <span class="c1"># loop over the data sample</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>            <span class="c1"># repeat 10 times </span>
            
            <span class="n">yo</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">(</span><span class="n">w0</span><span class="o">+</span><span class="n">w1</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">w2</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># obtained answer for pont i</span>

            <span class="n">w0</span><span class="o">=</span><span class="n">w0</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">yo</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span>            <span class="c1"># update of weights</span>
            <span class="n">w1</span><span class="o">=</span><span class="n">w1</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">yo</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">w2</span><span class="o">=</span><span class="n">w2</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">yo</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[[</span><span class="n">w0</span><span class="p">],[</span><span class="n">w1</span><span class="p">],[</span><span class="n">w2</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>Jego wydajność jest podobna do oryginalnego algorytmu perceptronu badanego powyżej:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="o">=</span><span class="p">[[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()],[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()],[</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()]]</span>      <span class="c1"># random weights from [-0.5,0.5]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   w0   w1/w0  w2/w0 error&quot;</span><span class="p">)</span>   <span class="c1"># header</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">0.7</span>                       <span class="c1"># initial learning speed</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>         <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.9995</span><span class="o">*</span><span class="n">eps</span>            <span class="c1"># decrease learning speed</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">teach_sd</span><span class="p">(</span><span class="n">samp2</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">weights</span><span class="p">)</span> <span class="c1"># update weights</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">%</span><span class="k">100</span>==99:
        <span class="n">w0_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>               <span class="c1"># updated weights </span>
        <span class="n">w1_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> 
        <span class="n">w2_o</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> 
        <span class="n">v1_o</span><span class="o">=</span><span class="n">w1_o</span><span class="o">/</span><span class="n">w0_o</span>
        <span class="n">v2_o</span><span class="o">=</span><span class="n">w2_o</span><span class="o">/</span><span class="n">w0_o</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">v1_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">v2_o</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
              <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">error</span><span class="p">(</span><span class="n">w0_o</span><span class="p">,</span> <span class="n">w0_o</span><span class="o">*</span><span class="n">v1_o</span><span class="p">,</span> <span class="n">w0_o</span><span class="o">*</span><span class="n">v2_o</span><span class="p">,</span> <span class="n">samp2</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">),</span><span class="mi">5</span><span class="p">))</span>                                          
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   w0   w1/w0  w2/w0 error
-10.24 1.982 -3.94 1.87852
-13.273 1.946 -3.859 1.28437
-15.273 1.932 -3.831 1.04334
-16.788 1.923 -3.815 0.90363
-18.011 1.916 -3.804 0.80881
-19.035 1.91 -3.796 0.73887
-19.913 1.904 -3.79 0.68469
-20.678 1.899 -3.784 0.64135
-21.352 1.895 -3.78 0.60589
-21.953 1.892 -3.776 0.57636
</pre></div>
</div>
</div>
</div>
<p>Podsumowując dotychczasowy materiał, wykazaliśmy, że można skutecznie uczyć jednowarstwy perceptron (pojedynczy neuronu MCP) za pomocą metody najstromszego spadku, minimalizując funkcję błędu generowaną przez badaną próbkę. W następnym podrozdziale uogólnimy ten pomysł na dowolny wielowarstwową sieć typu feed-forward.</p>
</div>
<div class="section" id="algorytm-propagacji-wstecznej-backprop">
<span id="bpa-lab"></span><h2>Algorytm propagacji wstecznej (backprop)<a class="headerlink" href="#algorytm-propagacji-wstecznej-backprop" title="Stały odnośnik do tego nagłówka">¶</a></h2>
<p>Materiał tego podrozdziału jest absolutnie <strong>kluczowy</strong> dla zrozumienia idei uczenia sieci neuronowych poprzez uczenie nadzorowane. Jednocześnie dla czytelnika mniej zaznajomionego z analizą matematyczną może być dość trudny, ponieważ pojawiają się wyprowadzenia i wzory z bogatą notacją. Nie udało się jednak znaleźć sposobu na przedstawienie materiału w prostszy sposób niż poniżej, z jednoczesnym zachowaniem niezbędnego rygoru.</p>
<div class="admonition note">
<p class="admonition-title">Informacja</p>
<p>Formuły, które wyprowadzamy tutaj krok po kroku, stanowią słynny <strong>algorytm wstecznej propagacji (backprop)</strong> <span id="id1">[<a class="reference internal" href="conclusion.html#id12" title="A. E. Bryson and Y.-C. Ho. Applied optimal control: Optimization, estimation, and control. Waltham, Mass: Blaisdell Pub. Co., 1969.">BH69</a>]</span> dla aktualizacji wag perceptronu wielowarstwowego. Wykorzystujemy tylko dwa podstawowe fakty:</p>
<ul class="simple">
<li><p><strong>tw. o pochodnej funkcji złożonej</strong> do obliczania pochodnej, oraz</p></li>
<li><p><strong>metodę najstromszego spadku</strong>, wyjaśnioną w poprzednim podrozdziale.</p></li>
</ul>
</div>
<p>Rozważmy perceptron z dowolną liczbą warstw neuronowych, <span class="math notranslate nohighlight">\(l\)</span>. Neurony w warstwach pośrednich <span class="math notranslate nohighlight">\(j=1,\dots,l-1\)</span> są ponumerowane odpowiednimi wskaźnikami <span class="math notranslate nohighlight">\(\alpha_j=0,\dots,n_j\)</span>, gdzie 0 oznacza węzeł progowy. W warstwie wyjściowej, nie zawierającej węzła progowego, wskaźnik przyjmuje wartości <span class="math notranslate nohighlight">\(\alpha_l=1,\dots,n_l\)</span>. Na przykład sieć z wykresu poniżej ma</p>
<div class="math notranslate nohighlight">
\[l=4, \; \; \alpha_1=0,\dots,4, \;\; \alpha_2=0,\dots,5, \;\; \alpha_3=0,\dots,3, \;\; \alpha_4=1,\dots,2,\]</div>
<p>ze wskaźnikami w każdej warstwie liczonymi od dołu.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/backprop_47_0.png" src="../_images/backprop_47_0.png" />
</div>
</div>
<p>Funkcja błędu to suma po punktach próbki treningowej oraz dodatkowo po węzłach w warstwie wyjściowej:</p>
<div class="math notranslate nohighlight">
\[
E(\{w\})=\sum_p \sum_{\alpha_l=1}^{n_l} \left[ y_{o,{\alpha_l}}^{(p)}(\{w\})-y_{t,{\alpha_l}}^{(p)}\right]^2,
\]</div>
<p>gdzie <span class="math notranslate nohighlight">\( \{w \} \)</span> reprezentują wszystkie wagi sieci.
Pojedynczy wkład punktu <span class="math notranslate nohighlight">\(p\)</span> do <span class="math notranslate nohighlight">\(E\)</span>, oznaczony jako <span class="math notranslate nohighlight">\(e\)</span>, to
suma po wszystkich neuronach w warstwie wyjściowej:</p>
<div class="math notranslate nohighlight">
\[
e(\{w\})= \sum_{{\alpha_l}=1}^{n_l}\left[ y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right]^2. 
\]</div>
<p>Dla zwięzłości, opuściliśmy górny wskaźnik <span class="math notranslate nohighlight">\((p)\)</span>.
Dla neuronu <span class="math notranslate nohighlight">\(\alpha_j\)</span> w warstwie <span class="math notranslate nohighlight">\(j\)</span> sygnałem wejściowym jest</p>
<div class="math notranslate nohighlight">
\[
s_{\alpha_j}^{j}=\sum_{\alpha_{j-1}=0}^{n_{j-1}} x_{\alpha_{j-1}}^{j-1} w_{\alpha_{j-1} \alpha_j}^{j}.
\]</div>
<p>Sygnały w warstwie wyjściowej mają postać</p>
<div class="math notranslate nohighlight">
\[
y_{o,{\alpha_l}}=f\left( s_{\alpha_l}^{l} \right)
\]</div>
<p>natomiast sygnały wyjściowe w warstwach pośrednich <span class="math notranslate nohighlight">\(j=1,\dots,l-1\)</span> to</p>
<div class="math notranslate nohighlight">
\[
x_{\alpha_j}^{j}=f \left ( s_{\alpha_j}^{j}\right ),\;\;\;\alpha_{j}=1,\dots,n_j, \;\; \; {\rm i} \;\;\; x_0^{j}=1,
\]</div>
<p>z węzłem progowym mającym wartość 1.</p>
<p>Kolejne podstawienia powyższych formuł do <span class="math notranslate nohighlight">\(e\)</span> są następujące:</p>
<p><span class="math notranslate nohighlight">\(e = \sum_{{\alpha_l}=1}^{n_l}\left( y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right)^2\)</span></p>
<p><span class="math notranslate nohighlight">\(=\sum_{{\alpha_l}=1}^{n_l} \left( f \left (\sum_{\alpha_{l-1}=0}^{n_{l-1}} x_{\alpha_{l-1}}^{l-1} w_{\alpha_{l-1} {\alpha_l}}^{l} \right )-y_{t,{\alpha_l}} \right)^2\)</span></p>
<p><span class="math notranslate nohighlight">\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f \left( \sum_{\alpha_{l-2}=0}^{n_{l-2}} x_{\alpha_{l-2}}^{l-2} w_{\alpha_{l-2} \alpha_{l-1}}^{l-1}\right) w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 \gamma}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)</span></p>
<p><span class="math notranslate nohighlight">\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\sum_{\alpha_{l-2}=1}^{n_{l-2}} f\left( \sum_{\alpha_{l-3}=0}^{n_{l-3}} x_{\alpha_{l-3}}^{l-3} w_{\alpha_{l-3} \alpha_{l-2}}^{l-2}\right) w_{\alpha_{l-2} \alpha_{l-1}}^{l-1} + 
x_{0}^{l-2} w_{0 \alpha_{l-1}}^{l-1}
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)</span></p>
<p><span class="math notranslate nohighlight">\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\dots f\left( \sum_{\alpha_{0}=0}^{n_{0}} x_{\alpha_{0}}^{0} w_{\alpha_{0} \alpha_{1}}^{1}\right) w_{\alpha_{1} \alpha_{2}}^{2} + 
x_{0}^{1} w_{0 \alpha_{2}}^{2} \dots
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)</span></p>
<p>Obliczając kolejne pochodne względem wag idąc wstecz, tj. od <span class="math notranslate nohighlight">\(j=l\)</span> do 1, otrzymujemy (patrz ćwiczenia)</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial e}{\partial w^j_{\alpha_{j-1} \alpha_j}} = x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j} , \;\;\; \alpha_{j-1}=0,\dots,n_{j-1}, \;\; \alpha_{j}=1,\dots,n_{j},
\]</div>
<p>gdzie</p>
<p><span class="math notranslate nohighlight">\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f'(s_{\alpha_l}^{l})\)</span>,</p>
<p><span class="math notranslate nohighlight">\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\)</span>.</p>
<p>Ostatnie wyrażenie to rekurencja wstecz. Zauważamy, że aby uzyskać <span class="math notranslate nohighlight">\(D^j\)</span>, potrzebujemy <span class="math notranslate nohighlight">\(D^{j+1}\)</span>, które uzyskaliśmy już w poprzednim kroku, oraz sygnał <span class="math notranslate nohighlight">\(s^j\)</span>, który znamy z propagacji sygnału do przodu. Ta rekurencja prowadzi do uproszczenia oblicznai pochodnych i aktualizacji wag.</p>
<p>Przy najstromszym spadku wagi są aktualizowane jako</p>
<div class="math notranslate nohighlight">
\[ w^j_{\alpha_{j-1} \alpha_j} \to  w^j_{\alpha_{j-1} \alpha_j} -\varepsilon x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j}, \]</div>
<p>W przypadku sigmoidu możemy użyć</p>
<div class="math notranslate nohighlight">
\[
\sigma'(s_A^{(i)})=\sigma'(s_A^{(i)}) (1-\sigma'(s_A^{(i)})) =x_A^{(i)}(1-x_A^{(i)}).
\]</div>
<div class="admonition note">
<p class="admonition-title">Informacja</p>
<p>Powyższe formuły wyjaśniają nazwę <strong>propagacja wsteczna</strong>, ponieważ w aktualizacji wag zaczynamy od ostatniej warstwy, a następnie posuwamy się rekurencyjnie do początku sieci. Na każdym kroku potrzebujemy tylko sygnału w danej warstwie i właściwości kolejnej warstwy! Te cechy wynikają z</p>
<ol class="simple">
<li><p>charakteru feed-forward sieci oraz</p></li>
<li><p>tw. o pochodnej funkcji złożonej.</p></li>
</ol>
</div>
<div class="admonition important">
<p class="admonition-title">Ważne</p>
<p>Praktyczne znaczenie cofania się warstwa po warstwie polega na tym, że w jednym kroku aktualizuje się znacznie mniej wag: tylko te, które wchodzą do danej warstwy, a nie wszystkie naraz. Ma to znaczenie dla zbieżności metody najstromszego spadku, zwłaszcza dla sieci głębokich (o wielu warswach).</p>
</div>
<p>Jeżeli funkcje aktywacji są różne w różnych warstwach (oznaczamy je <span class="math notranslate nohighlight">\(f_j\)</span> dla warstwy <span class="math notranslate nohighlight">\(j\)</span>), to zachodzi oczywista modyfikacja:</p>
<p><span class="math notranslate nohighlight">\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f_l'(s_{\alpha_l}^{l})\)</span>,</p>
<p><span class="math notranslate nohighlight">\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f_j'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\)</span>.</p>
<p>Nie jest to rzadkie, ponieważ w wielu zastosowaniach wybiera się różne funkcje aktywacji dla warstw pośrednich i warswy wyjściowej.</p>
<div class="section" id="kod-dla-algorytmu-backprop">
<h3>Kod dla algorytmu backprop<a class="headerlink" href="#kod-dla-algorytmu-backprop" title="Stały odnośnik do tego nagłówka">¶</a></h3>
<p>Następnie przedstawimy prosty kod realizujący algorytm backprop. Jest to bezpośrednia implementacja wyprowadzonych powyżej formuł. W kodzie zachowujemy jak najwięcej notacji z powyższego wyprowadzenia.</p>
<p>Kod ma tylko 12 linijek, nie licząc komentarzy!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="n">fe</span><span class="p">,</span><span class="n">la</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">ar</span><span class="p">,</span> <span class="n">we</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span><span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">,</span><span class="n">df</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">dsig</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    fe - array of features</span>
<span class="sd">    la - array of labels</span>
<span class="sd">    p  - index of the used data point</span>
<span class="sd">    ar - array of numbers of nodes in subsequent layers</span>
<span class="sd">    we - disctionary of weights</span>
<span class="sd">    eps - learning speed </span>
<span class="sd">    f   - activation function</span>
<span class="sd">    df  - derivaive of f</span>
<span class="sd">    &quot;&quot;&quot;</span>
 
    <span class="n">l</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ar</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="c1"># number of neuron layers (= index of the output layer)</span>
    <span class="n">nl</span><span class="o">=</span><span class="n">ar</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>    <span class="c1"># number of neurons in the otput layer  </span>
   
    <span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span><span class="n">we</span><span class="p">,</span><span class="n">fe</span><span class="p">[</span><span class="n">p</span><span class="p">],</span><span class="n">ff</span><span class="o">=</span><span class="n">f</span><span class="p">)</span> <span class="c1"># feed-forward of point p</span>
   
    <span class="c1"># formulas from the derivation in a one-to-one notation:</span>
    
    <span class="n">D</span><span class="o">=</span><span class="p">{}</span>                 
    <span class="n">D</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">l</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">l</span><span class="p">][</span><span class="n">gam</span><span class="p">]</span><span class="o">-</span><span class="n">la</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="n">gam</span><span class="p">])</span><span class="o">*</span>
                    <span class="n">df</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">we</span><span class="p">[</span><span class="n">l</span><span class="p">]))[</span><span class="n">gam</span><span class="p">]</span> <span class="k">for</span> <span class="n">gam</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nl</span><span class="p">)]})</span>   
    <span class="n">we</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">-=</span><span class="n">eps</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">D</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> 
    
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">l</span><span class="p">)):</span>           
        <span class="n">u</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">we</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="n">D</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]),</span><span class="mi">0</span><span class="p">)</span> 
        <span class="n">v</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">we</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>          
        <span class="n">D</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">j</span><span class="p">:</span> <span class="p">[</span><span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">df</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">))]})</span> 
        <span class="n">we</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">-=</span><span class="n">eps</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">D</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>      
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="przyklad-z-kolem">
<span id="circ-lab"></span><h2>Przykład z kołem<a class="headerlink" href="#przyklad-z-kolem" title="Stały odnośnik do tego nagłówka">¶</a></h2>
<p>Kod ilustrujemy na przykładzie klasyfikatora binarnego punktów wewnątrz okręgu.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cir</span><span class="p">():</span>
    <span class="n">x1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>                  <span class="c1"># coordinate 1</span>
    <span class="n">x2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>                  <span class="c1"># coordinate 2</span>
    <span class="k">if</span><span class="p">((</span><span class="n">x1</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="n">x2</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;</span> <span class="mf">0.4</span><span class="o">*</span><span class="mf">0.4</span><span class="p">):</span> <span class="c1"># inside circle, radius 0.4, center (0.5,0.5)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>                                  <span class="c1"># outside</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Do przyszłego użytku <strong>(nowa konwencja)</strong> podzielimy próbkę na oddzielne tablice <strong>cech</strong> (dwie współrzędne) i <strong>etykiet</strong> (1, jeśli punkt znajduje się wewnątrz okręgu, 0 w przeciwnym razie):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">cir</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">)])</span> <span class="c1"># sample</span>
<span class="n">features_c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">sample_c</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">labels_c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">sample_c</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sample_c</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">sample_c</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">sample_c</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">cool</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">.9</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/backprop_60_0.png" src="../_images/backprop_60_0.png" />
</div>
</div>
<p>Dobieramy następującą architekturę i początkowe parametry:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">arch_c</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>                  <span class="c1"># architecture</span>
<span class="n">weights</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">set_ran_w</span><span class="p">(</span><span class="n">arch_c</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>  <span class="c1"># scaled random initial weights in [-2,2]</span>
<span class="n">eps</span><span class="o">=</span><span class="mf">.7</span>                            <span class="c1"># initial learning speed </span>
</pre></div>
</div>
</div>
</div>
<p>Symulacja zabiera kilka minut.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>   <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">.995</span><span class="o">*</span><span class="n">eps</span>       <span class="c1"># decrease learning speed</span>
    <span class="k">if</span> <span class="n">k</span><span class="o">%</span><span class="k">100</span>==99: print(k+1,&#39; &#39;,end=&#39;&#39;)             # print progress        
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features_c</span><span class="p">)):</span>                <span class="c1"># loop over points</span>
        <span class="n">func</span><span class="o">.</span><span class="n">back_prop</span><span class="p">(</span><span class="n">features_c</span><span class="p">,</span><span class="n">labels_c</span><span class="p">,</span><span class="n">p</span><span class="p">,</span><span class="n">arch_c</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span>
                       <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">,</span><span class="n">df</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">dsig</span><span class="p">)</span> <span class="c1"># backprop</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100  200  300  400  500  600  700  800  900  1000  
</pre></div>
</div>
</div>
</div>
<p>Zmniejszenie szybkości uczenia się w każdej rundzie daje końcową wartość <span class="math notranslate nohighlight">\(\varepsilon\)</span>, która powinna być niewielka, ale nie za mała:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eps</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.004657778005182377
</pre></div>
</div>
</div>
</div>
<p>(zbyt mała wartość aktualizowałaby wagi w znikomy sposób, więc dalsze rundy byłyby bezużyteczne).</p>
<p>Podczas gdy faza nauki była dość długa, testowanie przebiega bardzo szybko:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test</span><span class="o">=</span><span class="p">[]</span> 

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3000</span><span class="p">):</span>
    <span class="n">po</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()]</span> 
    <span class="n">xt</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">arch_c</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">po</span><span class="p">,</span><span class="n">ff</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">)</span>   
    <span class="n">test</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">po</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">po</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">xt</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">arch_c</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">)])</span>

<span class="n">tt</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>

<span class="n">fig</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="c1"># drawing the circle</span>
<span class="n">ax</span><span class="o">=</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">circ</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">radius</span><span class="o">=</span><span class="mf">.4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circ</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tt</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">tt</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">tt</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">cool</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">.9</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/backprop_69_0.png" src="../_images/backprop_69_0.png" />
</div>
</div>
<p>Wytrenowana sieć wygląda następująco:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fnet</span><span class="o">=</span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w</span><span class="p">(</span><span class="n">arch_c</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="mf">.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Informacja</p>
<p>To fascynujące, że nauczyliśmy sieć rozpoznawać, czy punkt znajduje się w okręgu, a nie ma ona żadnego pojęcia o geometrii, odległości euklidesowej, równaniu okręgu itp. Sieć właśnie nauczyła się „empirycznie”, jak postępować, za pomocą próbki szkoleniowej!</p>
</div>
<div class="admonition note">
<p class="admonition-title">Informacja</p>
<p>Wynik przedstawiony na rysunku jest całkiem niezły, może z wyjątkiem, jak zwykle, punktów blisko granicy. Biorąc pod uwagę naszą dyskusję w rozdz. <a class="reference internal" href="more_layers.html#more-lab"><span class="std std-ref">Więcej warstw</span></a>, w którym wyznaczyliśmy wagi sieci z trzema warstwami neuronów na podstawie rozważań geometrycznych, jakość prezentowanego wyniku jest oszałamiająca. Nie widzimy żadnych prostych boków wielokąta, ale ładnie zaokrągloną granicę. Dalsza poprawa wyniku wymagałaby większej liczebności próbki szkoleniowej i dłuższego treningu, co jest czasochłonne.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Lokalne minima</p>
<p>Wspomnieliśmy wcześniej o pojawianiu się minimów lokalnych w optymalizacji wielowymiarowej jako o potencjalnym problemie. Na poniższym rysunku pokazujemy trzy różne wyniki kodu backprop dla naszego klasyfikatora punktów w okręgu. Zauważamy, że każdy z nich ma radykalnie inny zestaw optymalnych wag, podczas gdy spawdzenie na próbce testowej jest, przynajmniej na oko, równie dobre dla każdego przypadku. To pokazuje, że optymalizacja backprop prowadzi, zgodnie z przewidywaniami, do różnych minimów lokalnych. Jednak każde z nich działa wystarczająco i równie dobrze. To jest właśnie powód, dla którego algorytm backprop można wykorzystać w praktycznych problemach: istnieją miliony lokalnych minimów, ale to naprawdę nie ma znaczenia!</p>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/backprop_74_0.png" src="../_images/backprop_74_0.png" />
</div>
</div>
</div>
<div class="section" id="ogolne-uwagi">
<h2>Ogólne uwagi<a class="headerlink" href="#ogolne-uwagi" title="Stały odnośnik do tego nagłówka">¶</a></h2>
<p>Należy poczynić kilka istotnych i ogólnych obserwacji:</p>
<div class="admonition note">
<p class="admonition-title">Informacja</p>
<ul class="simple">
<li><p>Uczenie nadzorowane zajmuje bardzo dużo czasu, ale użycie wytrenowanej sieci trwa mgnienie oka. Asymetria wynika z prostego faktu, że optymalizacja wieloparametrowa wymaga bardzo wielu wywołań funkcji (tutaj <strong>feed-forward</strong>) i obliczneia pochodnych w wielu rundach (użyliśmy 1000 rund dla przykładu okręgu), ale użycie sieci dla przypadku jednego punktu wymaga tylko jednego wywołania funkcji.</p></li>
<li><p>Klasyfikator wyszkolony algorytmem backprop może działać niedokładnie dla punktów w pobliżu linii granicznych. Środkiem zaradczym jest dłuższe trenowanie i/lub zwiększenie
liczebności próbki szkoleniowej, w szczególności w pobliżu granicy.</p></li>
<li><p>Jednak zbyt długa nauka na tej samej próbce treningowej nie ma sensu, ponieważ w pewnym momencie dokładność przestaje się poprawiać.</p></li>
<li><p>Lokalne minima są powszecjne, ale w żadnym wypadku nie stanowi to przeszkody w stosowaniu algorytmu. To ważna praktyczna cecha.</p></li>
<li><p>Można stosować różne ulepszenia metody najstromszego spadku lub zupełnie inne metody minimalizacji (patrz ćwiczenia). Mogą one znacznie zwiększyć wydajność algorytmu.</p></li>
<li><p>Cofając się z aktualizacją wag w kolejnych warstwach, można wprowadzić współczynnik zwiększający uaktualnianie (patrz ćwiczenia). To pomaga w wydajności.</p></li>
<li><p>Wreszcie, inne funkcje aktywacji mogą być używane do poprawy wydajności (patrz kolejne wykłady).</p></li>
</ul>
</div>
</div>
<div class="section" id="cwiczenia">
<h2>Ćwiczenia<a class="headerlink" href="#cwiczenia" title="Stały odnośnik do tego nagłówka">¶</a></h2>
<div class="warning admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(~\)</span></p>
<ol class="simple">
<li><p>Udowodnij (analitycznie), obliczając pochodną, że <span class="math notranslate nohighlight">\( \sigma '(s) = \sigma (s) [1- \sigma (s)]\)</span>. Pokaż, że sigmoid jest <strong>jedyną</strong> funkcją z tą właściwością.</p></li>
<li><p>Wyprowadź jawnie wzory algorytmu backprop dla sieci z jedną i dwiema warstwami pośrednimi. Zwróć uwagę na pojawiającą się prawidłowość (powtarzalność) i udowodnij ogólne wzory z wykładu dla dowolnej liczby warstw pośrednich.</p></li>
<li><p>Zmodyfikuj przykład z wykładu dla klasyfikatora punktów w okręgu dla:</p>
<ul class="simple">
<li><p>półkola;</p></li>
<li><p>dwóch rozłącznych okręgów;</p></li>
<li><p>pierścienia;</p></li>
<li><p>dowolnego z twoich ulubionych kształtów.</p></li>
</ul>
</li>
<li><p>Powtórz 3, eksperymentując z liczbą warstw i neuronów, ale pamiętaj, że duża ich liczba wydłuża czas obliczeń i niekoniecznie poprawia wynik. Uszereguj każdy przypadek według liczby błędnie sklasyfikowanych punktów w próbce testowej. Znajdź optymalną/praktyczną architekturę dla każdego z rozważanych obszarów.</p></li>
<li><p>Jeśli sieć ma dużo neuronów i połączeń, przez każdą synapsę przepływa mało sygnału, stąd sieć jest odporna na niewielkie przypadkowe uszkodzenia. Tak dzieje się w naszym mózgu, który jest nieustannie „uszkadzany” (promienie kosmiczne, alkohol,…). Poza tym taką sieć po zniszczeniu można (już przy mniejszej liczbie połączeń) dodatkowo doszkolić. Weź wytrenowaną sieć z problemu 3. i usuń jedno z jej <strong>słabych</strong> połączeń (najpierw znajdź je, sprawdzając wagi), zmieniając odpowiednią wagę na 0. Przetestuj taką uszkodzoną sieć na próbce testowej i wyciągnij wnioski.</p></li>
<li><p><strong>Skalowanie wag w propagacji wstecznej.</strong>
Wadą zastosowania sigmoidu w algorytmie backprop jest bardzo powolna aktualizacja wag w warstwach odległych od warstwy wyjściowej (im bliżej początku sieci, tym wolniej). Remedium jest tutaj przeskalowanie wag, gdzie szybkość uczenia się warstw, licząc od tyłu, jest sukcesywnie zwiększana o pewien współczynnik. Pamiętamy, że kolejne pochodne wnoszą do szybkości aktualizacji współczynniki postaci <span class="math notranslate nohighlight">\( \sigma '(s) = \sigma (s) [1- \sigma (s)] = y (1-y) \)</span>, gdzie <span class="math notranslate nohighlight">\( y \)</span> wynosi w zakresie <span class="math notranslate nohighlight">\( (0, 1) \)</span>. Zatem wartość <span class="math notranslate nohighlight">\( y (1-y \)</span> nie może przekraczać 1/4, a w kolejnych warstwach (licząc od tyłu) czynnika <span class="math notranslate nohighlight">\( [y (1-y] ^ n \le 1/4 ^ n\)</span>).
Aby zapobiec temu „kurczeniu się”, wskaźnik uczenia się można przemnażać przez współczynniki kompensacyjne <span class="math notranslate nohighlight">\(4 ^ n: 4, 16, 64, 256, ... \)</span>. Kolejny argument heurystyczny <span id="id2">[<a class="reference internal" href="conclusion.html#id13" title="A. K. Rigler, J. M. Irvine, and Thomas P. Vogl. Rescaling of variables in back propagation learning. Neural Networks, 4(2):225–229, 1991. URL: https://doi.org/10.1016/0893-6080(91)90006-Q, doi:10.1016/0893-6080(91)90006-Q.">RIV91</a>]</span> sugeruje jeszcze szybciej rosnące czynniki  postaci <span class="math notranslate nohighlight">\(6^n\)</span>:<span class="math notranslate nohighlight">\(6,36,216,1296,...\)</span></p>
<ul class="simple">
<li><p>Wprowadź powyższe dwie receptury do kodu backprop.</p></li>
<li><p>Sprawdź, czy rzeczywiście poprawiają wydajność algorytmu dla głębszych sieci, na przykład dla klasyfikatora punktów okręgu itp.</p></li>
<li><p>W celu oceny wydajności wykonaj pomiar czasu wykonania (np. za pomocą pakietu biblioteki Python <strong>time</strong>).</p></li>
</ul>
</li>
<li><p><strong>Najstromsze spadek.</strong>
Zastosowana w wykładzie metoda najstromszego spadku do wyznaczania minimum funkcji wielu zmiennych zależy od gradientu lokalnego. Istnieją znacznie lepsze podejścia, które zapewniają szybszą zbieżność do (lokalnego) minimum. Jednym z nich jest przepis <a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">Barzilai-Borwein</a> wyjaśniony poniżej. Zaimplementuj tę metodę w algorytmie wstecznej propagacji. Wektory <span class="math notranslate nohighlight">\(x\)</span> w przestrzeni <span class="math notranslate nohighlight">\(n\)</span>-wymiarowej są aktualizowane w kolejnych iteracjach jako <span class="math notranslate nohighlight">\( x^{(m + 1)} = x^{(m)} - \gamma_m \nabla F (x^{(m)} )\)</span>,
gdzie <span class="math notranslate nohighlight">\(m\)</span> numeruje iterację, a szybkość uczenia się zależy od zachowania w dwóch (bieżącym i poprzednim) punktach:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ \gamma _ {m} = \frac {\left | \left (x^{(m)}-x^{(m-1)} \right) \cdot
\left [\nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right] \right |}
{\left \| \nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right \| ^ {2}}.
\]</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="more_layers.html" title="wstecz strona">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">wstecz</p>
            <p class="prev-next-title">Więcej warstw</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="interpol.html" title="dalej strona">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">dalej</p>
        <p class="prev-next-title">Interpolation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Przez Wojciech Broniowski<br/>
    
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>